{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential Decision Making - Submission Notebook\n",
    "================================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Decision Making\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"abstract\" id=\"org0039e50\">\n",
    "<p>\n",
    "This notebook will form part of your individual submission for the course. The notebook will roughly mimick the parts that are in the <code>PDF</code> worksheet. Your task is to complete the code that is missing in the parts below and answer the questions that we ask. The aim is <b>not</b> for you to <b>solve</b> the worksheet but rather for you to show your understanding of the material in the course, instead of re-running and aiming to get \"perfect\" results run things, make sure it is correct and then try to explain your results with a few sentences.\n",
    "</p>\n",
    "\n",
    "</div>\n",
    "\n",
    "First we need to implement the surrogate model, we will use a Gaussian process surrogate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def f(X, noise=0.0):\n",
    "    return -(-np.sin(3*X) - X**2 + 0.7*X + noise*np.random.randn(*X.shape))\n",
    "\n",
    "def squared_exponential_computer(x1,x2,theta):\n",
    "    # compute the squared exponential covariance function\n",
    "    return K\n",
    "\n",
    "def gpposterior(x_star,X,Y,lengthScale,sigma):\n",
    "    # return the posterior estimate of the GP\n",
    "    return mu, varSigma\n",
    "\n",
    "theta = np.zeros((2, ))\n",
    "theta[0] = 0.3  # lengthscale\n",
    "theta[1] = 1.0; # variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have the surrogate model up and running we need to implement the acquisition function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_improvement(f_star, mu, varSigma):\n",
    "    # return the value of the acquisition function at each\n",
    "    \n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have the above code up and running you should be able to reproduce the results that are in Figure 1 in the worksheet. Now lets try to run some additional experiments. First lets evaluate the effect of the initial start locations. Create a plot where on the `x-axis` have the number of times you have evaluated the true function and on the `y-axis` have the current minima, run the optimisation loop several times and plot the mean and two standard deviations for the minimal value at each iteration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (3876931541.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [12]\u001b[0;36m\u001b[0m\n\u001b[0;31m    for k in range(0, n_evals):\u001b[0m\n\u001b[0m                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "# implement a loop that tries a set of random-restarts\n",
    "np.random.seed(42)\n",
    "for j in range(0, n_starts):\n",
    "    for k in range(0, n_evals):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain why the plots looks this way? Does it make a difference how many initial points you start with?\n",
    "\n",
    "While the function have a local minima so it presents some challenges for optimisation it is still quite easy to find the minima. Let us try make the function a bit more challenging by adding a bit of noise to the function. Implement an additional loop around the previous two loops which alters the amount of noise added.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (3724125853.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [13]\u001b[0;36m\u001b[0m\n\u001b[0;31m    for k in range(0, n_evals):\u001b[0m\n\u001b[0m                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "# implement a loop that tries different noise-levels\n",
    "np.random.seed(42)\n",
    "for i in range(0, 10):\n",
    "    y = f(x, noise[i])\n",
    "    for j in range(0, n_starts):\n",
    "        for k in range(0, n_evals):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain the results by contrasting to the previous none-noisy evaluation. How does the \"best\" run compare to the \"best\" run in the previous example?\n",
    "\n",
    "As you have probably noticed the kernel-hyperparmeters have a huge effect on the results. This is a desirable effect as this is where we encode our knowledge of the function. We will now do one experiment where we will alter the lengthscale value and see how it effects the results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (2248304681.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [14]\u001b[0;36m\u001b[0m\n\u001b[0;31m    for k in range(0, n_evals):\u001b[0m\n\u001b[0m                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "# implement a loop that tries different\n",
    "np.random.seed(42)\n",
    "for i in range(0, lengthScales.shape[0]):\n",
    "    theta[0] = lengthScales[i]\n",
    "    for j in range(0, n_starts):\n",
    "        for k in range(0, n_evals):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain the results and why you see the specific effect? You might want to increase the `n_evals` a little bit compared to your previous experiment. You can also implement a criteria to leave the inner-loop based on how large your improvement is over a set number of iterations. As you probably notice the effect of the hyper-parameters is quite significant. Explain how you think we should approach a problem where we are uncertain about the hyper-parameters?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a final **extra** experiment try to fit the kernel parameters inside the inner-loop. The way to do this is to maximise the marginal likelihood of the surrogate model using gradient descent. You can alter the `numpy` code that we have implemented to `jax` instead and which will allow you to use `auto-differetiation` to compute gradients. Now you can implement a simple gradient descent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import grad\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def squared_exponential(x1, x2, theta):\n",
    "    # theta[0] - variance\n",
    "    # theta[1] - lengthscale\n",
    "    if x2 == None:\n",
    "        return theta[0]*jnp.exp(-cdist(x1, x1, metric='sqeuclidean')/theta[1]**2)\n",
    "    else:\n",
    "        return theta[0]*jnp.exp(-cdist(x1, x2, metric='sqeuclidean')/theta[1]**2)\n",
    "\n",
    "def logmarginal_likelihood(x, y, theta):\n",
    "    # implement the log-marginal likelihood of a GP\n",
    "    \n",
    "dLdtheta = grad(logmarginal_likelihood, argnums=2)\n",
    "for i in range(1000):\n",
    "    \n",
    "    theta -= dLdtheta(w) * 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain the results that you get above. Do you think this is a good strategy?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can submit the notebook on `Moodle`. The deadline for the submission is Friday the 4th of November at 23:59.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "org": null
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
