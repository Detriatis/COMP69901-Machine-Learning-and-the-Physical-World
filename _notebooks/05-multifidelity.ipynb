{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multifidelity Modelling\n",
    "=======================\n",
    "\n",
    "### [Neil D. Lawrence](http://inverseprobability.com)\n",
    "\n",
    "### 2020-11-06"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abstract**: This week we introduce multifidelity modelling. We use\n",
    "surrogate models to capture different qualities of information from\n",
    "different simulations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\tk}[1]{}\n",
    "\\newcommand{\\Amatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\KL}[2]{\\text{KL}\\left( #1\\,\\|\\,#2 \\right)}\n",
    "\\newcommand{\\Kaast}{\\kernelMatrix_{\\mathbf{ \\ast}\\mathbf{ \\ast}}}\n",
    "\\newcommand{\\Kastu}{\\kernelMatrix_{\\mathbf{ \\ast} \\inducingVector}}\n",
    "\\newcommand{\\Kff}{\\kernelMatrix_{\\mappingFunctionVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\Kfu}{\\kernelMatrix_{\\mappingFunctionVector \\inducingVector}}\n",
    "\\newcommand{\\Kuast}{\\kernelMatrix_{\\inducingVector \\bf\\ast}}\n",
    "\\newcommand{\\Kuf}{\\kernelMatrix_{\\inducingVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\Kuu}{\\kernelMatrix_{\\inducingVector \\inducingVector}}\n",
    "\\newcommand{\\Kuui}{\\Kuu^{-1}}\n",
    "\\newcommand{\\Qaast}{\\mathbf{Q}_{\\bf \\ast \\ast}}\n",
    "\\newcommand{\\Qastf}{\\mathbf{Q}_{\\ast \\mappingFunction}}\n",
    "\\newcommand{\\Qfast}{\\mathbf{Q}_{\\mappingFunctionVector \\bf \\ast}}\n",
    "\\newcommand{\\Qff}{\\mathbf{Q}_{\\mappingFunctionVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\aMatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\aScalar}{a}\n",
    "\\newcommand{\\aVector}{\\mathbf{a}}\n",
    "\\newcommand{\\acceleration}{a}\n",
    "\\newcommand{\\bMatrix}{\\mathbf{B}}\n",
    "\\newcommand{\\bScalar}{b}\n",
    "\\newcommand{\\bVector}{\\mathbf{b}}\n",
    "\\newcommand{\\basisFunc}{\\phi}\n",
    "\\newcommand{\\basisFuncVector}{\\boldsymbol{ \\basisFunc}}\n",
    "\\newcommand{\\basisFunction}{\\phi}\n",
    "\\newcommand{\\basisLocation}{\\mu}\n",
    "\\newcommand{\\basisMatrix}{\\boldsymbol{ \\Phi}}\n",
    "\\newcommand{\\basisScalar}{\\basisFunction}\n",
    "\\newcommand{\\basisVector}{\\boldsymbol{ \\basisFunction}}\n",
    "\\newcommand{\\activationFunction}{\\phi}\n",
    "\\newcommand{\\activationMatrix}{\\boldsymbol{ \\Phi}}\n",
    "\\newcommand{\\activationScalar}{\\basisFunction}\n",
    "\\newcommand{\\activationVector}{\\boldsymbol{ \\basisFunction}}\n",
    "\\newcommand{\\bigO}{\\mathcal{O}}\n",
    "\\newcommand{\\binomProb}{\\pi}\n",
    "\\newcommand{\\cMatrix}{\\mathbf{C}}\n",
    "\\newcommand{\\cbasisMatrix}{\\hat{\\boldsymbol{ \\Phi}}}\n",
    "\\newcommand{\\cdataMatrix}{\\hat{\\dataMatrix}}\n",
    "\\newcommand{\\cdataScalar}{\\hat{\\dataScalar}}\n",
    "\\newcommand{\\cdataVector}{\\hat{\\dataVector}}\n",
    "\\newcommand{\\centeredKernelMatrix}{\\mathbf{ \\MakeUppercase{\\centeredKernelScalar}}}\n",
    "\\newcommand{\\centeredKernelScalar}{b}\n",
    "\\newcommand{\\centeredKernelVector}{\\centeredKernelScalar}\n",
    "\\newcommand{\\centeringMatrix}{\\mathbf{H}}\n",
    "\\newcommand{\\chiSquaredDist}[2]{\\chi_{#1}^{2}\\left(#2\\right)}\n",
    "\\newcommand{\\chiSquaredSamp}[1]{\\chi_{#1}^{2}}\n",
    "\\newcommand{\\conditionalCovariance}{\\boldsymbol{ \\Sigma}}\n",
    "\\newcommand{\\coregionalizationMatrix}{\\mathbf{B}}\n",
    "\\newcommand{\\coregionalizationScalar}{b}\n",
    "\\newcommand{\\coregionalizationVector}{\\mathbf{ \\coregionalizationScalar}}\n",
    "\\newcommand{\\covDist}[2]{\\text{cov}_{#2}\\left(#1\\right)}\n",
    "\\newcommand{\\covSamp}[1]{\\text{cov}\\left(#1\\right)}\n",
    "\\newcommand{\\covarianceScalar}{c}\n",
    "\\newcommand{\\covarianceVector}{\\mathbf{ \\covarianceScalar}}\n",
    "\\newcommand{\\covarianceMatrix}{\\mathbf{C}}\n",
    "\\newcommand{\\covarianceMatrixTwo}{\\boldsymbol{ \\Sigma}}\n",
    "\\newcommand{\\croupierScalar}{s}\n",
    "\\newcommand{\\croupierVector}{\\mathbf{ \\croupierScalar}}\n",
    "\\newcommand{\\croupierMatrix}{\\mathbf{ \\MakeUppercase{\\croupierScalar}}}\n",
    "\\newcommand{\\dataDim}{p}\n",
    "\\newcommand{\\dataIndex}{i}\n",
    "\\newcommand{\\dataIndexTwo}{j}\n",
    "\\newcommand{\\dataMatrix}{\\mathbf{Y}}\n",
    "\\newcommand{\\dataScalar}{y}\n",
    "\\newcommand{\\dataSet}{\\mathcal{D}}\n",
    "\\newcommand{\\dataStd}{\\sigma}\n",
    "\\newcommand{\\dataVector}{\\mathbf{ \\dataScalar}}\n",
    "\\newcommand{\\decayRate}{d}\n",
    "\\newcommand{\\degreeMatrix}{\\mathbf{ \\MakeUppercase{\\degreeScalar}}}\n",
    "\\newcommand{\\degreeScalar}{d}\n",
    "\\newcommand{\\degreeVector}{\\mathbf{ \\degreeScalar}}\n",
    "\\newcommand{\\diag}[1]{\\text{diag}\\left(#1\\right)}\n",
    "\\newcommand{\\diagonalMatrix}{\\mathbf{D}}\n",
    "\\newcommand{\\diff}[2]{\\frac{\\text{d}#1}{\\text{d}#2}}\n",
    "\\newcommand{\\diffTwo}[2]{\\frac{\\text{d}^2#1}{\\text{d}#2^2}}\n",
    "\\newcommand{\\displacement}{x}\n",
    "\\newcommand{\\displacementVector}{\\textbf{\\displacement}}\n",
    "\\newcommand{\\distanceMatrix}{\\mathbf{ \\MakeUppercase{\\distanceScalar}}}\n",
    "\\newcommand{\\distanceScalar}{d}\n",
    "\\newcommand{\\distanceVector}{\\mathbf{ \\distanceScalar}}\n",
    "\\newcommand{\\eigenvaltwo}{\\ell}\n",
    "\\newcommand{\\eigenvaltwoMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\eigenvaltwoVector}{\\mathbf{l}}\n",
    "\\newcommand{\\eigenvalue}{\\lambda}\n",
    "\\newcommand{\\eigenvalueMatrix}{\\boldsymbol{ \\Lambda}}\n",
    "\\newcommand{\\eigenvalueVector}{\\boldsymbol{ \\lambda}}\n",
    "\\newcommand{\\eigenvector}{\\mathbf{ \\eigenvectorScalar}}\n",
    "\\newcommand{\\eigenvectorMatrix}{\\mathbf{U}}\n",
    "\\newcommand{\\eigenvectorScalar}{u}\n",
    "\\newcommand{\\eigenvectwo}{\\mathbf{v}}\n",
    "\\newcommand{\\eigenvectwoMatrix}{\\mathbf{V}}\n",
    "\\newcommand{\\eigenvectwoScalar}{v}\n",
    "\\newcommand{\\entropy}[1]{\\mathcal{H}\\left(#1\\right)}\n",
    "\\newcommand{\\errorFunction}{E}\n",
    "\\newcommand{\\expDist}[2]{\\left<#1\\right>_{#2}}\n",
    "\\newcommand{\\expSamp}[1]{\\left<#1\\right>}\n",
    "\\newcommand{\\expectation}[1]{\\left\\langle #1 \\right\\rangle }\n",
    "\\newcommand{\\expectationDist}[2]{\\left\\langle #1 \\right\\rangle _{#2}}\n",
    "\\newcommand{\\expectedDistanceMatrix}{\\mathcal{D}}\n",
    "\\newcommand{\\eye}{\\mathbf{I}}\n",
    "\\newcommand{\\fantasyDim}{r}\n",
    "\\newcommand{\\fantasyMatrix}{\\mathbf{ \\MakeUppercase{\\fantasyScalar}}}\n",
    "\\newcommand{\\fantasyScalar}{z}\n",
    "\\newcommand{\\fantasyVector}{\\mathbf{ \\fantasyScalar}}\n",
    "\\newcommand{\\featureStd}{\\varsigma}\n",
    "\\newcommand{\\gammaCdf}[3]{\\mathcal{GAMMA CDF}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gammaDist}[3]{\\mathcal{G}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gammaSamp}[2]{\\mathcal{G}\\left(#1,#2\\right)}\n",
    "\\newcommand{\\gaussianDist}[3]{\\mathcal{N}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gaussianSamp}[2]{\\mathcal{N}\\left(#1,#2\\right)}\n",
    "\\newcommand{\\given}{|}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\newcommand{\\heaviside}{H}\n",
    "\\newcommand{\\hiddenMatrix}{\\mathbf{ \\MakeUppercase{\\hiddenScalar}}}\n",
    "\\newcommand{\\hiddenScalar}{h}\n",
    "\\newcommand{\\hiddenVector}{\\mathbf{ \\hiddenScalar}}\n",
    "\\newcommand{\\identityMatrix}{\\eye}\n",
    "\\newcommand{\\inducingInputScalar}{z}\n",
    "\\newcommand{\\inducingInputVector}{\\mathbf{ \\inducingInputScalar}}\n",
    "\\newcommand{\\inducingInputMatrix}{\\mathbf{Z}}\n",
    "\\newcommand{\\inducingScalar}{u}\n",
    "\\newcommand{\\inducingVector}{\\mathbf{ \\inducingScalar}}\n",
    "\\newcommand{\\inducingMatrix}{\\mathbf{U}}\n",
    "\\newcommand{\\inlineDiff}[2]{\\text{d}#1/\\text{d}#2}\n",
    "\\newcommand{\\inputDim}{q}\n",
    "\\newcommand{\\inputMatrix}{\\mathbf{X}}\n",
    "\\newcommand{\\inputScalar}{x}\n",
    "\\newcommand{\\inputSpace}{\\mathcal{X}}\n",
    "\\newcommand{\\inputVals}{\\inputVector}\n",
    "\\newcommand{\\inputVector}{\\mathbf{ \\inputScalar}}\n",
    "\\newcommand{\\iterNum}{k}\n",
    "\\newcommand{\\kernel}{\\kernelScalar}\n",
    "\\newcommand{\\kernelMatrix}{\\mathbf{K}}\n",
    "\\newcommand{\\kernelScalar}{k}\n",
    "\\newcommand{\\kernelVector}{\\mathbf{ \\kernelScalar}}\n",
    "\\newcommand{\\kff}{\\kernelScalar_{\\mappingFunction \\mappingFunction}}\n",
    "\\newcommand{\\kfu}{\\kernelVector_{\\mappingFunction \\inducingScalar}}\n",
    "\\newcommand{\\kuf}{\\kernelVector_{\\inducingScalar \\mappingFunction}}\n",
    "\\newcommand{\\kuu}{\\kernelVector_{\\inducingScalar \\inducingScalar}}\n",
    "\\newcommand{\\lagrangeMultiplier}{\\lambda}\n",
    "\\newcommand{\\lagrangeMultiplierMatrix}{\\boldsymbol{ \\Lambda}}\n",
    "\\newcommand{\\lagrangian}{L}\n",
    "\\newcommand{\\laplacianFactor}{\\mathbf{ \\MakeUppercase{\\laplacianFactorScalar}}}\n",
    "\\newcommand{\\laplacianFactorScalar}{m}\n",
    "\\newcommand{\\laplacianFactorVector}{\\mathbf{ \\laplacianFactorScalar}}\n",
    "\\newcommand{\\laplacianMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\laplacianScalar}{\\ell}\n",
    "\\newcommand{\\laplacianVector}{\\mathbf{ \\ell}}\n",
    "\\newcommand{\\latentDim}{q}\n",
    "\\newcommand{\\latentDistanceMatrix}{\\boldsymbol{ \\Delta}}\n",
    "\\newcommand{\\latentDistanceScalar}{\\delta}\n",
    "\\newcommand{\\latentDistanceVector}{\\boldsymbol{ \\delta}}\n",
    "\\newcommand{\\latentForce}{f}\n",
    "\\newcommand{\\latentFunction}{u}\n",
    "\\newcommand{\\latentFunctionVector}{\\mathbf{ \\latentFunction}}\n",
    "\\newcommand{\\latentFunctionMatrix}{\\mathbf{ \\MakeUppercase{\\latentFunction}}}\n",
    "\\newcommand{\\latentIndex}{j}\n",
    "\\newcommand{\\latentScalar}{z}\n",
    "\\newcommand{\\latentVector}{\\mathbf{ \\latentScalar}}\n",
    "\\newcommand{\\latentMatrix}{\\mathbf{Z}}\n",
    "\\newcommand{\\learnRate}{\\eta}\n",
    "\\newcommand{\\lengthScale}{\\ell}\n",
    "\\newcommand{\\rbfWidth}{\\ell}\n",
    "\\newcommand{\\likelihoodBound}{\\mathcal{L}}\n",
    "\\newcommand{\\likelihoodFunction}{L}\n",
    "\\newcommand{\\locationScalar}{\\mu}\n",
    "\\newcommand{\\locationVector}{\\boldsymbol{ \\locationScalar}}\n",
    "\\newcommand{\\locationMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\variance}[1]{\\text{var}\\left( #1 \\right)}\n",
    "\\newcommand{\\mappingFunction}{f}\n",
    "\\newcommand{\\mappingFunctionMatrix}{\\mathbf{F}}\n",
    "\\newcommand{\\mappingFunctionTwo}{g}\n",
    "\\newcommand{\\mappingFunctionTwoMatrix}{\\mathbf{G}}\n",
    "\\newcommand{\\mappingFunctionTwoVector}{\\mathbf{ \\mappingFunctionTwo}}\n",
    "\\newcommand{\\mappingFunctionVector}{\\mathbf{ \\mappingFunction}}\n",
    "\\newcommand{\\scaleScalar}{s}\n",
    "\\newcommand{\\mappingScalar}{w}\n",
    "\\newcommand{\\mappingVector}{\\mathbf{ \\mappingScalar}}\n",
    "\\newcommand{\\mappingMatrix}{\\mathbf{W}}\n",
    "\\newcommand{\\mappingScalarTwo}{v}\n",
    "\\newcommand{\\mappingVectorTwo}{\\mathbf{ \\mappingScalarTwo}}\n",
    "\\newcommand{\\mappingMatrixTwo}{\\mathbf{V}}\n",
    "\\newcommand{\\maxIters}{K}\n",
    "\\newcommand{\\meanMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\meanScalar}{\\mu}\n",
    "\\newcommand{\\meanTwoMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\meanTwoScalar}{m}\n",
    "\\newcommand{\\meanTwoVector}{\\mathbf{ \\meanTwoScalar}}\n",
    "\\newcommand{\\meanVector}{\\boldsymbol{ \\meanScalar}}\n",
    "\\newcommand{\\mrnaConcentration}{m}\n",
    "\\newcommand{\\naturalFrequency}{\\omega}\n",
    "\\newcommand{\\neighborhood}[1]{\\mathcal{N}\\left( #1 \\right)}\n",
    "\\newcommand{\\neilurl}{http://inverseprobability.com/}\n",
    "\\newcommand{\\noiseMatrix}{\\boldsymbol{ E}}\n",
    "\\newcommand{\\noiseScalar}{\\epsilon}\n",
    "\\newcommand{\\noiseVector}{\\boldsymbol{ \\epsilon}}\n",
    "\\newcommand{\\norm}[1]{\\left\\Vert #1 \\right\\Vert}\n",
    "\\newcommand{\\normalizedLaplacianMatrix}{\\hat{\\mathbf{L}}}\n",
    "\\newcommand{\\normalizedLaplacianScalar}{\\hat{\\ell}}\n",
    "\\newcommand{\\normalizedLaplacianVector}{\\hat{\\mathbf{ \\ell}}}\n",
    "\\newcommand{\\numActive}{m}\n",
    "\\newcommand{\\numBasisFunc}{m}\n",
    "\\newcommand{\\numComponents}{m}\n",
    "\\newcommand{\\numComps}{K}\n",
    "\\newcommand{\\numData}{n}\n",
    "\\newcommand{\\numFeatures}{K}\n",
    "\\newcommand{\\numHidden}{h}\n",
    "\\newcommand{\\numInducing}{m}\n",
    "\\newcommand{\\numLayers}{\\ell}\n",
    "\\newcommand{\\numNeighbors}{K}\n",
    "\\newcommand{\\numSequences}{s}\n",
    "\\newcommand{\\numSuccess}{s}\n",
    "\\newcommand{\\numTasks}{m}\n",
    "\\newcommand{\\numTime}{T}\n",
    "\\newcommand{\\numTrials}{S}\n",
    "\\newcommand{\\outputIndex}{j}\n",
    "\\newcommand{\\paramVector}{\\boldsymbol{ \\theta}}\n",
    "\\newcommand{\\parameterMatrix}{\\boldsymbol{ \\Theta}}\n",
    "\\newcommand{\\parameterScalar}{\\theta}\n",
    "\\newcommand{\\parameterVector}{\\boldsymbol{ \\parameterScalar}}\n",
    "\\newcommand{\\partDiff}[2]{\\frac{\\partial#1}{\\partial#2}}\n",
    "\\newcommand{\\precisionScalar}{j}\n",
    "\\newcommand{\\precisionVector}{\\mathbf{ \\precisionScalar}}\n",
    "\\newcommand{\\precisionMatrix}{\\mathbf{J}}\n",
    "\\newcommand{\\pseudotargetScalar}{\\widetilde{y}}\n",
    "\\newcommand{\\pseudotargetVector}{\\mathbf{ \\pseudotargetScalar}}\n",
    "\\newcommand{\\pseudotargetMatrix}{\\mathbf{ \\widetilde{Y}}}\n",
    "\\newcommand{\\rank}[1]{\\text{rank}\\left(#1\\right)}\n",
    "\\newcommand{\\rayleighDist}[2]{\\mathcal{R}\\left(#1|#2\\right)}\n",
    "\\newcommand{\\rayleighSamp}[1]{\\mathcal{R}\\left(#1\\right)}\n",
    "\\newcommand{\\responsibility}{r}\n",
    "\\newcommand{\\rotationScalar}{r}\n",
    "\\newcommand{\\rotationVector}{\\mathbf{ \\rotationScalar}}\n",
    "\\newcommand{\\rotationMatrix}{\\mathbf{R}}\n",
    "\\newcommand{\\sampleCovScalar}{s}\n",
    "\\newcommand{\\sampleCovVector}{\\mathbf{ \\sampleCovScalar}}\n",
    "\\newcommand{\\sampleCovMatrix}{\\mathbf{s}}\n",
    "\\newcommand{\\scalarProduct}[2]{\\left\\langle{#1},{#2}\\right\\rangle}\n",
    "\\newcommand{\\sign}[1]{\\text{sign}\\left(#1\\right)}\n",
    "\\newcommand{\\sigmoid}[1]{\\sigma\\left(#1\\right)}\n",
    "\\newcommand{\\singularvalue}{\\ell}\n",
    "\\newcommand{\\singularvalueMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\singularvalueVector}{\\mathbf{l}}\n",
    "\\newcommand{\\sorth}{\\mathbf{u}}\n",
    "\\newcommand{\\spar}{\\lambda}\n",
    "\\newcommand{\\trace}[1]{\\text{tr}\\left(#1\\right)}\n",
    "\\newcommand{\\BasalRate}{B}\n",
    "\\newcommand{\\DampingCoefficient}{C}\n",
    "\\newcommand{\\DecayRate}{D}\n",
    "\\newcommand{\\Displacement}{X}\n",
    "\\newcommand{\\LatentForce}{F}\n",
    "\\newcommand{\\Mass}{M}\n",
    "\\newcommand{\\Sensitivity}{S}\n",
    "\\newcommand{\\basalRate}{b}\n",
    "\\newcommand{\\dampingCoefficient}{c}\n",
    "\\newcommand{\\mass}{m}\n",
    "\\newcommand{\\sensitivity}{s}\n",
    "\\newcommand{\\springScalar}{\\kappa}\n",
    "\\newcommand{\\springVector}{\\boldsymbol{ \\kappa}}\n",
    "\\newcommand{\\springMatrix}{\\boldsymbol{ \\mathcal{K}}}\n",
    "\\newcommand{\\tfConcentration}{p}\n",
    "\\newcommand{\\tfDecayRate}{\\delta}\n",
    "\\newcommand{\\tfMrnaConcentration}{f}\n",
    "\\newcommand{\\tfVector}{\\mathbf{ \\tfConcentration}}\n",
    "\\newcommand{\\velocity}{v}\n",
    "\\newcommand{\\sufficientStatsScalar}{g}\n",
    "\\newcommand{\\sufficientStatsVector}{\\mathbf{ \\sufficientStatsScalar}}\n",
    "\\newcommand{\\sufficientStatsMatrix}{\\mathbf{G}}\n",
    "\\newcommand{\\switchScalar}{s}\n",
    "\\newcommand{\\switchVector}{\\mathbf{ \\switchScalar}}\n",
    "\\newcommand{\\switchMatrix}{\\mathbf{S}}\n",
    "\\newcommand{\\tr}[1]{\\text{tr}\\left(#1\\right)}\n",
    "\\newcommand{\\loneNorm}[1]{\\left\\Vert #1 \\right\\Vert_1}\n",
    "\\newcommand{\\ltwoNorm}[1]{\\left\\Vert #1 \\right\\Vert_2}\n",
    "\\newcommand{\\onenorm}[1]{\\left\\vert#1\\right\\vert_1}\n",
    "\\newcommand{\\twonorm}[1]{\\left\\Vert #1 \\right\\Vert}\n",
    "\\newcommand{\\vScalar}{v}\n",
    "\\newcommand{\\vVector}{\\mathbf{v}}\n",
    "\\newcommand{\\vMatrix}{\\mathbf{V}}\n",
    "\\newcommand{\\varianceDist}[2]{\\text{var}_{#2}\\left( #1 \\right)}\n",
    "\\newcommand{\\vecb}[1]{\\left(#1\\right):}\n",
    "\\newcommand{\\weightScalar}{w}\n",
    "\\newcommand{\\weightVector}{\\mathbf{ \\weightScalar}}\n",
    "\\newcommand{\\weightMatrix}{\\mathbf{W}}\n",
    "\\newcommand{\\weightedAdjacencyMatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\weightedAdjacencyScalar}{a}\n",
    "\\newcommand{\\weightedAdjacencyVector}{\\mathbf{ \\weightedAdjacencyScalar}}\n",
    "\\newcommand{\\onesVector}{\\mathbf{1}}\n",
    "\\newcommand{\\zerosVector}{\\mathbf{0}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!---->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->\n",
    "<!--\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 22})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Introduction to Multi-fidelity Modeling in Emukit\n",
    "===================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview\n",
    "--------\n",
    "\n",
    "This section is based on the Emukit multifidelity tutoria found\n",
    "here\\](https://github.com/EmuKit/emukit/blob/master/notebooks/Emukit-tutorial-multi-fidelity-bayesian-optimization.ipynb)\n",
    "and written by Javier Gonzalez, Mark Pullin, Oleg Ponomarev and\n",
    "David-Elias Künstle.\n",
    "\n",
    "A common issue encountered when applying machine learning to\n",
    "environmental sciences and engineering problems is the difficulty or\n",
    "cost required to obtain sufficient data for building robust models.\n",
    "Possible examples include aerospace and nautical engineering, where it\n",
    "is both infeasible and prohibitively expensive to run a vast number of\n",
    "experiments using the actual vehicle. Even when there is no physical\n",
    "artifact involved, such as in climate modeling, data may still be hard\n",
    "to obtain when these can only be collected by running an expensive\n",
    "computer experiment, where the time required to acquire an individual\n",
    "data sample restricts the volume of data that can later be used for\n",
    "modeling.\n",
    "\n",
    "Constructing a reliable model when only few observations are available\n",
    "is challenging, which is why it is common practice to develop\n",
    "*simulators* of the actual system, from which data points can be more\n",
    "easily obtained. In engineering applications, such simulators often take\n",
    "the form of Computational Fluid Dynamics (CFD) tools which approximate\n",
    "the behaviour of the true artifact for a given design or configuration.\n",
    "However, although it is now possible to obtain more data samples, it is\n",
    "highly unlikely that these simulators model the true system exactly;\n",
    "instead, these are expected to contain some degree of bias and/or noise.\n",
    "\n",
    "From the above, one can deduce that naively combining observations from\n",
    "multiple information sources could result in the model giving biased\n",
    "predictions which do not accurately reflect the true problem. To this\n",
    "end, **multi-fidelity models** are designed to augment the limited true\n",
    "observations available with cheaply-obtained approximations in a\n",
    "principled manner. In such models, observations obtained from the true\n",
    "source are referred to as *high-fidelity* observations, whereas\n",
    "approximations are denoted as being *low-fidelity*. These low-fidelity\n",
    "observations are then systemically combined with the more accurate (but\n",
    "limited) observations in order to predict the high-fidelity output more\n",
    "effectively. Note than we can generally combine information from\n",
    "multiple lower fidelity sources, which can all be seen as auxiliary\n",
    "tasks in support of a single primary task.\n",
    "\n",
    "In this notebook, we shall investigate a selection of multi-fidelity\n",
    "models based on Gaussian processes which are readily available in\n",
    "`Emukit`. We start by investigating the traditional linear\n",
    "multi-fidelity model as proposed in (Kennedy and O’Hagan, 2000).\n",
    "Subsequently, we shall illustrate why this model can be unsuitable when\n",
    "the mapping from low to high-fidelity observations is nonlinear, and\n",
    "demonstrate how an alternate model proposed in Perdikaris et al. (2017)\n",
    "can alleviate this issue. The examples presented in this notebook can\n",
    "then be easily adapted to a variety of problem settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install emukit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear multi-fidelity model\n",
    "---------------------------\n",
    "\n",
    "The linear multi-fidelity model proposed in [\\[Kennedy and O’Hagan,\n",
    "2000\\]](#3.-References) is widely viewed as a reference point for all\n",
    "such models. In this model, the high-fidelity (true) function is modeled\n",
    "as a scaled sum of the low-fidelity function plus an error term: $$\n",
    "f_{high}(x) = f_{err}(x) + \\rho \\,f_{low}(x)\n",
    "$$ In this equation, $f_{low}(x)$ is taken to be a Gaussian process\n",
    "modeling the outputs of the lower fidelity function, while $\\rho$ is a\n",
    "scaling factor indicating the magnitude of the correlation to the\n",
    "high-fidelity data. Setting this to 0 implies that there is no\n",
    "correlation between observations at different fidelities. Meanwhile,\n",
    "$f_{err}(x)$ denotes yet another Gaussian process which models the bias\n",
    "term for the high-fidelity data. Note that $f_{err}(x)$ and $f_{low}(x)$\n",
    "are assumed to be independent processes which are only related by the\n",
    "equation given above.\n",
    "\n",
    "> **Note**: While we shall limit our explanation to the case of two\n",
    "> fidelities, this set-up can easily be generalized to cater for $T$\n",
    "> fidelities as follows:\n",
    ">\n",
    "> $$f_{t}(x) = f_{t}(x) + \\rho_{t-1} \\,f_{t-1}(x), \\quad t=1,\\dots, T$$\n",
    "\n",
    "If the training points are sorted such that the low and high-fidelity\n",
    "points are grouped together: $$\n",
    "\\begin{pmatrix}\n",
    "X_{low} \\\\\n",
    "X_{high}\n",
    "\\end{pmatrix}\n",
    "$$ we can express the model as a single Gaussian process having the\n",
    "following prior. $$\n",
    "\\begin{bmatrix}\n",
    "f_{low}\\left(h\\right)\\\\\n",
    "f_{high}\\left(h\\right)\n",
    "\\end{bmatrix}\n",
    "\\sim\n",
    "GP\n",
    "\\begin{pmatrix}\n",
    "\\begin{bmatrix}\n",
    "0 \\\\ 0\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "k_{low} & \\rho k_{low} \\\\\n",
    "\\rho k_{low} & \\rho^2 k_{low} + k_{err}\n",
    "\\end{bmatrix}\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear multi-fidelity modeling in Emukit\n",
    "\n",
    "As a first example of how the linear multi-fidelity model implemented in\n",
    "`Emukit` `emukit.multi_fidelity.models.GPyLinearMultiFidelityModel` can\n",
    "be used, we shall consider the two-fidelity Forrester function. This\n",
    "benchmark is frequently used to illustrate the capabilities of\n",
    "multi-fidelity models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors as mcolors\n",
    "colors = dict(mcolors.BASE_COLORS, **mcolors.CSS4_COLORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPy\n",
    "import emukit.multi_fidelity\n",
    "import emukit.test_functions\n",
    "from emukit.model_wrappers.gpy_model_wrappers import GPyMultiOutputWrapper\n",
    "from emukit.multi_fidelity.models import GPyLinearMultiFidelityModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate samples from the Forrester function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_fidelity = emukit.test_functions.forrester.forrester\n",
    "low_fidelity = emukit.test_functions.forrester.forrester_low\n",
    "\n",
    "x_plot = np.linspace(0, 1, 200)[:, None]\n",
    "y_plot_l = low_fidelity(x_plot)\n",
    "y_plot_h = high_fidelity(x_plot)\n",
    "\n",
    "x_train_l = np.atleast_2d(np.random.rand(12)).T\n",
    "x_train_h = np.atleast_2d(np.random.permutation(x_train_l)[:6])\n",
    "y_train_l = low_fidelity(x_train_l)\n",
    "y_train_h = high_fidelity(x_train_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inputs to the models are expected to take the form of ndarrays where\n",
    "the last column indicates the fidelity of the observed points.\n",
    "\n",
    "Although only the input points, $X$, are augmented with the fidelity\n",
    "level, the observed outputs $Y$ must also be converted to array form.\n",
    "\n",
    "For example, a dataset consisting of 3 low-fidelity points and 2\n",
    "high-fidelity points would be represented as follows, where the input is\n",
    "three-dimensional while the output is one-dimensional: $$\n",
    "X = \\begin{pmatrix}\n",
    "x_{low;0}^0 & x_{low;0}^1 & x_{low;0}^2 & 0\\\\\n",
    "x_{low;1}^0 & x_{low;1}^1 & x_{low;1}^2 & 0\\\\\n",
    "x_{low;2}^0 & x_{low;2}^1 & x_{low;2}^2 & 0\\\\\n",
    "x_{high;0}^0 & x_{high;0}^1 & x_{high;0}^2 & 1\\\\\n",
    "x_{high;1}^0 & x_{high;1}^1 & x_{high;1}^2 & 1\n",
    "\\end{pmatrix}\\quad\n",
    "Y = \\begin{pmatrix}\n",
    "y_{low;0}\\\\\n",
    "y_{low;1}\\\\\n",
    "y_{low;2}\\\\\n",
    "y_{high;0}\\\\\n",
    "y_{high;1}\n",
    "\\end{pmatrix}\n",
    "$$ A similar procedure must be carried out for obtaining predictions at\n",
    "new test points, whereby the fidelity indicated in the column then\n",
    "indicates the fidelity at which the function must be predicted for a\n",
    "designated point.\n",
    "\n",
    "For convenience of use, we provide helper methods for easily converting\n",
    "between a list of arrays (ordered from the lowest to the highest\n",
    "fidelity) and the required ndarray representation. This is found in\n",
    "`emukit.multi_fidelity.convert_lists_to_array`.\n",
    "\n",
    "Convert lists of arrays to ndarrays augmented with fidelity indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emukit.multi_fidelity.convert_lists_to_array import convert_x_list_to_array, convert_xy_lists_to_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = convert_xy_lists_to_arrays([x_train_l, x_train_h], [y_train_l, y_train_h])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the original functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "\n",
    "ax.plot(x_plot, y_plot_l, 'b')\n",
    "ax.plot(x_plot, y_plot_h, 'r')\n",
    "ax.scatter(x_train_l, y_train_l, color='b', s=40)\n",
    "ax.scatter(x_train_h, y_train_h, color='r', s=40)\n",
    "ax.set_ylabel('f (x)')\n",
    "ax.set_xlabel('x')\n",
    "ax.legend(['Low fidelity', 'High fidelity'])\n",
    "\n",
    "mlai.write_figure(fig, 'high-and-low-fidelity-forrester.svg', diagrams='./uq/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/uq/high-and-low-fidelity-forrester.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>High and low fidelity Forrester functions</i>\n",
    "\n",
    "Observe that in the example above we restrict our observations to 12\n",
    "from the lower fidelity function and only 6 from the high fidelity\n",
    "function. As we shall demonstrate further below, fitting a standard GP\n",
    "model to the few high fidelity observations is unlikely to result in an\n",
    "acceptable fit, which is why we shall instead consider the linear\n",
    "multi-fidelity model presented in this section.\n",
    "\n",
    "Below we fit a linear multi-fidelity model to the available low and high\n",
    "fidelity observations. Given the smoothness of the functions, we opt to\n",
    "use an *RBF* kernel for both the bias and correlation components of the\n",
    "model.\n",
    "\n",
    "**Note**: The model implementation defaults to a `MixedNoise` noise\n",
    "likelihood whereby there is independent Gaussian noise for each\n",
    "fidelity.\n",
    "\n",
    "This can be modified upfront using the ‘likelihood’ parameter in the\n",
    "model constructor, or by updating them directly after the model has been\n",
    "created. In the example below, we choose to fix the noise to ‘0’ for\n",
    "both fidelities in order to reflect that the observations are exact.\n",
    "\n",
    "Construct a linear multi-fidelity model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = [GPy.kern.RBF(1), GPy.kern.RBF(1)]\n",
    "lin_mf_kernel = emukit.multi_fidelity.kernels.LinearMultiFidelityKernel(kernels)\n",
    "gpy_lin_mf_model = GPyLinearMultiFidelityModel(X_train, Y_train, lin_mf_kernel, n_fidelities=2)\n",
    "gpy_lin_mf_model.mixed_noise.Gaussian_noise.fix(0)\n",
    "gpy_lin_mf_model.mixed_noise.Gaussian_noise_1.fix(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap the model using the given `GPyMultiOutputWrapper`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_mf_model = model = GPyMultiOutputWrapper(gpy_lin_mf_model, 2, n_optimization_restarts=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_mf_model.optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert x\\_plot to its ndarray representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_plot = convert_x_list_to_array([x_plot, x_plot])\n",
    "X_plot_l = X_plot[:len(x_plot)]\n",
    "X_plot_h = X_plot[len(x_plot):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute mean predictions and associated variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf_mean_lin_mf_model, lf_var_lin_mf_model = lin_mf_model.predict(X_plot_l)\n",
    "lf_std_lin_mf_model = np.sqrt(lf_var_lin_mf_model)\n",
    "hf_mean_lin_mf_model, hf_var_lin_mf_model = lin_mf_model.predict(X_plot_h)\n",
    "hf_std_lin_mf_model = np.sqrt(hf_var_lin_mf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the posterior mean and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "ax.fill_between(x_plot.flatten(), (lf_mean_lin_mf_model - 1.96*lf_std_lin_mf_model).flatten(), \n",
    "                 (lf_mean_lin_mf_model + 1.96*lf_std_lin_mf_model).flatten(), facecolor='g', alpha=0.3)\n",
    "ax.fill_between(x_plot.flatten(), (hf_mean_lin_mf_model - 1.96*hf_std_lin_mf_model).flatten(), \n",
    "                 (hf_mean_lin_mf_model + 1.96*hf_std_lin_mf_model).flatten(), facecolor='y', alpha=0.3)\n",
    "\n",
    "ax.plot(x_plot, y_plot_l, 'b')\n",
    "ax.plot(x_plot, y_plot_h, 'r')\n",
    "ax.plot(x_plot, lf_mean_lin_mf_model, '--', color='g')\n",
    "ax.plot(x_plot, hf_mean_lin_mf_model, '--', color='y')\n",
    "ax.scatter(x_train_l, y_train_l, color='b', s=40)\n",
    "ax.scatter(x_train_h, y_train_h, color='r', s=40)\n",
    "ax.set_ylabel('f (x)')\n",
    "ax.set_xlabel('x')\n",
    "ax.legend(['Low Fidelity', 'High Fidelity', 'Predicted Low Fidelity', 'Predicted High Fidelity'])\n",
    "\n",
    "mlai.write_figure(fig, 'linear-multi-fidelity-model.svg', diagrams='./uq/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/uq/linear-multi-fidelity.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Linear multi-fidelity model fit to low and high fidelity\n",
    "Forrester function</i>\n",
    "\n",
    "The above plot demonstrates how the multi-fidelity model learns the\n",
    "relationship between the low and high-fidelity observations in order to\n",
    "model both of the corresponding functions.\n",
    "\n",
    "In this example, the posterior mean almost fits the true function\n",
    "exactly, while the associated uncertainty returned by the model is also\n",
    "appropriately small given the good fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison to standard GP\n",
    "\n",
    "In the absence of such a multi-fidelity model, a regular Gaussian\n",
    "process would have been fit exclusively to the high fidelity data.\n",
    "\n",
    "As illustrated in the figure below, the resulting Gaussian process\n",
    "posterior yields a much worse fit to the data than that obtained by the\n",
    "multi-fidelity model. The uncertainty estimates are also poorly\n",
    "calibrated.\n",
    "\n",
    "Create standard GP model using only high-fidelity data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = GPy.kern.RBF(1)\n",
    "high_gp_model = GPy.models.GPRegression(x_train_h, y_train_h, kernel)\n",
    "high_gp_model.Gaussian_noise.fix(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the GP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_gp_model.optimize_restarts(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute mean predictions and associated variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_mean_high_gp_model, hf_var_high_gp_model  = high_gp_model.predict(x_plot)\n",
    "hf_std_hf_gp_model = np.sqrt(hf_var_high_gp_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the posterior mean and variance for the high-fidelity GP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "\n",
    "ax.fill_between(x_plot.flatten(), (hf_mean_lin_mf_model - 1.96*hf_std_lin_mf_model).flatten(), \n",
    "                 (hf_mean_lin_mf_model + 1.96*hf_std_lin_mf_model).flatten(), facecolor='y', alpha=0.3)\n",
    "ax.fill_between(x_plot.flatten(), (hf_mean_high_gp_model - 1.96*hf_std_hf_gp_model).flatten(), \n",
    "                 (hf_mean_high_gp_model + 1.96*hf_std_hf_gp_model).flatten(), facecolor='k', alpha=0.1)\n",
    "\n",
    "ax.plot(x_plot, y_plot_h, color='r')\n",
    "ax.plot(x_plot, hf_mean_lin_mf_model, '--', color='y')\n",
    "ax.plot(x_plot, hf_mean_high_gp_model, 'k--')\n",
    "ax.scatter(x_train_h, y_train_h, color='r')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$f(x)$')\n",
    "ax.legend(['True Function', 'Linear Multi-fidelity GP', 'High fidelity GP'])\n",
    "\n",
    "mlai.write_figure(fig, 'linear-multi-fidelity-high-fidelity-gp.svg', directory='./uq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/uq/linear-multi-fidelity-high-fidelity-gp.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Comparison of linear multi-fidelity model and high fidelity\n",
    "GP</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nonlinear multi-fidelity model\n",
    "------------------------------\n",
    "\n",
    "Although the model described above works well when the mapping between\n",
    "the low and high-fidelity functions is linear, several issues may be\n",
    "encountered when this is not the case.\n",
    "\n",
    "Consider the following example, where the low and high fidelity\n",
    "functions are defined as follows: $$\n",
    "f_{low}(x) = sin(8\\pi x)\n",
    "$$\n",
    "\n",
    "$$\n",
    "f_{high}(x) = (x - \\sqrt{2}) \\, f_{low}^2\n",
    "$$\n",
    "\n",
    "Generate data for nonlinear example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_fidelity = emukit.test_functions.non_linear_sin.nonlinear_sin_high\n",
    "low_fidelity = emukit.test_functions.non_linear_sin.nonlinear_sin_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot = np.linspace(0, 1, 200)[:, None]\n",
    "y_plot_l = low_fidelity(x_plot)\n",
    "y_plot_h = high_fidelity(x_plot)\n",
    "\n",
    "n_low_fidelity_points = 50\n",
    "n_high_fidelity_points = 14\n",
    "\n",
    "x_train_l = np.linspace(0, 1, n_low_fidelity_points)[:, None]\n",
    "y_train_l = low_fidelity(x_train_l)\n",
    "\n",
    "x_train_h = x_train_l[::4, :]\n",
    "y_train_h = high_fidelity(x_train_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert lists of arrays to ND-arrays augmented with fidelity indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = convert_xy_lists_to_arrays([x_train_l, x_train_h], [y_train_l, y_train_h])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "\n",
    "ax.plot(x_plot, y_plot_l, 'b')\n",
    "ax.plot(x_plot, y_plot_h, 'r')\n",
    "ax.scatter(x_train_l, y_train_l, color='b', s=40)\n",
    "ax.scatter(x_train_h, y_train_h, color='r', s=40)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('f (x)')\n",
    "ax.set_xlim([0, 1])\n",
    "ax.legend(['Low fidelity', 'High fidelity'])\n",
    "\n",
    "mlai.write_figure(fig, 'high-and-low-fidelity-functions.svg', directory='./uq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure:\n",
    "<i></i>{<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/uq/high-and-low-fidelity-functions'.svg\" class=\"High and low fidelity functions\" width=\"80%\" style=\"vertical-align:middle;high-and-low-fidelity-functions\">\n",
    "\n",
    "In this case, the mapping between the two functions is nonlinear, as can\n",
    "be observed by plotting the high fidelity observations as a function of\n",
    "the lower fidelity observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "ax.set_ylabel('HF(x)')\n",
    "ax.set_xlabel('LF(x)')\n",
    "ax.plot(y_plot_l, y_plot_h, color=colors['purple'])\n",
    "ax.legend(['HF-LF Correlation'], loc='lower center')\n",
    "\n",
    "mlai.write_figure(fig, 'mapping-low-to-high-fidelity.svg', directory='\\writeDiagrams/uq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/uq/mapping-low-to-high-fidelity.svg\" class=\"\" width=\"80\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Mapping from low fidelity to high fidelity.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failure of linear multi-fidelity model\n",
    "\n",
    "Below we fit the linear multi-fidelity model to this new problem and\n",
    "plot the results.\n",
    "\n",
    "Construct a linear multi-fidelity model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = [GPy.kern.RBF(1), GPy.kern.RBF(1)]\n",
    "lin_mf_kernel = emukit.multi_fidelity.kernels.LinearMultiFidelityKernel(kernels)\n",
    "gpy_lin_mf_model = GPyLinearMultiFidelityModel(X_train, Y_train, lin_mf_kernel, n_fidelities=2)\n",
    "gpy_lin_mf_model.mixed_noise.Gaussian_noise.fix(0)\n",
    "gpy_lin_mf_model.mixed_noise.Gaussian_noise_1.fix(0)\n",
    "\n",
    "lin_mf_model = model = GPyMultiOutputWrapper(gpy_lin_mf_model, 2, n_optimization_restarts=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_mf_model.optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert test points to appropriate representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_plot = convert_x_list_to_array([x_plot, x_plot])\n",
    "X_plot_low = X_plot[:200]\n",
    "X_plot_high = X_plot[200:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute mean and variance predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_mean_lin_mf_model, hf_var_lin_mf_model = lin_mf_model.predict(X_plot_high)\n",
    "hf_std_lin_mf_model = np.sqrt(hf_var_lin_mf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare linear and nonlinear model fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "ax.plot(x_plot, y_plot_h, 'r')\n",
    "ax.plot(x_plot, hf_mean_lin_mf_model, '--', color='y')\n",
    "ax.scatter(x_train_h, y_train_h, color='r')\n",
    "ax.fill_between(x_plot.flatten(), (hf_mean_lin_mf_model - 1.96*hf_std_lin_mf_model).flatten(), \n",
    "                 (hf_mean_lin_mf_model + 1.96*hf_std_lin_mf_model).flatten(), color='y', alpha=0.3)\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('f (x)')\n",
    "ax.legend(['True Function', 'Linear multi-fidelity GP'], loc='lower right')\n",
    "mlai.write_figure(fig, 'linear-multi-fidelity-model-fit.svg', diagrams='./uq/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/uq/linear-multi-fidelity-model-fit.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Linear multi-fidelity model fit to high fidelity function</i>\n",
    "\n",
    "As expected, the linear multi-fidelity model was unable to capture the\n",
    "nonlinear relationship between the low and high-fidelity data.\n",
    "Consequently, the resulting fit of the true function is also poor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonlinear Multi-fidelity model\n",
    "\n",
    "In view of the deficiencies of the linear multi-fidelity model, a\n",
    "nonlinear multi-fidelity model is proposed in Perdikaris et al. (2017)\n",
    "in order to better capture these correlations. This nonlinear model is\n",
    "constructed as follows: $$ \n",
    "f_{high}(x) = \\rho( \\, f_{low}(x)) + \\delta(x) \n",
    "$$\n",
    "\n",
    "Replacing the linear scaling factor with a non-deterministic function\n",
    "results in a model which can thus capture the nonlinear relationship\n",
    "between the fidelities.\n",
    "\n",
    "This model is implemented in Emukit as\n",
    "`emukit.multi_fidelity.models.NonLinearModel`.\n",
    "\n",
    "It is defined in a sequential manner where a Gaussian process model is\n",
    "trained for every set of fidelity data available. Once again, we\n",
    "manually fix the noise parameter for each model to 0. The parameters of\n",
    "the two Gaussian processes are then optimized sequentially, starting\n",
    "from the low-fidelity.\n",
    "\n",
    "Create nonlinear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emukit.multi_fidelity.models.non_linear_multi_fidelity_model import make_non_linear_kernels, NonLinearMultiFidelityModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_kernel = GPy.kern.RBF\n",
    "kernels = make_non_linear_kernels(base_kernel, 2, X_train.shape[1] - 1)\n",
    "nonlin_mf_model = NonLinearMultiFidelityModel(X_train, Y_train, n_fidelities=2, kernels=kernels, \n",
    "                                              verbose=True, optimization_restarts=5)\n",
    "for m in nonlin_mf_model.models:\n",
    "    m.Gaussian_noise.variance.fix(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonlin_mf_model.optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compute the mean and variance predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_mean_nonlin_mf_model, hf_var_nonlin_mf_model = nonlin_mf_model.predict(X_plot_high)\n",
    "hf_std_nonlin_mf_model = np.sqrt(hf_var_nonlin_mf_model)\n",
    "\n",
    "lf_mean_nonlin_mf_model, lf_var_nonlin_mf_model = nonlin_mf_model.predict(X_plot_low)\n",
    "lf_std_nonlin_mf_model = np.sqrt(lf_var_nonlin_mf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "ax.fill_between(x_plot.flatten(), (lf_mean_nonlin_mf_model - 1.96*lf_std_nonlin_mf_model).flatten(), \n",
    "                 (lf_mean_nonlin_mf_model + 1.96*lf_std_nonlin_mf_model).flatten(), color='g', alpha=0.3)\n",
    "ax.fill_between(x_plot.flatten(), (hf_mean_nonlin_mf_model - 1.96*hf_std_nonlin_mf_model).flatten(), \n",
    "                 (hf_mean_nonlin_mf_model + 1.96*hf_std_nonlin_mf_model).flatten(), color='y', alpha=0.3)\n",
    "ax.plot(x_plot, y_plot_l, 'b')\n",
    "ax.plot(x_plot, y_plot_h, 'r')\n",
    "ax.plot(x_plot, lf_mean_nonlin_mf_model, '--', color='g')\n",
    "ax.plot(x_plot, hf_mean_nonlin_mf_model, '--', color='y')\n",
    "ax.scatter(x_train_h, y_train_h, color='r')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('f (x)')\n",
    "ax.set_xlim(0, 1)\n",
    "ax.legend(['Low Fidelity', 'High Fidelity', 'Predicted Low Fidelity', 'Predicted High Fidelity'])\n",
    "\n",
    "mlai.write_figure(fig, 'nonlinear-multi-fidelity-model-fit.svg', directory='./uq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/uq/nonlinear-multi-fidelity-model-fit.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Nonlinear multi-fidelity model fit to low and high fidelity\n",
    "functions.</i>\n",
    "\n",
    "Fitting the nonlinear fidelity model to the available data very closely\n",
    "fits the high-fidelity function while also fitting the low-fidelity\n",
    "function exactly. This is a vast improvement over the results obtained\n",
    "using the linear model. We can also confirm that the model is properly\n",
    "capturing the correlation between the low and high-fidelity observations\n",
    "by plotting the mapping learned by the model to the true mapping shown\n",
    "earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "ax.set_ylabel('HF(x)')\n",
    "ax.set_xlabel('LF(x)')\n",
    "ax.plot(y_plot_l, y_plot_h, '-', color=colors['purple'])\n",
    "ax.plot(lf_mean_nonlin_mf_model, hf_mean_nonlin_mf_model, 'k--')\n",
    "ax.legend(['True HF-LF Correlation', 'Learned HF-LF Correlation'], loc='lower center')\n",
    "\n",
    "mlai.write_figure(fig, 'mapping-low-fidelity-to-high-fidelity.svg', directory='./uq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/uq/mapping-low-fidelity-to-high-fidelity.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Mapping from low fidelity to high fidelity</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks!\n",
    "-------\n",
    "\n",
    "For more information on these subjects and more you might want to check\n",
    "the following resources.\n",
    "\n",
    "-   twitter: [@lawrennd](https://twitter.com/lawrennd)\n",
    "-   podcast: [The Talking Machines](http://thetalkingmachines.com)\n",
    "-   newspaper: [Guardian Profile\n",
    "    Page](http://www.theguardian.com/profile/neil-lawrence)\n",
    "-   blog:\n",
    "    [http://inverseprobability.com](http://inverseprobability.com/blog.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kennedy, M.C., O’Hagan, A., 2000. Predicting the output from a complex\n",
    "computer code when fast approximations are available. Biometrika 87,\n",
    "1–13.\n",
    "\n",
    "Perdikaris, P., Raissi, M., Damianou, A., Lawrence, N.D., Karnidakis,\n",
    "G.E., 2017. Nonlinear information fusion algorithms for data-efficient\n",
    "multi-fidelity modelling. Proc. R. Soc. A 473.\n",
    "<https://doi.org/10.1098/rspa.2016.0751>"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
