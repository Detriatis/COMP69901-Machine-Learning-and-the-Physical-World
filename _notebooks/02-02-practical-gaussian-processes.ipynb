{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Gaussian Processes\n",
    "\n",
    "### [Carl Henrik Ek](http://carlhenrik.com)\n",
    "\n",
    "### 2024-10-22"
   ],
   "id": "94e880ad-6355-424b-84da-d1ffe3f47748"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abstract**: Gaussian processes provide a probability measure that\n",
    "allows us to perform statistical inference over the space of functions.\n",
    "While GPs are nice as mathematical objects when we need to implement\n",
    "them in practice we often run into issues. In this worksheet we will do\n",
    "a little bit of a whirlwind tour of a couple of approaches to address\n",
    "these problems. We will look at how we can address the numerical issues\n",
    "that often appear and we will look at approximations to circumvent the\n",
    "computational cost associated with Gaussian processes. Importantly when\n",
    "continuing using these models in the course you are most likely not\n",
    "going to implement them yourself but instead use some of the many\n",
    "excellent software packages that exists. The methods that we describe\n",
    "here are going to show you how these packages implement GPs and it will\n",
    "hopefully give you an idea of the type of thinking that goes into\n",
    "implementation of machine learning models."
   ],
   "id": "7fbbfc78-18bb-447f-8b16-129e0693b4a5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "$$"
   ],
   "id": "109d975c-2b69-4d58-8efd-e1c4f0cae749"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.cell .markdown}\n",
    "\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!---->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->\n",
    "<!--\n",
    "\n",
    "-->"
   ],
   "id": "ebad55a2-bbee-4cef-bfd1-73be358d10e7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ],
   "id": "9a8ae56b-0e99-47dd-bbe5-ad9357c57762"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 22})"
   ],
   "id": "280ec45f-dbd8-4809-a233-afbc2c74f073"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--setupplotcode{import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_context('paper')\n",
    "sns.set_palette('colorblind')}-->"
   ],
   "id": "3de3990e-082b-40ab-8572-ac9649bc8d2b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## notutils\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_software/includes/notutils-software.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_software/includes/notutils-software.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "This small package is a helper package for various notebook utilities\n",
    "used below.\n",
    "\n",
    "The software can be installed using"
   ],
   "id": "8c846c66-150c-43ac-bb21-1d45abfffc31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install notutils"
   ],
   "id": "84385725-f134-46fe-84f6-5f9d1f34c551"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the command prompt where you can access your python installation.\n",
    "\n",
    "The code is also available on GitHub:\n",
    "<https://github.com/lawrennd/notutils>\n",
    "\n",
    "Once `notutils` is installed, it can be imported in the usual manner."
   ],
   "id": "ae82f475-491c-405b-b3f0-c3c40ecdcbea"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notutils"
   ],
   "id": "bc15cb8f-f616-455c-ac3e-8f44fb806dfa"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pods\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_software/includes/pods-software.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_software/includes/pods-software.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "In Sheffield we created a suite of software tools for ‘Open Data\n",
    "Science’. Open data science is an approach to sharing code, models and\n",
    "data that should make it easier for companies, health professionals and\n",
    "scientists to gain access to data science techniques.\n",
    "\n",
    "You can also check this blog post on [Open Data\n",
    "Science](http://inverseprobability.com/2014/07/01/open-data-science).\n",
    "\n",
    "The software can be installed using"
   ],
   "id": "66e76ef8-9d9c-4247-9354-31cc6e35d97e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pods"
   ],
   "id": "c1e4312f-979d-458d-9cb5-315f61f7cf31"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the command prompt where you can access your python installation.\n",
    "\n",
    "The code is also available on GitHub: <https://github.com/lawrennd/ods>\n",
    "\n",
    "Once `pods` is installed, it can be imported in the usual manner."
   ],
   "id": "7825ca34-c039-43f8-9efc-c2df80706d15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pods"
   ],
   "id": "811cd6de-00a8-4ce4-8913-d47e4faefb04"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mlai\n",
    "\n",
    "<span class=\"editsection-bracket\"\n",
    "style=\"\">\\[</span><span class=\"editsection\"\n",
    "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_software/includes/mlai-software.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_software/includes/mlai-software.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
    "\n",
    "The `mlai` software is a suite of helper functions for teaching and\n",
    "demonstrating machine learning algorithms. It was first used in the\n",
    "Machine Learning and Adaptive Intelligence course in Sheffield in 2013.\n",
    "\n",
    "The software can be installed using"
   ],
   "id": "eab0b179-fad3-4d0f-88d9-1088ded9af16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mlai"
   ],
   "id": "94c897d9-3891-4e23-b510-fec900f89ba0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the command prompt where you can access your python installation.\n",
    "\n",
    "The code is also available on GitHub: <https://github.com/lawrennd/mlai>\n",
    "\n",
    "Once `mlai` is installed, it can be imported in the usual manner."
   ],
   "id": "f1e2237c-f6bb-4bc0-b774-dda30bc31e34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai\n",
    "from mlai import plot"
   ],
   "id": "7d9c79fe-35e9-40d9-b160-aa7040f9738c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have not done any type of learning with Gaussian processes.\n",
    "Learning is the process where we adapt the parameters of the model to\n",
    "the data that we observe. For a probabilistic model the object here is\n",
    "to fit the distribution of the model such that the data has high\n",
    "likelihood under the model. You can do this at many different levels,\n",
    "you can fit the parameters of the likelihood directly to data, referred\n",
    "to as maximum likelihood but in that case you have not taken into\n",
    "account the information in the prior, i.e. you are fitting the data in a\n",
    "completely unconstrained way which is likely to lead to overfitting.\n",
    "Instead we try to marginalise out as many of the parameters as we can to\n",
    "reflect the knowledge that we have of the problem and then optimise the\n",
    "remaining ones.\n",
    "\n",
    "Remember, there will always be parameters left in a model as long as you\n",
    "place a parametrised distribution that you integrate out. For Gaussian\n",
    "processes, in most practical applications, we marginalise out the prior\n",
    "over the function values, $p(f | \\theta)$ from the likelihood to reach\n",
    "the marginal likelihood,\n",
    "\n",
    "$$p(y | X, \\theta) = \\int p(y | f)p(f | X, \\theta)d\\theta$$\n",
    "\n",
    "to reach the marginal likelihood which does not have $f$ as parameters.\n",
    "However, this distribution still has the parameters of the prior\n",
    "$\\theta$ as a dependency.[1] Learning now implies altering these\n",
    "parameters so that we find the ones that the probability of the observed\n",
    "data is maximised,\n",
    "\n",
    "$$\\theta^* = \\argmax_\\theta p(y | X, \\theta)$$\n",
    "\n",
    "Now let us do this specifically for a zero mean Gaussian process prior.\n",
    "We will focus on the zero mean setting as it is usually not the mean\n",
    "that gives us problems but the covariance function. However, if you want\n",
    "to have a parametrised mean function most of the things that we talk\n",
    "about will be the same. Given that most distributions you will ever work\n",
    "with is in the exponential class the normal approach to this is to\n",
    "maximise the log marginal likelihood. If we write this up for a Gaussian\n",
    "process it will have the following form,\n",
    "\n",
    "$$\\log p(y | X, \\theta) = \\int p(y | f)p(f | X, \\theta)d\\theta$$\n",
    "$$ = -\\frac{1}{2}y^T\\left(k(X, X + \\beta^{-1}I)\\right)^{-1}y - \\frac{1}{2}\\log \\det\\left(k(X, X) + \\beta^{-1}I\\right) - \\frac{N}{2}\\log 2\\pi$$\n",
    "\n",
    "[1] sometimes you will hear these referred to as hyper-parameters."
   ],
   "id": "a0fca232-25f0-4fd6-8316-94ae2537f22b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Stability\n",
    "\n",
    "To address the numerical issues related to the two expressions above we\n",
    "are going to exploit that the problem that we work on is actually quite\n",
    "structured. The covariance matrix is in a class of matrices that are\n",
    "called positive-definite matrices meaning they are symmetric, full rank\n",
    "and with all eigen-values positive. This we can exploit to make\n",
    "computations better conditioned. To do so we are going to use the\n",
    "Cholesky decomposition which allows us to write a positive definite\n",
    "matrix as a product of a lower-triangular matrix and its transpose,\n",
    "\n",
    "$$K = LL^T$$\n",
    "\n",
    "Let’s first look at how the decomposition can be used to compute the log\n",
    "determinant term in the marginal log-likelihood,"
   ],
   "id": "2b8bedb5-91d3-4081-bda6-14eed8a13c90"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import linalg\n",
    "\n",
    "def compute_logdet(K):\n",
    "    \"\"\"\n",
    "    Compute log determinant of matrix K using Cholesky decomposition\n",
    "    \n",
    "    Parameters:\n",
    "    K (ndarray): Positive definite matrix\n",
    "    \n",
    "    Returns:\n",
    "    float: log determinant of K\n",
    "    \"\"\"\n",
    "    L = np.linalg.cholesky(K)\n",
    "    return 2 * np.sum(np.log(np.diag(L)))"
   ],
   "id": "a313945a-62b6-4be4-8ca6-ddc06b49254f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decomposition allows us to write:\n",
    "\n",
    "$$\\log \\det K = \\log \\det(LL^T) = \\log (\\det L)^2$$\n",
    "\n",
    "Now we have to compute the determinant of the Cholesky factor $L$, this\n",
    "turns out to be very easy as the determinant of a upper/lower diagonal\n",
    "matrix is the product of the values on the diagonal,\n",
    "\n",
    "$$\\det L = \\prod_{i} \\ell_{ii}$$\n",
    "\n",
    "If we put everything together we get:\n",
    "\n",
    "$$\\log \\det K = \\log\\left(\\prod_{i} \\ell_{ii}\\right)^2 = 2\\sum_{i} \\ell_{ii}$$\n",
    "\n",
    "So the log determinant of the covariance matrix is simply the sum of the\n",
    "diagonal elements of the Cholesky factor. From our first classes in\n",
    "Computer science we know that summing lots of values of different scale\n",
    "is much better for keeping precision compared to taking the product.\n",
    "\n",
    "You can also use the Cholesky factors in order to address the term that\n",
    "includes the inverse and making this better conditioned. What we will do\n",
    "is to solve the inverse by solving two systems of linear equations, both\n",
    "who have already been made into upper and lower triangular form\n",
    "therefore being trivial to solve."
   ],
   "id": "e89c71d8-8a55-4b05-bde3-80508b8967cf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_cholesky(K, y):\n",
    "    \"\"\"\n",
    "    Solve system Kx = y using Cholesky decomposition\n",
    "    \n",
    "    Parameters:\n",
    "    K (ndarray): Positive definite matrix\n",
    "    y (ndarray): Right hand side vector\n",
    "    \n",
    "    Returns:\n",
    "    ndarray: Solution x\n",
    "    \"\"\"\n",
    "    L = np.linalg.cholesky(K)\n",
    "    # Forward substitution\n",
    "    z = linalg.solve_triangular(L, y, lower=True)\n",
    "    # Backward substitution \n",
    "    x = linalg.solve_triangular(L.T, z, lower=False)\n",
    "    return x"
   ],
   "id": "c5b92a15-55b7-40ac-8bdc-a955d271aee2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s write down how this works step by step. For solving the system\n",
    "$Ax = b$, instead of computing $x = A^{-1}b$ directly, we use the\n",
    "Cholesky decomposition $A = LL^T$ giving us:\n",
    "\n",
    "$$LL^Tx = b$$\n",
    "\n",
    "We can solve this in two steps:\n",
    "\n",
    "1.  First solve $Lz = b$ by forward substitution: $$\\begin{aligned}\n",
    "    \\ell_{1,1}z_1 &= b_1 \\\\\n",
    "    \\ell_{2,1}z_1 + \\ell_{2,2}z_2 &= b_2 \\\\\n",
    "    &\\vdots \\\\\n",
    "    \\ell_{n,1}z_1 + \\ell_{n,2}z_2 + \\ldots + \\ell_{n,n}z_n &= b_n\n",
    "    \\end{aligned}$$\n",
    "\n",
    "2.  Then solve $L^Tx = z$ by backward substitution: $$\\begin{aligned}\n",
    "    \\ell_{n,n}x_n &= z_n \\\\\n",
    "    \\ell_{n-1,n-1}x_{n-1} + \\ell_{n,n-1}x_n &= z_{n-1} \\\\\n",
    "    &\\vdots \\\\\n",
    "    \\ell_{1,1}x_1 + \\ell_{2,1}x_2 + \\ldots + \\ell_{n,1}x_n &= z_1\n",
    "    \\end{aligned}$$"
   ],
   "id": "49e5bdb1-343e-46a5-acf1-242af8780889"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate Inference\n",
    "\n",
    "To compute the marginal likelihood of the Gaussian process requires\n",
    "inverting a matrix that is the size of the data. As we have already seen\n",
    "this can be numerically tricky. It is also a very expensive process of\n",
    "cubic complexity which severely limits the size of data-sets that we can\n",
    "use."
   ],
   "id": "81176d4c-d734-4cc2-bdc6-4421a66adb05"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Inference\n",
    "\n",
    "The key idea behind variational inference is to approximate an\n",
    "intractable posterior distribution $p(x|y)$ with a simpler distribution\n",
    "$q(x)$. We do this by minimizing the KL divergence between these\n",
    "distributions."
   ],
   "id": "9cc23887-5087-4e1f-a05c-b8358b4ddaae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(q_mean, q_cov, p_mean, p_cov):\n",
    "    \"\"\"\n",
    "    Compute KL divergence between two multivariate Gaussians\n",
    "    \n",
    "    Parameters:\n",
    "    q_mean, p_mean (ndarray): Mean vectors\n",
    "    q_cov, p_cov (ndarray): Covariance matrices\n",
    "    \n",
    "    Returns:\n",
    "    float: KL(q||p)\n",
    "    \"\"\"\n",
    "    k = len(q_mean)\n",
    "    \n",
    "    # Compute inverse of p_cov\n",
    "    L = np.linalg.cholesky(p_cov)\n",
    "    p_cov_inv = linalg.solve_triangular(L.T, \n",
    "                                      linalg.solve_triangular(L, np.eye(k), \n",
    "                                                           lower=True), \n",
    "                                      lower=False)\n",
    "    \n",
    "    # Compute terms\n",
    "    trace_term = np.trace(p_cov_inv @ q_cov)\n",
    "    mu_term = (p_mean - q_mean).T @ p_cov_inv @ (p_mean - q_mean)\n",
    "    logdet_term = np.log(np.linalg.det(p_cov)) - np.log(np.linalg.det(q_cov))\n",
    "    \n",
    "    return 0.5 * (trace_term + mu_term - k + logdet_term)"
   ],
   "id": "9f4319bb-ba30-4422-83cd-d70e0d8b03c8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can derive a lower bound on the log marginal likelihood using\n",
    "Jensen’s inequality. For a convex function $f$:\n",
    "\n",
    "$$f(\\int g\\,dx) \\leq \\int f \\circ g\\,dx$$\n",
    "\n",
    "For the log function (which is concave), the inequality is reversed:\n",
    "\n",
    "$$\\log(\\int g\\,dx) \\geq \\int \\log(g)\\,dx$$\n",
    "\n",
    "Using this, we can derive the Evidence Lower BOund (ELBO):\n",
    "\n",
    "$$\\log p(y) \\geq \\mathbb{E}_{q(f)}[\\log p(y|f)] - \\text{KL}(q(f)||p(f))$$"
   ],
   "id": "066fce69-3963-4c47-8d1a-410acabe31ae"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Approximations\n",
    "\n",
    "To make GPs scalable to large datasets, we introduce inducing points - a\n",
    "smaller set of points that summarize the GP. Let’s implement this\n",
    "approach:"
   ],
   "id": "6d9c6c08-6c9d-4246-b565-3449d90a8b6f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_gp(X, y, Z, kernel, noise_var=1.0):\n",
    "    \"\"\"\n",
    "    Implement sparse GP using inducing points\n",
    "    \n",
    "    Parameters:\n",
    "    X (ndarray): Input locations (N x D)\n",
    "    y (ndarray): Observations (N,)\n",
    "    Z (ndarray): Inducing point locations (M x D)\n",
    "    kernel: Kernel function\n",
    "    noise_var: Observation noise variance\n",
    "    \n",
    "    Returns:\n",
    "    tuple: Predictive mean and variance\n",
    "    \"\"\"\n",
    "    # Compute kernel matrices\n",
    "    Kuf = kernel(Z, X)  # M x N\n",
    "    Kuu = kernel(Z, Z)  # M x M\n",
    "    \n",
    "    # Compute Cholesky of Kuu\n",
    "    L = np.linalg.cholesky(Kuu)\n",
    "    \n",
    "    # Compute intermediate terms\n",
    "    A = linalg.solve_triangular(L, Kuf, lower=True)\n",
    "    AAT = A @ A.T\n",
    "    \n",
    "    # Add noise variance\n",
    "    Qff = Kuf.T @ linalg.solve(Kuu, Kuf)\n",
    "    \n",
    "    # Compute mean and variance\n",
    "    mean = Kuf.T @ linalg.solve(Kuu + AAT/noise_var, \n",
    "                               linalg.solve(Kuu, Kuf @ y))\n",
    "    var = kernel(X, X) - Qff + \\\n",
    "          Kuf.T @ linalg.solve(Kuu + AAT/noise_var, \n",
    "                              linalg.solve(Kuu, Kuf))\n",
    "    \n",
    "    return mean, var"
   ],
   "id": "1d5697f1-7eba-4e47-be59-a5cb522d2e4b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software Packages\n",
    "\n",
    "Several excellent software packages exist for working with Gaussian\n",
    "processes. Here’s a brief example using GPyTorch:"
   ],
   "id": "e6e39d0d-407e-4ab1-98b0-7d6a6945f0fc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "import torch\n",
    "\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "def train_gp(train_x, train_y, n_iterations=100):\n",
    "    \"\"\"\n",
    "    Train a GP model using GPyTorch\n",
    "    \"\"\"\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = ExactGPModel(train_x, train_y, likelihood)\n",
    "    \n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "    \n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    \n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return model, likelihood"
   ],
   "id": "90dbb16a-175b-47c5-90b5-b3cdca674c3d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this worksheet, we’ve covered the practical aspects of implementing\n",
    "Gaussian processes:\n",
    "\n",
    "1.  Numerical stability through Cholesky decomposition\n",
    "2.  Approximate inference using variational methods\n",
    "3.  Sparse approximations for scaling to larger datasets\n",
    "4.  Practical implementation using modern software packages\n",
    "\n",
    "While GPs provide an elegant mathematical framework, making them work in\n",
    "practice requires careful attention to computational details. The\n",
    "methods we’ve discussed here form the basis of modern GP\n",
    "implementations."
   ],
   "id": "feeec9e7-6a59-43b5-8dd2-edbd932d90a1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thanks!\n",
    "\n",
    "For more information on these subjects and more you might want to check\n",
    "the following resources.\n",
    "\n",
    "-   book: [The Atomic\n",
    "    Human](https://www.penguin.co.uk/books/455130/the-atomic-human-by-lawrence-neil-d/9780241625248)\n",
    "-   twitter: [@lawrennd](https://twitter.com/lawrennd)\n",
    "-   podcast: [The Talking Machines](http://thetalkingmachines.com)\n",
    "-   newspaper: [Guardian Profile\n",
    "    Page](http://www.theguardian.com/profile/neil-lawrence)\n",
    "-   blog:\n",
    "    [http://inverseprobability.com](http://inverseprobability.com/blog.html)"
   ],
   "id": "872653ac-afb4-4d7c-ba3c-f33b4c92d584"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ],
   "id": "295c0c16-6cd5-458b-8b2d-8595320d8765"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
