{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sensitivity Analysis\n",
    "====================\n",
    "\n",
    "### [Neil D. Lawrence](http://inverseprobability.com)\n",
    "\n",
    "### 2020-11-05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abstract**: This week we introduce sensitivity analysis through\n",
    "Emukit, showing how Emukit can deliver Sobol indices for understanding\n",
    "how the output of the system is affected by different inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\tk}[1]{}\n",
    "\\newcommand{\\Amatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\KL}[2]{\\text{KL}\\left( #1\\,\\|\\,#2 \\right)}\n",
    "\\newcommand{\\Kaast}{\\kernelMatrix_{\\mathbf{ \\ast}\\mathbf{ \\ast}}}\n",
    "\\newcommand{\\Kastu}{\\kernelMatrix_{\\mathbf{ \\ast} \\inducingVector}}\n",
    "\\newcommand{\\Kff}{\\kernelMatrix_{\\mappingFunctionVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\Kfu}{\\kernelMatrix_{\\mappingFunctionVector \\inducingVector}}\n",
    "\\newcommand{\\Kuast}{\\kernelMatrix_{\\inducingVector \\bf\\ast}}\n",
    "\\newcommand{\\Kuf}{\\kernelMatrix_{\\inducingVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\Kuu}{\\kernelMatrix_{\\inducingVector \\inducingVector}}\n",
    "\\newcommand{\\Kuui}{\\Kuu^{-1}}\n",
    "\\newcommand{\\Qaast}{\\mathbf{Q}_{\\bf \\ast \\ast}}\n",
    "\\newcommand{\\Qastf}{\\mathbf{Q}_{\\ast \\mappingFunction}}\n",
    "\\newcommand{\\Qfast}{\\mathbf{Q}_{\\mappingFunctionVector \\bf \\ast}}\n",
    "\\newcommand{\\Qff}{\\mathbf{Q}_{\\mappingFunctionVector \\mappingFunctionVector}}\n",
    "\\newcommand{\\aMatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\aScalar}{a}\n",
    "\\newcommand{\\aVector}{\\mathbf{a}}\n",
    "\\newcommand{\\acceleration}{a}\n",
    "\\newcommand{\\bMatrix}{\\mathbf{B}}\n",
    "\\newcommand{\\bScalar}{b}\n",
    "\\newcommand{\\bVector}{\\mathbf{b}}\n",
    "\\newcommand{\\basisFunc}{\\phi}\n",
    "\\newcommand{\\basisFuncVector}{\\boldsymbol{ \\basisFunc}}\n",
    "\\newcommand{\\basisFunction}{\\phi}\n",
    "\\newcommand{\\basisLocation}{\\mu}\n",
    "\\newcommand{\\basisMatrix}{\\boldsymbol{ \\Phi}}\n",
    "\\newcommand{\\basisScalar}{\\basisFunction}\n",
    "\\newcommand{\\basisVector}{\\boldsymbol{ \\basisFunction}}\n",
    "\\newcommand{\\activationFunction}{\\phi}\n",
    "\\newcommand{\\activationMatrix}{\\boldsymbol{ \\Phi}}\n",
    "\\newcommand{\\activationScalar}{\\basisFunction}\n",
    "\\newcommand{\\activationVector}{\\boldsymbol{ \\basisFunction}}\n",
    "\\newcommand{\\bigO}{\\mathcal{O}}\n",
    "\\newcommand{\\binomProb}{\\pi}\n",
    "\\newcommand{\\cMatrix}{\\mathbf{C}}\n",
    "\\newcommand{\\cbasisMatrix}{\\hat{\\boldsymbol{ \\Phi}}}\n",
    "\\newcommand{\\cdataMatrix}{\\hat{\\dataMatrix}}\n",
    "\\newcommand{\\cdataScalar}{\\hat{\\dataScalar}}\n",
    "\\newcommand{\\cdataVector}{\\hat{\\dataVector}}\n",
    "\\newcommand{\\centeredKernelMatrix}{\\mathbf{ \\MakeUppercase{\\centeredKernelScalar}}}\n",
    "\\newcommand{\\centeredKernelScalar}{b}\n",
    "\\newcommand{\\centeredKernelVector}{\\centeredKernelScalar}\n",
    "\\newcommand{\\centeringMatrix}{\\mathbf{H}}\n",
    "\\newcommand{\\chiSquaredDist}[2]{\\chi_{#1}^{2}\\left(#2\\right)}\n",
    "\\newcommand{\\chiSquaredSamp}[1]{\\chi_{#1}^{2}}\n",
    "\\newcommand{\\conditionalCovariance}{\\boldsymbol{ \\Sigma}}\n",
    "\\newcommand{\\coregionalizationMatrix}{\\mathbf{B}}\n",
    "\\newcommand{\\coregionalizationScalar}{b}\n",
    "\\newcommand{\\coregionalizationVector}{\\mathbf{ \\coregionalizationScalar}}\n",
    "\\newcommand{\\covDist}[2]{\\text{cov}_{#2}\\left(#1\\right)}\n",
    "\\newcommand{\\covSamp}[1]{\\text{cov}\\left(#1\\right)}\n",
    "\\newcommand{\\covarianceScalar}{c}\n",
    "\\newcommand{\\covarianceVector}{\\mathbf{ \\covarianceScalar}}\n",
    "\\newcommand{\\covarianceMatrix}{\\mathbf{C}}\n",
    "\\newcommand{\\covarianceMatrixTwo}{\\boldsymbol{ \\Sigma}}\n",
    "\\newcommand{\\croupierScalar}{s}\n",
    "\\newcommand{\\croupierVector}{\\mathbf{ \\croupierScalar}}\n",
    "\\newcommand{\\croupierMatrix}{\\mathbf{ \\MakeUppercase{\\croupierScalar}}}\n",
    "\\newcommand{\\dataDim}{p}\n",
    "\\newcommand{\\dataIndex}{i}\n",
    "\\newcommand{\\dataIndexTwo}{j}\n",
    "\\newcommand{\\dataMatrix}{\\mathbf{Y}}\n",
    "\\newcommand{\\dataScalar}{y}\n",
    "\\newcommand{\\dataSet}{\\mathcal{D}}\n",
    "\\newcommand{\\dataStd}{\\sigma}\n",
    "\\newcommand{\\dataVector}{\\mathbf{ \\dataScalar}}\n",
    "\\newcommand{\\decayRate}{d}\n",
    "\\newcommand{\\degreeMatrix}{\\mathbf{ \\MakeUppercase{\\degreeScalar}}}\n",
    "\\newcommand{\\degreeScalar}{d}\n",
    "\\newcommand{\\degreeVector}{\\mathbf{ \\degreeScalar}}\n",
    "\\newcommand{\\diag}[1]{\\text{diag}\\left(#1\\right)}\n",
    "\\newcommand{\\diagonalMatrix}{\\mathbf{D}}\n",
    "\\newcommand{\\diff}[2]{\\frac{\\text{d}#1}{\\text{d}#2}}\n",
    "\\newcommand{\\diffTwo}[2]{\\frac{\\text{d}^2#1}{\\text{d}#2^2}}\n",
    "\\newcommand{\\displacement}{x}\n",
    "\\newcommand{\\displacementVector}{\\textbf{\\displacement}}\n",
    "\\newcommand{\\distanceMatrix}{\\mathbf{ \\MakeUppercase{\\distanceScalar}}}\n",
    "\\newcommand{\\distanceScalar}{d}\n",
    "\\newcommand{\\distanceVector}{\\mathbf{ \\distanceScalar}}\n",
    "\\newcommand{\\eigenvaltwo}{\\ell}\n",
    "\\newcommand{\\eigenvaltwoMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\eigenvaltwoVector}{\\mathbf{l}}\n",
    "\\newcommand{\\eigenvalue}{\\lambda}\n",
    "\\newcommand{\\eigenvalueMatrix}{\\boldsymbol{ \\Lambda}}\n",
    "\\newcommand{\\eigenvalueVector}{\\boldsymbol{ \\lambda}}\n",
    "\\newcommand{\\eigenvector}{\\mathbf{ \\eigenvectorScalar}}\n",
    "\\newcommand{\\eigenvectorMatrix}{\\mathbf{U}}\n",
    "\\newcommand{\\eigenvectorScalar}{u}\n",
    "\\newcommand{\\eigenvectwo}{\\mathbf{v}}\n",
    "\\newcommand{\\eigenvectwoMatrix}{\\mathbf{V}}\n",
    "\\newcommand{\\eigenvectwoScalar}{v}\n",
    "\\newcommand{\\entropy}[1]{\\mathcal{H}\\left(#1\\right)}\n",
    "\\newcommand{\\errorFunction}{E}\n",
    "\\newcommand{\\expDist}[2]{\\left<#1\\right>_{#2}}\n",
    "\\newcommand{\\expSamp}[1]{\\left<#1\\right>}\n",
    "\\newcommand{\\expectation}[1]{\\left\\langle #1 \\right\\rangle }\n",
    "\\newcommand{\\expectationDist}[2]{\\left\\langle #1 \\right\\rangle _{#2}}\n",
    "\\newcommand{\\expectedDistanceMatrix}{\\mathcal{D}}\n",
    "\\newcommand{\\eye}{\\mathbf{I}}\n",
    "\\newcommand{\\fantasyDim}{r}\n",
    "\\newcommand{\\fantasyMatrix}{\\mathbf{ \\MakeUppercase{\\fantasyScalar}}}\n",
    "\\newcommand{\\fantasyScalar}{z}\n",
    "\\newcommand{\\fantasyVector}{\\mathbf{ \\fantasyScalar}}\n",
    "\\newcommand{\\featureStd}{\\varsigma}\n",
    "\\newcommand{\\gammaCdf}[3]{\\mathcal{GAMMA CDF}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gammaDist}[3]{\\mathcal{G}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gammaSamp}[2]{\\mathcal{G}\\left(#1,#2\\right)}\n",
    "\\newcommand{\\gaussianDist}[3]{\\mathcal{N}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\gaussianSamp}[2]{\\mathcal{N}\\left(#1,#2\\right)}\n",
    "\\newcommand{\\uniformDist}[3]{\\mathcal{U}\\left(#1|#2,#3\\right)}\n",
    "\\newcommand{\\uniformSamp}[2]{\\mathcal{U}\\left(#1,#2\\right)}\n",
    "\\newcommand{\\given}{|}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\newcommand{\\heaviside}{H}\n",
    "\\newcommand{\\hiddenMatrix}{\\mathbf{ \\MakeUppercase{\\hiddenScalar}}}\n",
    "\\newcommand{\\hiddenScalar}{h}\n",
    "\\newcommand{\\hiddenVector}{\\mathbf{ \\hiddenScalar}}\n",
    "\\newcommand{\\identityMatrix}{\\eye}\n",
    "\\newcommand{\\inducingInputScalar}{z}\n",
    "\\newcommand{\\inducingInputVector}{\\mathbf{ \\inducingInputScalar}}\n",
    "\\newcommand{\\inducingInputMatrix}{\\mathbf{Z}}\n",
    "\\newcommand{\\inducingScalar}{u}\n",
    "\\newcommand{\\inducingVector}{\\mathbf{ \\inducingScalar}}\n",
    "\\newcommand{\\inducingMatrix}{\\mathbf{U}}\n",
    "\\newcommand{\\inlineDiff}[2]{\\text{d}#1/\\text{d}#2}\n",
    "\\newcommand{\\inputDim}{q}\n",
    "\\newcommand{\\inputMatrix}{\\mathbf{X}}\n",
    "\\newcommand{\\inputScalar}{x}\n",
    "\\newcommand{\\inputSpace}{\\mathcal{X}}\n",
    "\\newcommand{\\inputVals}{\\inputVector}\n",
    "\\newcommand{\\inputVector}{\\mathbf{ \\inputScalar}}\n",
    "\\newcommand{\\iterNum}{k}\n",
    "\\newcommand{\\kernel}{\\kernelScalar}\n",
    "\\newcommand{\\kernelMatrix}{\\mathbf{K}}\n",
    "\\newcommand{\\kernelScalar}{k}\n",
    "\\newcommand{\\kernelVector}{\\mathbf{ \\kernelScalar}}\n",
    "\\newcommand{\\kff}{\\kernelScalar_{\\mappingFunction \\mappingFunction}}\n",
    "\\newcommand{\\kfu}{\\kernelVector_{\\mappingFunction \\inducingScalar}}\n",
    "\\newcommand{\\kuf}{\\kernelVector_{\\inducingScalar \\mappingFunction}}\n",
    "\\newcommand{\\kuu}{\\kernelVector_{\\inducingScalar \\inducingScalar}}\n",
    "\\newcommand{\\lagrangeMultiplier}{\\lambda}\n",
    "\\newcommand{\\lagrangeMultiplierMatrix}{\\boldsymbol{ \\Lambda}}\n",
    "\\newcommand{\\lagrangian}{L}\n",
    "\\newcommand{\\laplacianFactor}{\\mathbf{ \\MakeUppercase{\\laplacianFactorScalar}}}\n",
    "\\newcommand{\\laplacianFactorScalar}{m}\n",
    "\\newcommand{\\laplacianFactorVector}{\\mathbf{ \\laplacianFactorScalar}}\n",
    "\\newcommand{\\laplacianMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\laplacianScalar}{\\ell}\n",
    "\\newcommand{\\laplacianVector}{\\mathbf{ \\ell}}\n",
    "\\newcommand{\\latentDim}{q}\n",
    "\\newcommand{\\latentDistanceMatrix}{\\boldsymbol{ \\Delta}}\n",
    "\\newcommand{\\latentDistanceScalar}{\\delta}\n",
    "\\newcommand{\\latentDistanceVector}{\\boldsymbol{ \\delta}}\n",
    "\\newcommand{\\latentForce}{f}\n",
    "\\newcommand{\\latentFunction}{u}\n",
    "\\newcommand{\\latentFunctionVector}{\\mathbf{ \\latentFunction}}\n",
    "\\newcommand{\\latentFunctionMatrix}{\\mathbf{ \\MakeUppercase{\\latentFunction}}}\n",
    "\\newcommand{\\latentIndex}{j}\n",
    "\\newcommand{\\latentScalar}{z}\n",
    "\\newcommand{\\latentVector}{\\mathbf{ \\latentScalar}}\n",
    "\\newcommand{\\latentMatrix}{\\mathbf{Z}}\n",
    "\\newcommand{\\learnRate}{\\eta}\n",
    "\\newcommand{\\lengthScale}{\\ell}\n",
    "\\newcommand{\\rbfWidth}{\\ell}\n",
    "\\newcommand{\\likelihoodBound}{\\mathcal{L}}\n",
    "\\newcommand{\\likelihoodFunction}{L}\n",
    "\\newcommand{\\locationScalar}{\\mu}\n",
    "\\newcommand{\\locationVector}{\\boldsymbol{ \\locationScalar}}\n",
    "\\newcommand{\\locationMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\variance}[1]{\\text{var}\\left( #1 \\right)}\n",
    "\\newcommand{\\mappingFunction}{f}\n",
    "\\newcommand{\\mappingFunctionMatrix}{\\mathbf{F}}\n",
    "\\newcommand{\\mappingFunctionTwo}{g}\n",
    "\\newcommand{\\mappingFunctionTwoMatrix}{\\mathbf{G}}\n",
    "\\newcommand{\\mappingFunctionTwoVector}{\\mathbf{ \\mappingFunctionTwo}}\n",
    "\\newcommand{\\mappingFunctionVector}{\\mathbf{ \\mappingFunction}}\n",
    "\\newcommand{\\scaleScalar}{s}\n",
    "\\newcommand{\\mappingScalar}{w}\n",
    "\\newcommand{\\mappingVector}{\\mathbf{ \\mappingScalar}}\n",
    "\\newcommand{\\mappingMatrix}{\\mathbf{W}}\n",
    "\\newcommand{\\mappingScalarTwo}{v}\n",
    "\\newcommand{\\mappingVectorTwo}{\\mathbf{ \\mappingScalarTwo}}\n",
    "\\newcommand{\\mappingMatrixTwo}{\\mathbf{V}}\n",
    "\\newcommand{\\maxIters}{K}\n",
    "\\newcommand{\\meanMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\meanScalar}{\\mu}\n",
    "\\newcommand{\\meanTwoMatrix}{\\mathbf{M}}\n",
    "\\newcommand{\\meanTwoScalar}{m}\n",
    "\\newcommand{\\meanTwoVector}{\\mathbf{ \\meanTwoScalar}}\n",
    "\\newcommand{\\meanVector}{\\boldsymbol{ \\meanScalar}}\n",
    "\\newcommand{\\mrnaConcentration}{m}\n",
    "\\newcommand{\\naturalFrequency}{\\omega}\n",
    "\\newcommand{\\neighborhood}[1]{\\mathcal{N}\\left( #1 \\right)}\n",
    "\\newcommand{\\neilurl}{http://inverseprobability.com/}\n",
    "\\newcommand{\\noiseMatrix}{\\boldsymbol{ E}}\n",
    "\\newcommand{\\noiseScalar}{\\epsilon}\n",
    "\\newcommand{\\noiseVector}{\\boldsymbol{ \\epsilon}}\n",
    "\\newcommand{\\norm}[1]{\\left\\Vert #1 \\right\\Vert}\n",
    "\\newcommand{\\normalizedLaplacianMatrix}{\\hat{\\mathbf{L}}}\n",
    "\\newcommand{\\normalizedLaplacianScalar}{\\hat{\\ell}}\n",
    "\\newcommand{\\normalizedLaplacianVector}{\\hat{\\mathbf{ \\ell}}}\n",
    "\\newcommand{\\numActive}{m}\n",
    "\\newcommand{\\numBasisFunc}{m}\n",
    "\\newcommand{\\numComponents}{m}\n",
    "\\newcommand{\\numComps}{K}\n",
    "\\newcommand{\\numData}{n}\n",
    "\\newcommand{\\numFeatures}{K}\n",
    "\\newcommand{\\numHidden}{h}\n",
    "\\newcommand{\\numInducing}{m}\n",
    "\\newcommand{\\numLayers}{\\ell}\n",
    "\\newcommand{\\numNeighbors}{K}\n",
    "\\newcommand{\\numSequences}{s}\n",
    "\\newcommand{\\numSuccess}{s}\n",
    "\\newcommand{\\numTasks}{m}\n",
    "\\newcommand{\\numTime}{T}\n",
    "\\newcommand{\\numTrials}{S}\n",
    "\\newcommand{\\outputIndex}{j}\n",
    "\\newcommand{\\paramVector}{\\boldsymbol{ \\theta}}\n",
    "\\newcommand{\\parameterMatrix}{\\boldsymbol{ \\Theta}}\n",
    "\\newcommand{\\parameterScalar}{\\theta}\n",
    "\\newcommand{\\parameterVector}{\\boldsymbol{ \\parameterScalar}}\n",
    "\\newcommand{\\partDiff}[2]{\\frac{\\partial#1}{\\partial#2}}\n",
    "\\newcommand{\\precisionScalar}{j}\n",
    "\\newcommand{\\precisionVector}{\\mathbf{ \\precisionScalar}}\n",
    "\\newcommand{\\precisionMatrix}{\\mathbf{J}}\n",
    "\\newcommand{\\pseudotargetScalar}{\\widetilde{y}}\n",
    "\\newcommand{\\pseudotargetVector}{\\mathbf{ \\pseudotargetScalar}}\n",
    "\\newcommand{\\pseudotargetMatrix}{\\mathbf{ \\widetilde{Y}}}\n",
    "\\newcommand{\\rank}[1]{\\text{rank}\\left(#1\\right)}\n",
    "\\newcommand{\\rayleighDist}[2]{\\mathcal{R}\\left(#1|#2\\right)}\n",
    "\\newcommand{\\rayleighSamp}[1]{\\mathcal{R}\\left(#1\\right)}\n",
    "\\newcommand{\\responsibility}{r}\n",
    "\\newcommand{\\rotationScalar}{r}\n",
    "\\newcommand{\\rotationVector}{\\mathbf{ \\rotationScalar}}\n",
    "\\newcommand{\\rotationMatrix}{\\mathbf{R}}\n",
    "\\newcommand{\\sampleCovScalar}{s}\n",
    "\\newcommand{\\sampleCovVector}{\\mathbf{ \\sampleCovScalar}}\n",
    "\\newcommand{\\sampleCovMatrix}{\\mathbf{s}}\n",
    "\\newcommand{\\scalarProduct}[2]{\\left\\langle{#1},{#2}\\right\\rangle}\n",
    "\\newcommand{\\sign}[1]{\\text{sign}\\left(#1\\right)}\n",
    "\\newcommand{\\sigmoid}[1]{\\sigma\\left(#1\\right)}\n",
    "\\newcommand{\\singularvalue}{\\ell}\n",
    "\\newcommand{\\singularvalueMatrix}{\\mathbf{L}}\n",
    "\\newcommand{\\singularvalueVector}{\\mathbf{l}}\n",
    "\\newcommand{\\sorth}{\\mathbf{u}}\n",
    "\\newcommand{\\spar}{\\lambda}\n",
    "\\newcommand{\\trace}[1]{\\text{tr}\\left(#1\\right)}\n",
    "\\newcommand{\\BasalRate}{B}\n",
    "\\newcommand{\\DampingCoefficient}{C}\n",
    "\\newcommand{\\DecayRate}{D}\n",
    "\\newcommand{\\Displacement}{X}\n",
    "\\newcommand{\\LatentForce}{F}\n",
    "\\newcommand{\\Mass}{M}\n",
    "\\newcommand{\\Sensitivity}{S}\n",
    "\\newcommand{\\basalRate}{b}\n",
    "\\newcommand{\\dampingCoefficient}{c}\n",
    "\\newcommand{\\mass}{m}\n",
    "\\newcommand{\\sensitivity}{s}\n",
    "\\newcommand{\\springScalar}{\\kappa}\n",
    "\\newcommand{\\springVector}{\\boldsymbol{ \\kappa}}\n",
    "\\newcommand{\\springMatrix}{\\boldsymbol{ \\mathcal{K}}}\n",
    "\\newcommand{\\tfConcentration}{p}\n",
    "\\newcommand{\\tfDecayRate}{\\delta}\n",
    "\\newcommand{\\tfMrnaConcentration}{f}\n",
    "\\newcommand{\\tfVector}{\\mathbf{ \\tfConcentration}}\n",
    "\\newcommand{\\velocity}{v}\n",
    "\\newcommand{\\sufficientStatsScalar}{g}\n",
    "\\newcommand{\\sufficientStatsVector}{\\mathbf{ \\sufficientStatsScalar}}\n",
    "\\newcommand{\\sufficientStatsMatrix}{\\mathbf{G}}\n",
    "\\newcommand{\\switchScalar}{s}\n",
    "\\newcommand{\\switchVector}{\\mathbf{ \\switchScalar}}\n",
    "\\newcommand{\\switchMatrix}{\\mathbf{S}}\n",
    "\\newcommand{\\tr}[1]{\\text{tr}\\left(#1\\right)}\n",
    "\\newcommand{\\loneNorm}[1]{\\left\\Vert #1 \\right\\Vert_1}\n",
    "\\newcommand{\\ltwoNorm}[1]{\\left\\Vert #1 \\right\\Vert_2}\n",
    "\\newcommand{\\onenorm}[1]{\\left\\vert#1\\right\\vert_1}\n",
    "\\newcommand{\\twonorm}[1]{\\left\\Vert #1 \\right\\Vert}\n",
    "\\newcommand{\\vScalar}{v}\n",
    "\\newcommand{\\vVector}{\\mathbf{v}}\n",
    "\\newcommand{\\vMatrix}{\\mathbf{V}}\n",
    "\\newcommand{\\varianceDist}[2]{\\text{var}_{#2}\\left( #1 \\right)}\n",
    "\\newcommand{\\vecb}[1]{\\left(#1\\right):}\n",
    "\\newcommand{\\weightScalar}{w}\n",
    "\\newcommand{\\weightVector}{\\mathbf{ \\weightScalar}}\n",
    "\\newcommand{\\weightMatrix}{\\mathbf{W}}\n",
    "\\newcommand{\\weightedAdjacencyMatrix}{\\mathbf{A}}\n",
    "\\newcommand{\\weightedAdjacencyScalar}{a}\n",
    "\\newcommand{\\weightedAdjacencyVector}{\\mathbf{ \\weightedAdjacencyScalar}}\n",
    "\\newcommand{\\onesVector}{\\mathbf{1}}\n",
    "\\newcommand{\\zerosVector}{\\mathbf{0}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!---->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- Do not edit this file locally. -->\n",
    "<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->\n",
    "<!--\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install emukit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emukit Sensitivity Analysis\n",
    "---------------------------\n",
    "\n",
    "This introduction is based on [Introduction to Global Sensitivity\n",
    "Analysis with\n",
    "Emukit](https://github.com/EmuKit/emukit/blob/master/notebooks/Emukit-tutorial-sensitivity-montecarlo.ipynb)\n",
    "written by Mark Pullin, Javier Gonzalez, Juan Emmanuel Johnson and\n",
    "Andrei Paleyes. Some references include (Kennedy and O’Hagan, 2000;\n",
    "Saltelli et al., 2010, 2008, 2004; Sobol, 2001, 1990)\n",
    "\n",
    "> A possible definition of sensitivity analysis is the following: The\n",
    "> study of how uncertainty in the output of a model (numerical or\n",
    "> otherwise) can be apportioned to different sources of uncertainty in\n",
    "> the model input (Saltelli et al., 2004). A related practice is\n",
    "> ‘uncertainty analysis’, which focuses rather on quantifying\n",
    "> uncertainty in model output. Ideally, uncertainty and sensitivity\n",
    "> analyses should be run in tandem, with uncertainty analysis preceding\n",
    "> in current practice.\n",
    ">\n",
    "> In Chapter 1 of Saltelli et al. (2008)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import colors as mcolors\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve('https://raw.githubusercontent.com/lawrennd/talks/gh-pages/mlai.py','mlai.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve('https://raw.githubusercontent.com/lawrennd/talks/gh-pages/teaching_plots.py','teaching_plots.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve('https://raw.githubusercontent.com/lawrennd/talks/gh-pages/gp_tutorial.py','gp_tutorial.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pyDOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlai\n",
    "import teaching_plots as plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sensitivity analysis is a statistical technique widely used to test the\n",
    "reliability of real systems. Imagine a simulator of taxis picking up\n",
    "customers in a city like the one showed in the [Emukit\n",
    "playground](https://github.com/amzn/emukit-playground). The profit of\n",
    "the taxi company depends on factors like the number of taxis on the road\n",
    "and the price per trip. In this example, a global sensitivity analysis\n",
    "of the simulator could be useful to decompose the variance of the profit\n",
    "in a way that can be assigned to the input variables of the simulator.\n",
    "\n",
    "There are different ways of doing a sensitivity analysis of the\n",
    "variables of a simulator. In this notebook we will start with an\n",
    "approach based on Monte Carlo sampling that is useful when evaluating\n",
    "the simulator is cheap. If evaluating the simulator is expensive,\n",
    "emulators can then be used to speed up computations. We will show this\n",
    "in the last part of the notebook. Next, we start with a few formal\n",
    "definitions and literature review so we can understand the basics of\n",
    "Sensitivity Analysis and how it can performed with Emukit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local Sensitivity\n",
    "-----------------\n",
    "\n",
    "Given any function, $g(\\cdot)$, we might be interested in how sensitive\n",
    "that function is to variations in its input space. One route to\n",
    "determining this is to compute the partial derivatives of that function\n",
    "with respect to its inputs, $$\n",
    "\\frac{\\partial}{\\partial x_i} g(\\mathbf{ x}).\n",
    "$$ The matrix of all these partial derivatives is known as the Jacobian.\n",
    "\n",
    "These types of local sensitivity analysis can be used for determining\n",
    "the effect of changing an input variable around an operating point. But\n",
    "they don’t give us an understanding of the response of the target\n",
    "function to variations in the input across the domain of inputs. For\n",
    "this, we need to look to *global sensitivity analysis*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global Sensitivity Analysis\n",
    "---------------------------\n",
    "\n",
    "In global sensitivity analysis, rather than looking around a single\n",
    "operating point, we’re interested in the overall sensitivity of a\n",
    "function to its inputs, or combinations of inputs, across its entire\n",
    "domain. The key tool in determining this sensitivity is known as the\n",
    "ANOVA decomposition, or the *Hoeffding-Sobol decomposition*.\n",
    "\n",
    "For global sensitivity analysis, we need to make an assumption about how\n",
    "are inputs are going to vary to create different values of the function.\n",
    "The fundamental object we’re interested in is the total variance of the\n",
    "function, $$\n",
    "\\text{var}\\left(g(\\mathbf{ x})\\right) = \\left\\langle g(\\mathbf{ x})^2 \\right\\rangle _{p(\\mathbf{ x})} - \\left\\langle g(\\mathbf{ x}) \\right\\rangle _{p(\\mathbf{ x})}^2\n",
    "$$ where $$\n",
    "\\left\\langle h(\\mathbf{ x}) \\right\\rangle _{p(\\mathbf{ x})} = \\int_\\mathbf{ x}h(\\mathbf{ x}) p(\\mathbf{ x}) \\text{d}\\mathbf{ x}\n",
    "$$ is the expectation of the function $h(\\mathbf{ x})$ under the density\n",
    "$p(\\mathbf{ x})$, which represents the probability distribution of\n",
    "inputs we’re interested in.\n",
    "\n",
    "The total variance of the function gives us the overal variation of the\n",
    "function across the domain of inputs, as represented by the probability\n",
    "density, $p(\\mathbf{ x})$. Normally, we perform analysis by assuming\n",
    "that, $$\n",
    "p(\\mathbf{ x}) = \\prod_{i=1}^pp(x_i)\n",
    "$$ and that each $p(x_i)$ is *uniformly distributed* across its input\n",
    "domain. Assuming we scale the input domain down to the interval\n",
    "$[0, 1]$, that gives us $$\n",
    "x_i \\sim \\mathcal{U}\\left(0,1\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hoeffding-Sobol Decomposition\n",
    "-----------------------------\n",
    "\n",
    "The Hoeffding-Sobol, or ANOVA, decomposition of a function allows us to\n",
    "write it as, $$\n",
    "\\begin{align*}\n",
    "g(\\mathbf{ x}) = & g_0 + \\sum_{i=1}^pg_i(x_i) + \\sum_{i<j}^{p} g_{ij}(x_i,x_j) + \\cdots \\\\\n",
    "& + g_{1,2,\\dots,p}(x_1,x_2,\\dots,x_p),\n",
    "\\end{align*}\n",
    "$$ where $$\n",
    "g_0 = \\left\\langle g(\\mathbf{ x}) \\right\\rangle _{p(\\mathbf{ x})}\n",
    "$$ and $$\n",
    "g_i(x_i) = \\left\\langle g(\\mathbf{ x}) \\right\\rangle _{p(\\mathbf{ x}_{\\sim i})} - g_0,\n",
    "$$ where we’re using the notation $p(\\mathbf{ x}_{\\sim i})$ to represent\n",
    "the input distribution with the $i$th variable marginalised, $$\n",
    "p(\\mathbf{ x}_{\\sim i}) = \\int p(\\mathbf{ x}) \\text{d}x_i\n",
    "$$ Higher order terms in the decomposition represent interactions\n",
    "between inputs, $$\n",
    "g_{i,j}(x_i, x_j) = \\left\\langle g(\\mathbf{ x}) \\right\\rangle _{p(\\mathbf{ x}_{\\sim i,j})} - g_i(x_i) - g_j(x_j) - g_0\n",
    "$$ and similar expressions can be written for higher order terms up to\n",
    "$g_{1,2,\\dots,p}(\\mathbf{ x})$.\n",
    "\n",
    "Note that to compute each of these individual terms, you need to first\n",
    "compute the low order terms, and then compute the high order terms. This\n",
    "can be problematic when $p$ is large.\n",
    "\n",
    "We’re interested in the variance of the function $g$, so implicitly\n",
    "we’re assuming that the square of this function is integrable across its\n",
    "domain, i.e. we’re assuming that\n",
    "$\\left\\langle g(\\mathbf{ x})^2 \\right\\rangle _{p(\\mathbf{ x})}$ exists\n",
    "and is finite.\n",
    "\n",
    "The Sobol decomposition has some important properties, in particular, it\n",
    "components are orthogonal, so this means that when we substitute it in\n",
    "to the variance, we have, $$\n",
    "\\begin{align*}\n",
    "\\text{var}(g) = & \\left\\langle g(\\mathbf{ x})^2  \\right\\rangle _{p(\\mathbf{ x})} - \\left\\langle g(\\mathbf{ x}) \\right\\rangle _{p(\\mathbf{ x})}^2 \\\\\n",
    " = & \\left\\langle g(\\mathbf{ x})^2  \\right\\rangle _{p(\\mathbf{ x})} - g_0^2\\\\\n",
    " = & \\sum_{i=1}^p\\text{var}\\left(g_i(x_i)\\right) + \\sum_{i<j}^{p} \\text{var}\\left(g_{ij}(x_i,x_j)\\right) & + \\cdots + \\text{var}\\left(g_{1,2,\\dots,p}(x_1,x_2,\\dots,x_p)\\right).\n",
    "\\end{align*}\n",
    "$$ So, this decomposition gives us a decomposition of the function in\n",
    "terms of variances. It’s for this reason that it’s sometimes known as an\n",
    "ANOVA decomposition. ANOVA stands a for *analysis of variance*. The\n",
    "ANOVA decomposition decomposes the function into additive variance parts\n",
    "that are each stemming from interactions between different inputs.\n",
    "\n",
    "As is common in various analyses of variance, we can rescale the\n",
    "components with the *total variance* of the function. These rescaled\n",
    "components are known as *Sobol indicies*. $$\n",
    "S_\\ell = \\frac{\\text{var}\\left(g(\\mathbf{ x}_\\ell))\\right)}{\\text{var}\\left(g(\\mathbf{ x})\\right)},\n",
    "$$ where the $\\ell$ represents the relevent set of indices for the\n",
    "different combinations of inputs.\n",
    "\n",
    "In practice, For an elegant approach that exploits a particular\n",
    "covariance function structure to perform global sensitivity analysis see\n",
    "Durrande et al. (2013)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: the Ishigami function\n",
    "==============================\n",
    "\n",
    "We illustrate the exact calculation of the Sobol indices with the three\n",
    "dimensional Ishigami function of (Ishigami and Homma, 1989)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ishigami Function\n",
    "\n",
    "The Ishigami function (Ishigami and Homma, 1989) is a well-known example\n",
    "for uncertainty and sensitivity analysis methods because of its strong\n",
    "nonlinearity and peculiar dependence on $x_3$. More details of this\n",
    "function can be found in (Sobol and Levitan, 1999).\n",
    "\n",
    "Mathematically, the form of the Ishigami function is $$\n",
    "g(\\textbf{x}) = \\sin(x_1) + a \\sin^2(x_2) + b x_3^4 \\sin(x_1). \n",
    "$$ We will set the parameters to be $a = 5$ and $b=0.1$ . The input\n",
    "variables are sampled randomly $x_i \\sim \\text{Uniform}(-\\pi,\\pi)$.\n",
    "\n",
    "Next we create the function object and visualize its shape marginally\n",
    "for each one of its three inputs.\n",
    "\n",
    "Load the Ishigami function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emukit.test_functions.sensitivity import Ishigami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ishigami = Ishigami(a=5, b=0.1)\n",
    "target_function = ishigami.fidelity1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That gives us the target function, next we define the input space for\n",
    "the simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from emukit.core import ContinuousParameter, ParameterSpace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_domain = (-np.pi,np.pi)\n",
    "           \n",
    "space = ParameterSpace(\n",
    "          [ContinuousParameter('x1', *variable_domain), \n",
    "           ContinuousParameter('x2', *variable_domain),\n",
    "           ContinuousParameter('x3', *variable_domain)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving to any further analysis, we first plot the non-zero\n",
    "components $g(\\mathbf{ x})$. These components are $$\n",
    "\\begin{align*}\n",
    "g_1(x_1) & = \\sin(x_1) \\\\\n",
    "g_2(x_1) & = a \\sin^2 (x_2) \\\\\n",
    "g_{13}(x_1,x_3) & = b x_3^4 \\sin(x_1) \n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_grid = np.linspace(*variable_domain,100)\n",
    "target_simulator = ishigami.fidelity1\n",
    "f1 = ishigami.f1(x_grid)\n",
    "f2 = ishigami.f2(x_grid)\n",
    "F13 = ishigami.f13(np.array([x_grid,x_grid]).T)[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=plot.big_wide_figsize)\n",
    "gs = axs[1, 1].get_gridspec()\n",
    "for ax in axs[1, 0:]:\n",
    "    ax.remove()\n",
    "\n",
    "ax2 = fig.add_subplot(gs[1, 0:], projection='3d')\n",
    "\n",
    "axs[0,0].plot(x_grid, f1,'-r')\n",
    "axs[0,0].set_xlabel('$x_1$')\n",
    "axs[0,0].set_ylabel('$f_1$')\n",
    "\n",
    "axs[0,1].plot(x_grid,f2,'-r')\n",
    "axs[0,1].set_xlabel('$x_2$')\n",
    "axs[0,1].set_ylabel('$f_2$')\n",
    "\n",
    "X, Y = np.meshgrid(x_grid, x_grid)\n",
    "surf = ax2.plot_surface(X, Y, F13, cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "\n",
    "ax2.set_xlabel('$x_1$')\n",
    "ax2.set_ylabel('$x_3$')\n",
    "ax2.set_zlabel('$f_{13}$')\n",
    "\n",
    "mlai.write_figure(filename='non-zero-sobol-ishigami.svg', directory='./uq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/uq/non-zero-sobol-ishigami.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The non-zero components of the Ishigami function.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total Variance\n",
    "--------------\n",
    "\n",
    "The total variance $\\text{var}(y)$ in this example is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ishigami.variance_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is the sum of the variance of $\\text{var}\\left(g(x_1)\\right)$,\n",
    "$\\text{var}\\left(g(x_2)\\right)$ and $\\text{var}\\left(g(x_{1,3})\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ishigami.variance_x1, ishigami.variance_x2, ishigami.variance_x13)\n",
    "print(ishigami.variance_x1 + ishigami.variance_x2 + ishigami.variance_x13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Order Sobol Indices using Monte Carlo\n",
    "-------------------------------------------\n",
    "\n",
    "The first order Sobol indices are a measure of “first order sensitivity”\n",
    "of each input variable. They account for the proportion of variance of\n",
    "$y$ explained by changing each variable alone while marginalizing over\n",
    "the rest. Recall that the Sobol index of the $i$th variable is computed\n",
    "as $$\n",
    "S_i = \\frac{\\text{var}\\left(g(x_i)\\right)}{\\text{var}(g(\\mathbf{ x})}.\n",
    "$$ This value is standardized using the total variance so it is possible\n",
    "to account for a fractional contribution of each variable to the total\n",
    "variance of the output.\n",
    "\n",
    "The Sobol indices for higher order interactions $S_{i,j}$ are computed\n",
    "similarly. Due to the normalization by the total variance, the the sum\n",
    "of all Sobol indices equals to one.\n",
    "\n",
    "In most cases we are interested in the first order indices. The Ishigami\n",
    "function has the benefit that these can be computed analytically. In\n",
    "`EmuKit` you can extract these values with the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ishigami.main_effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But in general these indices need to be sampled using Monte Carlo or one\n",
    "of the quasi-Monte Carlo methods we’ve seen in the model-free\n",
    "experimental design. Details are given in (Sobol, 2001).\n",
    "\n",
    "With Emukit, the first-order Sobol indices can be easily computed. We\n",
    "first need to define the space where of target simulator is analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emukit.sensitivity.monte_carlo import ModelFreeMonteCarloSensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)  # for reproducibility\n",
    "\n",
    "num_monte_carlo_points = 10000  # Number of MC samples\n",
    "senstivity_ishigami = ModelFreeMonteCarloSensitivity(target_simulator, space)\n",
    "main_effects, total_effects, _ = senstivity_ishigami.compute_effects(num_monte_carlo_points = num_monte_carlo_points)\n",
    "print(main_effects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare the true effects with the Monte Carlo effects in a bar-plot.\n",
    "The total effects are discussed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "\n",
    "d = {'Sobol True': ishigami.main_effects,\n",
    "     'Monte Carlo': main_effects}\n",
    "\n",
    "pd.DataFrame(d).plot(kind='bar', ax=ax)\n",
    "ax.set_title('First-order Sobol indices - Ishigami')\n",
    "ax.set_ylabel('% of explained output variance')\n",
    "\n",
    "mlai.write_figure(filename='first-order-sobol-indices-ishigami.svg', directory='./uq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/uq/first-order-sobol-indices-ishigami.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The non-zero components of the Ishigami function.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total Effects Using Monte Carlo\n",
    "-------------------------------\n",
    "\n",
    "Computing high order sensitivity indices can be computationally very\n",
    "demanding in high dimensional scenarios and measuring the total\n",
    "influence of each variable on the variance of the output is infeasible.\n",
    "To solve this issue the *total* indices are used which account for the\n",
    "contribution to the output variance of $x_i$ including all variance\n",
    "caused by the variable alone and all its interactions of any order.\n",
    "\n",
    "The total effect for $x_i$ is given by: $$ \n",
    "S_{Ti} = \\frac{E_{\\mathbf{ x}_{\\sim i}} \\left(\\text{var}_{x_i} (y\\mid \\mathbf{ x}_{\\sim i}) \\right)}{\\text{var}(y)} = 1 - \\frac{\\text{var}_{\\mathbf{ x}_{\\sim i}} \\left(E_{x_i} (y\\mid \\mathbf{ x}_{\\sim i}) \\right)}{\\text{var}(y)}\n",
    "$$\n",
    "\n",
    "Note that the sum of $S_{Ti}$ is not necessarily one in this case unless\n",
    "the model is additive. In the Ishigami example the value of the total\n",
    "effects is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ishigami.total_effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous example, the total effects can be computed with Monte\n",
    "Carlo. In the next plot we show the comparison with the true total\n",
    "effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "\n",
    "d = {'Sobol True': ishigami.total_effects,\n",
    "     'Monte Carlo': total_effects}\n",
    "\n",
    "pd.DataFrame(d).plot(kind='bar', ax=ax)\n",
    "ax.set_title('Total effects - Ishigami')\n",
    "ax.set_ylabel('Effects value')\n",
    "\n",
    "mlai.write_figure(filename='total-effects-ishigami.svg', directory='./uq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/uq/total-effects-ishigami.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>The total effects from the Ishigami function as computed via\n",
    "Monte Carlo estimate alongside the true total effects for the Ishigami\n",
    "function.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the sensitivity indices using the output of a model\n",
    "-------------------------------------------------------------\n",
    "\n",
    "In the example used above the Ishigami function is very cheap to\n",
    "evaluate. However, in most real scenarios the functions of interest are\n",
    "expensive and we need to limit ourselves to a few number of evaluations.\n",
    "Using Monte Carlo methods is infeasible in these scenarios as a large\n",
    "number of samples are typically required to provide good estimates of\n",
    "the Sobol indices.\n",
    "\n",
    "An alternative in these cases is to use Gaussaian process emulator of\n",
    "the function of interest trained on a few inputs and outputs (Marrel et\n",
    "al., 2009). If the model is properly trained, its mean prediction which\n",
    "is cheap to evaluate, can be used to compute the Monte Carlo estimates\n",
    "of the Sobol indices, the variance from the GP emulator can also be used\n",
    "to assess our uncertainty about the Sobol indices. Let’s see how we can\n",
    "do this in Emukit.\n",
    "\n",
    "We start by generating 100 samples in the input domain. Note that this a\n",
    "just 1% of the number of samples that we used to compute the Sobol\n",
    "coefficients using Monte Carlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emukit.core.initial_designs import RandomDesign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desing = RandomDesign(space)\n",
    "x = desing.get_samples(500)\n",
    "y = ishigami.fidelity1(x)[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we fit a standard Gaussian process to the samples and we wrap it as\n",
    "an Emukit model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPy.models import GPRegression\n",
    "from emukit.model_wrappers import GPyModelWrapper\n",
    "from emukit.sensitivity.monte_carlo import MonteCarloSensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gpy = GPRegression(x,y)\n",
    "model_emukit = GPyModelWrapper(model_gpy)\n",
    "model_emukit.optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to compute the coefficients using the class\n",
    "`ModelBasedMonteCarloSensitivity` which directly calls the model and\n",
    "uses its predictive mean to compute the Monte Carlo estimates of the\n",
    "Sobol indices. We plot the true estimates, those computed using 10000\n",
    "direct evaluations of the objecte using Monte Carlo and those computed\n",
    "using a Gaussian process model trained on 100 evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senstivity_ishigami_gpbased = MonteCarloSensitivity(model = model_emukit, input_domain = space)\n",
    "main_effects_gp, total_effects_gp, _ = senstivity_ishigami_gpbased.compute_effects(num_monte_carlo_points = num_mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "\n",
    "main_effects_gp = {ivar: main_effects_gp[ivar][0] for ivar in main_effects_gp}\n",
    "\n",
    "d = {'Sobol True': ishigami.main_effects,\n",
    "     'Monte Carlo': main_effects,\n",
    "     'GP Monte Carlo':main_effects_gp}\n",
    "\n",
    "pd.DataFrame(d).plot(kind='bar', ax=ax)\n",
    "plt.title('First-order Sobol indices - Ishigami')\n",
    "plt.ylabel('% of explained output variance')\n",
    "\n",
    "mlai.write_figure(filename='first-order-sobol-indices-gp-ishigami.svg', directory='./uq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/uq/first-order-sobol-indices-gp-ishigami.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>First Order sobol indices as estimated by Monte Carlo and\n",
    "GP-emulator based Monte Carlo.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "\n",
    "total_effects_gp = {ivar: total_effects_gp[ivar][0] for ivar in total_effects_gp}\n",
    "\n",
    "d = {'Sobol True': ishigami.total_effects,\n",
    "     'Monte Carlo': total_effects,\n",
    "     'GP Monte Carlo':total_effects_gp}\n",
    "\n",
    "pd.DataFrame(d).plot(kind='bar', ax=ax)\n",
    "ax.set_title('Total effects - Ishigami')\n",
    "ax.set_ylabel('% of explained output variance')\n",
    "\n",
    "mlai.write_figure(filename='total-effects-sobol-indices-gp-ishigami.svg', directory='./uq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/uq/total-effects-sobol-indices-gp-ishigami.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Total effects as estimated by Monte Carlo and GP based Monte\n",
    "Carlo.</i>\n",
    "\n",
    "We observe some discrepacies with respect to the real value of the Sobol\n",
    "index when using the Gaussian process but we get a fairly good a\n",
    "approximation a very reduced number of evaluations of the original\n",
    "target function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions\n",
    "-----------\n",
    "\n",
    "The Sobol indices are a tool for explaining the variance of the output\n",
    "of a function as components of the input variables. Monte Carlo is an\n",
    "approach for computing these indices if the function is cheap to\n",
    "evaluate. Other approaches are needed when $g(\\cdot)$ is expensive to\n",
    "compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Catapult Simulation\n",
    "-------------------\n",
    "\n",
    "<svg viewBox=\"0 0 200 200\" style=\"width:15%\">\n",
    "\n",
    "<defs> <clipPath id=\"clip0\">\n",
    "\n",
    "<style>\n",
    "circle {\n",
    "  fill: black;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<circle cx=\"100\" cy=\"100\" r=\"100\"/> </clipPath> </defs>\n",
    "\n",
    "<title>\n",
    "\n",
    "Nicolas Durrande\n",
    "\n",
    "</title>\n",
    "\n",
    "<image preserveAspectRatio=\"xMinYMin slice\" width=\"100%\" xlink:href=\"../slides/diagrams/people/nicolas-Durrande2.jpg\" clip-path=\"url(#clip0)\"/>\n",
    "\n",
    "</svg>\n",
    "\n",
    "As a worked example we’re going to introduce a catapult simulation\n",
    "written by Nicolas Durrande,\n",
    "<a href=\"https://durrande.shinyapps.io/catapult/\" class=\"uri\">https://durrande.shinyapps.io/catapult/</a>.\n",
    "\n",
    "<img class=\"\" src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/uq/catapult-simulation.png\" style=\"width:80%\">\n",
    "\n",
    "Figure: <i>A catapult simulation for experimenting with surrogate\n",
    "models, kindly provided by Nicolas Durrande</i>\n",
    "\n",
    "The simulator allows you to set various parameters of the catapult\n",
    "including the axis of rotation, `roation_axis`, the position of the arm\n",
    "stop, `arm_stop`, and the location of the two bindings of the catapult’s\n",
    "spring, `spring_binding_1` and `spring_binding_2`.\n",
    "\n",
    "These parameters are then collated in a vector, $$\n",
    "\\mathbf{ x}_i = \\begin{bmatrix}\n",
    "\\texttt{rotation_axis} \\\\\n",
    "\\texttt{arm_stop} \\\\\n",
    "\\texttt{spring_binding_1} \\\\\n",
    "\\texttt{spring_binding_2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Having set those parameters, you can run an experiment, by firing the\n",
    "catapult. This will show you how far it goes.\n",
    "\n",
    "Because you will need to operate the catapult yourself, we’ll create a\n",
    "function to query you about the result of an individual firing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catapult_distance(x):\n",
    "    \"\"\"Request user input for the catapult.\"\"\"\n",
    "    y = np.zeros((x.shape[0], 1))\n",
    "    for i in range(x.shape[0]):\n",
    "        rotation_axis=x[i, 0]\n",
    "        arm_stop=x[i, 1]\n",
    "        spring_binding_1=x[i, 2]\n",
    "        spring_binding_2=x[i, 3]\n",
    "            \n",
    "        print('Please set the following values:')\n",
    "        print('x_1 = {rotation_axis:.2f} (rotation axis)'.format(rotation_axis=rotation_axis))\n",
    "        print('x_2 = {arm_stop:.2f} (arm stop)'.format(arm_stop=arm_stop))\n",
    "        print('x_3 = {spring_binding_1:.2f} (spring binding 1)'.format(spring_binding_1=spring_binding_1))\n",
    "        print('x_4 = {spring_binding_2:.2f} (spring binding 2)'.format(spring_binding_2=spring_binding_2))\n",
    "        y[i, 0] = float(input('What is the distance? '))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also set the parameter space for the model. Each of these\n",
    "variables is scaled to operate $\\in [0, 1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emukit.core import ContinuousParameter, ParameterSpace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_domain = [0,1]\n",
    "           \n",
    "space = ParameterSpace(\n",
    "          [ContinuousParameter('rotation_axis', *variable_domain), \n",
    "           ContinuousParameter('arm_stop', *variable_domain),\n",
    "           ContinuousParameter('spring_binding_1', *variable_domain),\n",
    "           ContinuousParameter('spring_binding_2', *variable_domain)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we perform sensitivity analysis, we need to build an emulator of\n",
    "the catapulter, which we do using our experimental design process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimental Design for the Catapult\n",
    "------------------------------------\n",
    "\n",
    "Now we will build an emulator for the catapult using the experimental\n",
    "design loop.\n",
    "\n",
    "We’ll start with a small model-free design, we’ll use a random design\n",
    "for initializing our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emukit.core.initial_designs import RandomDesign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "design = RandomDesign(space)\n",
    "x = design.get_samples(5)\n",
    "y = catapult_distance(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPy.models import GPRegression\n",
    "from emukit.model_wrappers import GPyModelWrapper\n",
    "from emukit.sensitivity.monte_carlo import MonteCarloSensitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the GPy model. The variance of the RBF kernel is set to $150^2$\n",
    "because that’s roughly the square of the range of the catapult. We set\n",
    "the noise variance to a small value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gpy = GPRegression(x,y)\n",
    "model_gpy.kern.variance = 150**2\n",
    "model_gpy.likelihood.variance.fix(1e-5)\n",
    "display(model_gpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap the model for EmuKit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_emukit = GPyModelWrapper(model_gpy)\n",
    "model_emukit.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(model_gpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set up the model loop. We’ll use integrated variance reduction as\n",
    "the acquisition function for our model-based design loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emukit.experimental_design.experimental_design_loop import ExperimentalDesignLoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emukit.experimental_design.acquisitions import IntegratedVarianceReduction, ModelVariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integrated_variance = IntegratedVarianceReduction(space=space,\n",
    "                                                  model=model_emukit)\n",
    "ed = ExperimentalDesignLoop(space=space, \n",
    "                            model=model_emukit, \n",
    "                            acquisition = integrated_variance)\n",
    "ed.run_loop(catapult_distance, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sensitivity Analysis of a Catapult Simulation\n",
    "---------------------------------------------\n",
    "\n",
    "The final step is to compute the coefficients using the class\n",
    "`ModelBasedMonteCarloSensitivity` which directly calls the model and\n",
    "uses its predictive mean to compute the Monte Carlo estimates of the\n",
    "Sobol indices. We plot the true estimates, those computed using 10000\n",
    "direct evaluations of the objecte using Monte Carlo and those computed\n",
    "using a Gaussian process model trained on 100 evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_mc = 10000\n",
    "senstivity = MonteCarloSensitivity(model = model_emukit, input_domain = space)\n",
    "main_effects_gp, total_effects_gp, _ = senstivity.compute_effects(num_monte_carlo_points = num_mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import teaching_plots as plot\n",
    "import mlai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "\n",
    "main_effects_gp_plot = {ivar: main_effects_gp[ivar][0] for ivar in main_effects_gp}\n",
    "\n",
    "d = {'GP Monte Carlo':main_effects_gp_plot}\n",
    "\n",
    "pd.DataFrame(d).plot(kind='bar', ax=ax)\n",
    "plt.ylabel('% of explained output variance')\n",
    "\n",
    "mlai.write_figure(filename='first-order-sobol-indices-gp-catapult.svg', directory='./uq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/uq/first-order-sobol-indices-gp-catapult.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>First Order sobol indices as estimated by GP-emulator based\n",
    "Monte Carlo on the catapult.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
    "\n",
    "total_effects_gp_plot = {ivar: total_effects_gp[ivar][0] for ivar in total_effects_gp}\n",
    "\n",
    "d = {'GP Monte Carlo':total_effects_gp_plot}\n",
    "\n",
    "pd.DataFrame(d).plot(kind='bar', ax=ax)\n",
    "ax.set_ylabel('% of explained output variance')\n",
    "\n",
    "mlai.write_figure(filename='total-effects-sobol-indices-gp-catapult.svg', directory='./uq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://inverseprobability.com/talks/slides/../slides/diagrams/uq/total-effects-sobol-indices-gp-catapult.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
    "\n",
    "Figure: <i>Total effects as estimated by GP based Monte Carlo on the\n",
    "catapult.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks!\n",
    "-------\n",
    "\n",
    "For more information on these subjects and more you might want to check\n",
    "the following resources.\n",
    "\n",
    "-   twitter: [@lawrennd](https://twitter.com/lawrennd)\n",
    "-   podcast: [The Talking Machines](http://thetalkingmachines.com)\n",
    "-   newspaper: [Guardian Profile\n",
    "    Page](http://www.theguardian.com/profile/neil-lawrence)\n",
    "-   blog:\n",
    "    [http://inverseprobability.com](http://inverseprobability.com/blog.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durrande, N., Ginsbourger, D., Olivier, Carraro, L., 2013. ANOVA kernels\n",
    "and RKHS of zero mean functions for model-based sensitivity analysis.\n",
    "Journal of Multivariate Analysis 115, 57–67.\n",
    "<https://doi.org/https://doi.org/10.1016/j.jmva.2012.08.016>\n",
    "\n",
    "Ishigami, T., Homma, T., 1989. An importance quantification technique in\n",
    "uncertainty analysis for computer models. \\[1990\\] Proceedings. First\n",
    "International Symposium on Uncertainty Modeling and Analysis 398–403.\n",
    "\n",
    "Kennedy, M.C., O’Hagan, A., 2000. Predicting the output from a complex\n",
    "computer code when fast approximations are available. Biometrika 87,\n",
    "1–13.\n",
    "\n",
    "Marrel, A., Iooss, B., Laurent, B., Roustant, O., 2009. Calculations of\n",
    "Sobol indices for the Gaussian process metamodel. Reliability\n",
    "Engineering & System Safety 94, 742–751.\n",
    "<https://doi.org/https://doi.org/10.1016/j.ress.2008.07.008>\n",
    "\n",
    "Saltelli, A., Annoni, P., Azzini, I., Campolongo, F., Ratto, M.,\n",
    "Tarantola, S., 2010. Variance based sensitivity analysis of model\n",
    "output. Design and estimator for the total sensitivity index. Computer\n",
    "Physics Communications 181, 259–270.\n",
    "<https://doi.org/10.1016/j.cpc.2009.09.018>\n",
    "\n",
    "Saltelli, A., Ratto, M., Andres, T., Campolongo, F., Cariboni, J.,\n",
    "Gatelli, D., Saisana, M., Tarantola, S., 2008. Global sensitivity\n",
    "analysis: The primer. wiley.\n",
    "\n",
    "Saltelli, A., Tarantola, S., Campolongo, F., Ratto, M., 2004.\n",
    "Sensitivity analysis in practice: A guide to assessing scientific\n",
    "methods. wiley.\n",
    "\n",
    "Sobol, I.M., 2001. Global sensitivity indices for nonlinear mathematical\n",
    "models and their Monte Carlo estimates. Mathematics and Computers in\n",
    "Simulation 55, 271–280. <https://doi.org/10.1016/S0378-4754(00)00270-6>\n",
    "\n",
    "Sobol, I.M., 1990. On sensitivity estimation for nonlinear mathematical\n",
    "models. Matematicheskoe Modelirovanie 2, 112–118.\n",
    "\n",
    "Sobol, I.M., Levitan, Y.L., 1999. On the use of variance reducing\n",
    "multipliers in Monte Carlo computations of a global sensitivity index.\n",
    "Computer Physics Communications 117, 52–61.\n",
    "<https://doi.org/10.1016/S0010-4655(98)00156-8>"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
