---
title: "Practical 1"
author:
- given: Carl Henrik
  family: Ek
  url: http://carlhenrik.com
  institute: University of Cambridge
  twitter: 
  gscholar: 
  orcid: 
edit_url: https://github.com/lawrennd/talks/edit/gh-pages/_mlphysical/practical-one.md
date: 2022-10-27
published: 2022-10-27
week: 0
featured_image: assets/images/practical-one.png
transition: None
ipynb: 01-practical-one.ipynb
layout: practical
categories:
- notes
---



<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!--

-->
<h1 id="sequential-decision-making">Sequential Decision Making</h1>
<p>This notebook will form part of your individual submission for the
course. The notebook will roughly mimick the parts that are in the PDF
worksheet. Your task is to complete the code that is missing in the
parts below and answer the questions that we ask. The aim is not for you
to solve the worksheet but rather for you to show your understanding of
the material in the course, instead of re-running and aiming to get
“perfect” results run things, make sure it is correct and then try to
explain your results with a few sentences.</p>
<p>First we need to implement the surrogate model, we will use a
Gaussian process surrogate.</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> norm</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.spatial.distance <span class="im">import</span> cdist</span></code></pre></div>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(X, noise<span class="op">=</span><span class="fl">0.0</span>):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>(<span class="op">-</span>np.sin(<span class="dv">3</span><span class="op">*</span>X) <span class="op">-</span> X<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="fl">0.7</span><span class="op">*</span>X <span class="op">+</span> noise<span class="op">*</span>np.random.randn(<span class="op">*</span>X.shape))</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> squared_exponential_computer(x1,x2,theta):</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># compute the squared exponential covariance function</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> K</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gpposterior(x_star,X,Y,lengthScale,sigma):</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># return the posterior estimate of the GP</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mu, varSigma</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>theta <span class="op">=</span> np.zeros((<span class="dv">2</span>, ))</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>theta[<span class="dv">0</span>] <span class="op">=</span> <span class="fl">0.3</span>  <span class="co"># lengthscale</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>theta[<span class="dv">1</span>] <span class="op">=</span> <span class="fl">1.0</span><span class="op">;</span> <span class="co"># variance</span></span></code></pre></div>
<p>When we have the surrogate model up and running we need to implement
the acquisition function.</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> expected_improvement(f_star, mu, varSigma):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># return the value of the acquisition function at each</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> alpha</span></code></pre></div>
<p>Once you have the above code up and running you should be able to
reproduce the results that are in Figure 1 in the worksheet. Now lets
try to run some additional experiments. First lets evaluate the effect
of the initial start locations. Create a plot where on the x-axis have
the number of times you have evaluated the true function and on the
y-axis have the current minima, run the optimisation loop several times
and plot the mean and two standard deviations for the minimal value at
each iteration.</p>
<h3 id="exercise-1">Exercise 1</h3>
<p>implement a loop that tries a set of random-restarts</p>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, n_starts):</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, n_evals):</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Your code here</span></span></code></pre></div>
<p>{Explain why the plots looks this way? Does it make a difference how
many initial points you start with?}</p>
<p>While the function have a local minima so it presents some challenges
for optimisation it is still quite easy to find the minima. Let us try
make the function a bit more challenging by adding a bit of noise to the
function.</p>
<h3 id="exercise-2">Exercise 2</h3>
<p>Implement an additional loop around the previous two loops which
alters the amount of noise added.</p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># implement a loop that tries different noise-levels</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="dv">10</span>):</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> f(x, noise[i])</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, n_starts):</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, n_evals):</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Your code here</span></span></code></pre></div>
<h3 id="exercise-3">Exercise 3</h3>
<p>Explain the results by contrasting to the previous none-noisy
evaluation. How does the “best” run compare to the “best” run in the
previous example?</p>
<p>As you have probably noticed the kernel-hyperparmeters have a huge
effect on the results. This is a desirable effect as this is where we
encode our knowledge of the function. We will now do one experiment
where we will alter the lengthscale value and see how it effects the
results.</p>
<h2 id="extra">Extra</h2>
<p>For a final extra experiment try to fit the kernel parameters inside
the inner-loop. The way to do this is to maximise the marginal
likelihood of the surrogate model using gradient descent. You can alter
the numpy code that we have implemented to jax instead and which will
allow you to use auto-differetiation to compute gradients. Now you can
implement a simple gradient descent</p>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> jax <span class="im">import</span> grad</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.numpy <span class="im">as</span> jnp</span></code></pre></div>
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> squared_exponential(x1, x2, theta):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># theta[0] - variance</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># theta[1] - lengthscale</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> x2 <span class="op">==</span> <span class="va">None</span>:</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> theta[<span class="dv">0</span>]<span class="op">*</span>jnp.exp(<span class="op">-</span>cdist(x1, x1, metric<span class="op">=</span><span class="st">&#39;sqeuclidean&#39;</span>)<span class="op">/</span>theta[<span class="dv">1</span>]<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> theta[<span class="dv">0</span>]<span class="op">*</span>jnp.exp(<span class="op">-</span>cdist(x1, x2, metric<span class="op">=</span><span class="st">&#39;sqeuclidean&#39;</span>)<span class="op">/</span>theta[<span class="dv">1</span>]<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> logmarginal_likelihood(x, y, theta):</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># implement the log-marginal likelihood of a GP</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>dLdtheta <span class="op">=</span> grad(logmarginal_likelihood, argnums<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">-=</span> dLdtheta(w) <span class="op">*</span> <span class="fl">0.01</span></span></code></pre></div>
<h1 id="submission">Submission</h1>
<p>You can submit the notebook on Moodle. Name your notebook using your
CRSid as <code>crsid_practical-one.ipynb</code> before submitting to
Moodle.</p>
<p>The deadline for the submission is Friday the 4th of November at
23:59.</p>

