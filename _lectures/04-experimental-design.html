---
title: "Emulation and Experimental Design"
venue: "Virtual (Zoom)"
abstract: "<p>This lecture will review the ideas behind using surrogate models for experimental design.</p>"
author:
- given: Neil D.
  family: Lawrence
  url: http://inverseprobability.com
  institute: 
  twitter: lawrennd
  gscholar: r3SJcvoAAAAJ
  orchid: 
date: 2020-10-29
published: 2020-10-29
week: 4
reveal: 04-experimental-design.slides.html
ipynb: 04-experimental-design.ipynb
layout: lecture
categories:
- notes
---



<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!--

-->
<h1 id="emulation">Emulation</h1>
<h2 id="experiment-analyze-design">Experiment, Analyze, Design</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_supply-chain/includes/experiment-analyze-design.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_supply-chain/includes/experiment-analyze-design.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>One thing about working in an industrial environment, is the way that short term thinking actions become important. For example, in Formula One, the teams are working on a two week cycle to digest information from the previous week’s race and incorporate updates to the car or their strategy.</p>
<p>However, businesses have to also think about more medium term horizons. For example, in Formula 1 you need to worry about next year’s car. So while you’re working on updating this year’s car, you also need to think about what will happen for next year and prioritise these conflicting needs appropriately.</p>
<p>In the Amazon supply chain, there are the equivalent demands. If we accept that an artificial intelligence is just an automated decision making system. And if we measure in terms of money automatically spent, or goods automatically moved, then Amazon’s buying system is perhaps the world’s largest AI.</p>
<p>Those decisions are being made on short time schedules, purchases are made by the system on weekly cycles. But just as in Formula 1, there is also a need to think about what needs to be done next month, next quarter and next year. Planning meetings are held not only on a weekly basis (known as weekly business reviews), but monthly, quarterly and then yearly meetings for planning spends and investments.</p>
<p>Amazon is known for being longer term thinking than many companies, and a lot of this is coming from the CEO. One quote from Jeff Bezos that stuck with me was the following.</p>
<blockquote>
<p>“I very frequently get the question: ‘What’s going to change in the next 10 years?’ And that is a very interesting question; it’s a very common one. I almost never get the question: ‘What’s not going to change in the next 10 years?’ And I submit to you that that second question is actually the more important of the two – because you can build a business strategy around the things that are stable in time. … [I]n our retail business, we know that customers want low prices, and I know that’s going to be true 10 years from now. They want fast delivery; they want vast selection. It’s impossible to imagine a future 10 years from now where a customer comes up and says, ‘Jeff I love Amazon; I just wish the prices were a little higher,’ [or] ‘I love Amazon; I just wish you’d deliver a little more slowly.’ Impossible. And so the effort we put into those things, spinning those things up, we know the energy we put into it today will still be paying off dividends for our customers 10 years from now. When you have something that you know is true, even over the long term, you can afford to put a lot of energy into it.”</p>
</blockquote>
<p>This quote is incredibly important for long term thinking. Indeed, it’s a failure of many of our simulations that they focus on what is going to happen, not what will not happen. In Amazon, this meant that there was constant focus on these three areas, keeping costs low, making delivery fast and improving selection. For example, shortly before I left Amazon moved its entire US network from 2 day delivery to 1 day delivery. This involves changing the way the entire buying system operates. Or, more recently, the company has had to radically change the portfolio of products it buys in the face of Covid19.</p>
<!--These challenges are not just there for Amazon and Formula 1. In Sheffield, we worked closely with a Chesterfield based company called Fusion Group. They make joints that fuse PTFE pipes together. These pipes are used for transporting both water and gas. Their founder, Eric Bridgstock, was an engineer who introduced PTFE piping to the UK when working for DuPont. Eric set up Fusion group to manufacture the fusion fittings. Because PTFE pipes carry water or gas at high pressure, when these fittings fail significant damage can occur. When these fittings were originally installed in the early 1980s, the job was done by a specialist, but nowadays the pipe weld is compelted by the same team that digs the hole. While costs have come down, the number of PTFE weld failures went up. Eric's company focussed on new systems for auto-->
<div class="figure">
<div id="experiment-analyze-design-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/ml/experiment-analyze-design.svg" width="50%" style=" ">
</object>
</div>
<div id="experiment-analyze-design-magnify" class="magnify" onclick="magnifyFigure(&#39;experiment-analyze-design&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="experiment-analyze-design-caption" class="caption-frame">
<p>Figure: The experiment, analyze, design flywheel for scientific innovation.</p>
</div>
</div>
<p>From the perspective of the team we had in the supply chain, we looked at what we most needed to focus on. Amazon moves very quickly, but we could also take a leaf out of Jeff’s book, and instead of worrying about what was going to change, remember what wasn’t going to change.</p>
<blockquote>
<p>We don’t know what science we’ll want to do in 5 years time, but we won’t want slower experiments, we won’t want more expensive experiments and we won’t want a narrower selection of experiments.</p>
</blockquote>
<p>As a result, our focus was on how to speed up the process of experiments, increase the diversity of experiments that we can do, and keep the experiments price as low as possible.</p>
<p>The faster the innovation fly-wheel can be iterated, then the quicker we can ask about different parts of the supply chain, and the better we can tailor systems to answering those questions.</p>
<p>As a result our objective became a two-order magnitude increase in number of experiments run across a five year period.</p>
<h2 id="statistical-emulation">Statistical Emulation</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_uq/includes/emulation.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_uq/includes/emulation.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<div class="figure">
<div id="statistical-emulation-1-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/uq/statistical-emulation000.svg" width="80%" style=" ">
</object>
</div>
<div id="statistical-emulation-1-magnify" class="magnify" onclick="magnifyFigure(&#39;statistical-emulation-1&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="statistical-emulation-1-caption" class="caption-frame">
<p>Figure: Real world systems consist of simulators that capture our domain knowledge about how our systems operate. Different simulators run at different speeds and granularities.</p>
</div>
</div>
<p>In many real world systems, decisions are made through simulating the environment. Simulations may operate at different granularities. For example, simulations are used in weather forecasts and climate forecasts. Interestingly, the UK Met office uses the same code for both, it has a <a href="https://www.metoffice.gov.uk/research/approach/modelling-systems/unified-model/index">“Unified Model” approach</a>, but they operate climate simulations one at greater spatial and temporal resolutions.</p>
<div class="figure">
<div id="met-office-unified-model-figure" class="figure-frame">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/simulation/unified_model_systems_13022018_1920.png" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="met-office-unified-model-magnify" class="magnify" onclick="magnifyFigure(&#39;met-office-unified-model&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="met-office-unified-model-caption" class="caption-frame">
<p>Figure: The UK Met office runs a shared code base for its simulations of climate and the weather. This plot shows the different spatial and temporal scales used.</p>
</div>
</div>
<div class="figure">
<div id="statistical-emulation-2-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/uq/statistical-emulation001.svg" width="80%" style=" ">
</object>
</div>
<div id="statistical-emulation-2-magnify" class="magnify" onclick="magnifyFigure(&#39;statistical-emulation-2&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="statistical-emulation-2-caption" class="caption-frame">
<p>Figure: A statistical emulator is a system that reconstructs the simulation with a statistical model.</p>
</div>
</div>
<p>§ A statistical emulator is a data-driven model that learns about the underlying simulation. Importantly, learns with uncertainty, so it ‘knows what it doesn’t know’. In practice, we can call the emulator in place of the simulator. If the emulator ‘doesn’t know’, it can call the simulator for the answer.</p>
<div class="figure">
<div id="statistical-emulation-5-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/uq/statistical-emulation004.svg" width="80%" style=" ">
</object>
</div>
<div id="statistical-emulation-5-magnify" class="magnify" onclick="magnifyFigure(&#39;statistical-emulation-5&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="statistical-emulation-5-caption" class="caption-frame">
<p>Figure: A statistical emulator is a system that reconstructs the simulation with a statistical model. As well as reconstructing the simulation, a statistical emulator can be used to correlate with the real world.</p>
</div>
</div>
<p>As well as reconstructing an individual simulator, the emulator can calibrate the simulation to the real world, by monitoring differences between the simulator and real data. This allows the emulator to characterise where the simulation can be relied on, i.e. we can validate the simulator.</p>
<p>Similarly, the emulator can adjudicate between simulations. This is known as <em>multi-fidelity emulation</em>. The emulator characterizes which emulations perform well where.</p>
<p>If all this modelling is done with judiscious handling of the uncertainty, the <em>computational doubt</em>, then the emulator can assist in desciding what experiment should be run next to aid a decision: should we run a simulator, in which case which one, or should we attempt to acquire data from a real world intervention.</p>
<h2 id="gpy-a-gaussian-process-framework-in-python">GPy: A Gaussian Process Framework in Python</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gpy-software.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gpy-software.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>Gaussian processes are a flexible tool for non-parametric analysis with uncertainty. The GPy software was started in Sheffield to provide a easy to use interface to GPs. One which allowed the user to focus on the modelling rather than the mathematics.</p>
<div class="figure">
<div id="gpy-software-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/gp/gpy.png" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="gpy-software-magnify" class="magnify" onclick="magnifyFigure(&#39;gpy-software&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gpy-software-caption" class="caption-frame">
<p>Figure: GPy is a BSD licensed software code base for implementing Gaussian process models in Python. It is designed for teaching and modelling. We welcome contributions which can be made through the Github repository <a href="https://github.com/SheffieldML/GPy" class="uri">https://github.com/SheffieldML/GPy</a></p>
</div>
</div>
<p>GPy is a BSD licensed software code base for implementing Gaussian process models in python. This allows GPs to be combined with a wide variety of software libraries.</p>
<p>The software itself is available on <a href="https://github.com/SheffieldML/GPy">GitHub</a> and the team welcomes contributions.</p>
<p>The aim for GPy is to be a probabilistic-style programming language, i.e. you specify the model rather than the algorithm. As well as a large range of covariance functions the software allows for non-Gaussian likelihoods, multivariate outputs, dimensionality reduction and approximations for larger data sets.</p>
<p>The documentation for GPy can be found <a href="https://gpy.readthedocs.io/en/latest/">here</a>.</p>
<h2 id="gpy-tutorial">GPy Tutorial</h2>
<div class="centered" style="">
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip0">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Nicolas Durrande
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="../slides/diagrams/people/nicolas-durrande.jpg" clip-path="url(#clip0)"/>
</svg>
</div>
<p>This GPy tutorial is based on material we share in the Gaussian process summer school for teaching these models <a href="https://gpss.cc" class="uri">https://gpss.cc</a>. It contains material from various members and former members of the Sheffield machine learning group, but particular mention should be made of <a href="https://sites.google.com/site/nicolasdurrandehomepage/">Nicolas Durrande</a>.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="op">%</span>pip install gpy</span></code></pre></div>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="im">import</span> urllib.request</span></code></pre></div>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a>urllib.request.urlretrieve(<span class="st">&#39;https://raw.githubusercontent.com/lawrennd/talks/gh-pages/mlai.py&#39;</span>,<span class="st">&#39;mlai.py&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a>urllib.request.urlretrieve(<span class="st">&#39;https://raw.githubusercontent.com/lawrennd/talks/gh-pages/teaching_plots.py&#39;</span>,<span class="st">&#39;teaching_plots.py&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a>urllib.request.urlretrieve(<span class="st">&#39;https://raw.githubusercontent.com/lawrennd/talks/gh-pages/gp_tutorial.py&#39;</span>,<span class="st">&#39;gp_tutorial.py&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb6-2"><a href="#cb6-2"></a><span class="im">import</span> GPy</span></code></pre></div>
<p>To give a feel for the sofware we’ll start by creating an exponentiated quadratic covariance function, <br /><span class="math display">$$
\kernelScalar(\inputVector, \inputVector^\prime) = \alpha \exp\left(-\frac{\ltwoNorm{\inputVector - \inputVector^\prime}^2}{2\ell^2}\right),
$$</span><br /> where the length scale is <span class="math inline">ℓ</span> and the variance is <span class="math inline"><em>α</em></span>.</p>
<p>To set this up in GPy we create a kernel in the following manner.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a>input_dim<span class="op">=</span><span class="dv">1</span></span>
<span id="cb7-2"><a href="#cb7-2"></a>alpha <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb7-3"><a href="#cb7-3"></a>lengthscale <span class="op">=</span> <span class="fl">2.0</span></span>
<span id="cb7-4"><a href="#cb7-4"></a>kern <span class="op">=</span> GPy.kern.RBF(input_dim<span class="op">=</span>input_dim, variance<span class="op">=</span>alpha, lengthscale<span class="op">=</span>lengthscale)</span></code></pre></div>
<p>That builds a kernel object for us. The kernel can be displayed.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a>display(kern)</span></code></pre></div>
<p>Or because it’s one dimensional, you can also plot the kernel as a function of its inputs (while the other is fixed).</p>
<div class="figure">
<div id="gpy-eq-covariance-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/kern/gpy-eq-covariance.svg" width="50%" style=" ">
</object>
</div>
<div id="gpy-eq-covariance-magnify" class="magnify" onclick="magnifyFigure(&#39;gpy-eq-covariance&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gpy-eq-covariance-caption" class="caption-frame">
<p>Figure: The exponentiated quadratic covariance function as plotted by the <code>GPy.kern.plot</code> command.</p>
</div>
</div>
<p>You can set the lengthscale of the covariance to different values and plot the result.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a>kern <span class="op">=</span> GPy.kern.RBF(input_dim<span class="op">=</span>input_dim)     <span class="co"># By default, the parameters are set to 1.</span></span>
<span id="cb9-2"><a href="#cb9-2"></a>lengthscales <span class="op">=</span> np.asarray([<span class="fl">0.2</span>,<span class="fl">0.5</span>,<span class="fl">1.</span>,<span class="fl">2.</span>,<span class="fl">4.</span>])</span></code></pre></div>
<h2 id="covariance-functions-in-gpy">Covariance Functions in GPy</h2>
<p>Many covariance functions are already implemented in GPy. Instead of rbf, try constructing and plotting the following covariance functions: <code>exponential</code>, <code>Matern32</code>, <code>Matern52</code>, <code>Brownian</code>, <code>linear</code>, <code>bias</code>, <code>rbfcos</code>, <code>periodic_Matern32</code>, etc. Some of these covariance functions, such as <code>rbfcos</code>, are not parametrized by a variance and a lengthscale. Furthermore, not all kernels are stationary (i.e., they can’t all be written as <span class="math inline"><em>k</em>(<em>x</em>, <em>y</em>) = <em>f</em>(<em>x</em> − <em>y</em>)</span>, see for example the Brownian covariance function). For plotting so it may be interesting to change the value of the fixed input.</p>
<h2 id="combining-covariance-functions-in-gpy">Combining Covariance Functions in GPy</h2>
<p>In GPy you can easily combine covariance functions you have created using the sum and product operators, <code>+</code> and <code>*</code>. So, for example, if we wish to combine an exponentiated quadratic covariance with a Matern 5/2 then we can write</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a>kern1 <span class="op">=</span> GPy.kern.RBF(<span class="dv">1</span>, variance<span class="op">=</span><span class="fl">1.</span>, lengthscale<span class="op">=</span><span class="fl">2.</span>)</span>
<span id="cb10-2"><a href="#cb10-2"></a>kern2 <span class="op">=</span> GPy.kern.Matern52(<span class="dv">1</span>, variance<span class="op">=</span><span class="fl">2.</span>, lengthscale<span class="op">=</span><span class="fl">4.</span>)</span>
<span id="cb10-3"><a href="#cb10-3"></a>kern <span class="op">=</span> kern1 <span class="op">+</span> kern2</span>
<span id="cb10-4"><a href="#cb10-4"></a>display(kern)</span></code></pre></div>
<div class="figure">
<div id="gpy-eq-plus-matern52-covariance-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/kern/gpy-eq-plus-matern52-covariance.svg" width="80%" style=" ">
</object>
</div>
<div id="gpy-eq-plus-matern52-covariance-magnify" class="magnify" onclick="magnifyFigure(&#39;gpy-eq-plus-matern52-covariance&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gpy-eq-plus-matern52-covariance-caption" class="caption-frame">
<p>Figure: A combination of the exponentiated quadratic covariance plus the Matern <span class="math inline">5/2</span> covariance.</p>
</div>
</div>
<p>Or if we wanted to multiply them we can write</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a>kern1 <span class="op">=</span> GPy.kern.RBF(<span class="dv">1</span>, variance<span class="op">=</span><span class="fl">1.</span>, lengthscale<span class="op">=</span><span class="fl">2.</span>)</span>
<span id="cb11-2"><a href="#cb11-2"></a>kern2 <span class="op">=</span> GPy.kern.Matern52(<span class="dv">1</span>, variance<span class="op">=</span><span class="fl">2.</span>, lengthscale<span class="op">=</span><span class="fl">4.</span>)</span>
<span id="cb11-3"><a href="#cb11-3"></a>kern <span class="op">=</span> kern1 <span class="op">*</span> kern2</span>
<span id="cb11-4"><a href="#cb11-4"></a>display(kern)</span></code></pre></div>
<div class="figure">
<div id="gpy-eq-times-matern52-covariance-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/kern/gpy-eq-times-matern52-covariance.svg" width="80%" style=" ">
</object>
</div>
<div id="gpy-eq-times-matern52-covariance-magnify" class="magnify" onclick="magnifyFigure(&#39;gpy-eq-times-matern52-covariance&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gpy-eq-times-matern52-covariance-caption" class="caption-frame">
<p>Figure: A combination of the exponentiated quadratic covariance multiplied by the Matern <span class="math inline">5/2</span> covariance.</p>
</div>
</div>
<p>You can learn about how to implement <a href="https://gpy.readthedocs.io/en/latest/tuto_creating_new_kernels.html">new kernel objects in GPy here</a>.</p>
<h2 id="a-gaussian-process-regression-model">A Gaussian Process Regression Model</h2>
<p>We will now combine the Gaussian process prior with some data to form a GP regression model with GPy. We will generate data from the function <br /><span class="math display">$$
\mappingFunction( \inputScalar ) = − \cos(\pi \inputScalar ) + \sin(4\pi \inputScalar )$ over $[0, 1],
$$</span><br /> adding some noise to give <br /><span class="math display">$$
\dataScalar(\inputScalar) = \mappingFunction(\inputScalar) + \noiseScalar,
$$</span><br /> with the noise being Gaussian distributed, <span class="math inline">$\noiseScalar \sim \gaussianSamp{0}{0.01}$</span>.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a>X <span class="op">=</span> np.linspace(<span class="fl">0.05</span>,<span class="fl">0.95</span>,<span class="dv">10</span>)[:,np.newaxis]</span>
<span id="cb12-2"><a href="#cb12-2"></a>Y <span class="op">=</span> <span class="op">-</span>np.cos(np.pi<span class="op">*</span>X) <span class="op">+</span> np.sin(<span class="dv">4</span><span class="op">*</span>np.pi<span class="op">*</span>X) <span class="op">+</span> np.random.normal(loc<span class="op">=</span><span class="fl">0.0</span>, scale<span class="op">=</span><span class="fl">0.1</span>, size<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">1</span>))</span></code></pre></div>
<div class="figure">
<div id="noisy-sine-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/noisy-sine.svg" width="80%" style=" ">
</object>
</div>
<div id="noisy-sine-magnify" class="magnify" onclick="magnifyFigure(&#39;noisy-sine&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="noisy-sine-caption" class="caption-frame">
<p>Figure: Data from the noisy sine wave for fitting with a GPy model.</p>
</div>
</div>
<p>A GP regression model based on an exponentiated quadratic covariance function can be defined by first defining a covariance function.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a>kern <span class="op">=</span> GPy.kern.RBF(input_dim<span class="op">=</span><span class="dv">1</span>, variance<span class="op">=</span><span class="fl">1.</span>, lengthscale<span class="op">=</span><span class="fl">1.</span>)</span></code></pre></div>
<p>And then combining it with the data to form a Gaussian process model.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a>model <span class="op">=</span> GPy.models.GPRegression(X,Y,kern)</span></code></pre></div>
<p>Just as for the covariance function object, we can find out about the model using the command <code>display(model)</code>.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a>display(model)</span></code></pre></div>
<p>Note that by default the model includes some observation noise with variance 1. We can see the posterior mean prediction and visualize the marginal posterior variances using <code>model.plot()</code>.</p>
<div class="figure">
<div id="noisy-sine-gp-fit-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/noisy-sine-gp-fit.svg" width="80%" style=" ">
</object>
</div>
<div id="noisy-sine-gp-fit-magnify" class="magnify" onclick="magnifyFigure(&#39;noisy-sine-gp-fit&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="noisy-sine-gp-fit-caption" class="caption-frame">
<p>Figure: A Gaussian process fit to the noisy sine data. Here the parameters of the process and the covariance function haven’t yet been optimized.</p>
</div>
</div>
<p>You can also look directly at the predictions for the model using.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1"></a>Xstar <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">100</span>)[:, np.newaxis]</span>
<span id="cb16-2"><a href="#cb16-2"></a>Ystar, Vstar <span class="op">=</span> model.predict(Xstar)</span></code></pre></div>
<p>Which gives you the mean (<code>Ystar</code>), the variance (<code>Vstar</code>) at the locations given by <code>Xstar</code>.</p>
<h2 id="covariance-function-parameter-estimation">Covariance Function Parameter Estimation</h2>
<p>As we have seen during the lectures, the parameters values can be estimated by maximizing the likelihood of the observations. Since we don’t want one of the variance to become negative during the optimization, we can constrain all parameters to be positive before running the optimisation.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1"></a>model.constrain_positive()</span></code></pre></div>
<p>The warnings are because the parameters are already constrained by default, the software is warning us that they are being reconstrained.</p>
<p>Now we can optimize the model using the <code>model.optimize()</code> method. Here we switch messages on, which allows us to see the progession of the optimization.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1"></a>model.optimize(messages<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<p>By default the optimization is using a limited memory BFGS optimizer <span class="citation" data-cites="Byrd:lbfgsb95">(Byrd, Lu, and Nocedal 1995)</span>.</p>
<p>Once again we can display the model, now to see how the parameters have changed.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1"></a>display(model)</span></code></pre></div>
<p>The lengthscale is much smaller, as well as the noise level. The variance of the exponentiated quadratic has also reduced.</p>
<div class="figure">
<div id="noisy-sine-gp-optimized-fit-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/noisy-sine-gp-optimized-fit.svg" width="80%" style=" ">
</object>
</div>
<div id="noisy-sine-gp-optimized-fit-magnify" class="magnify" onclick="magnifyFigure(&#39;noisy-sine-gp-optimized-fit&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="noisy-sine-gp-optimized-fit-caption" class="caption-frame">
<p>Figure: A Gaussian process fit to the noisy sine data with parameters optimized.</p>
</div>
</div>
<h2 id="gpy-and-emulation">GPy and Emulation</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gpy-emulation.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gpy-emulation.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>Let <span class="math inline">$\inputVector$</span> be a random variable defined over the real numbers, <span class="math inline">ℜ</span>, and <span class="math inline">$\mappingFunction(\cdot)$</span> be a function mapping between the real numbers <span class="math inline">ℜ → ℜ</span>.</p>
<p>The problem of <em>uncertainty propagation</em> is the study of the distribution of the random variable <span class="math inline">$\mappingFunction(\inputVector)$</span>.</p>
<p>We’re going to address this problem using emulation and GPy. We will see in this section the advantage of using a model when only a few observations of <span class="math inline"><em>f</em></span> are available.</p>
<p>Firstly we’ll make use of a test function known as the Branin test function. <br /><span class="math display">$$
\mappingFunction(\inputVector) = a(\inputScalar_2 - b\inputScalar_1^2 + c\inputScalar_1 - r)^2 + s(1-t \cos(\inputScalar_1)) + s
$$</span><br /> where we are setting <span class="math inline"><em>a</em> = 1</span>, <span class="math inline"><em>b</em> = 5.1/(4<em>π</em><sup>2</sup>)</span>, <span class="math inline"><em>c</em> = 5/<em>π</em></span>, <span class="math inline"><em>r</em> = 6</span>, <span class="math inline"><em>s</em> = 10</span> and <span class="math inline"><em>t</em> = 1/(8<em>π</em>)</span>.</p>
<h1 id="definition-of-the-branin-test-function">Definition of the Branin test function</h1>
<div class="sourceCode" id="cb20"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1"></a><span class="kw">def</span> branin(X):</span>
<span id="cb20-2"><a href="#cb20-2"></a>    y <span class="op">=</span> ((X[:,<span class="dv">1</span>]<span class="op">-</span><span class="fl">5.1</span><span class="op">/</span>(<span class="dv">4</span><span class="op">*</span>np.pi<span class="op">**</span><span class="dv">2</span>)<span class="op">*</span>X[:,<span class="dv">0</span>]<span class="op">**</span><span class="dv">2</span><span class="op">+</span><span class="dv">5</span><span class="op">*</span>X[:,<span class="dv">0</span>]<span class="op">/</span>np.pi<span class="op">-</span><span class="dv">6</span>)<span class="op">**</span><span class="dv">2</span> </span>
<span id="cb20-3"><a href="#cb20-3"></a>        <span class="op">+</span> <span class="dv">10</span><span class="op">*</span>(<span class="dv">1</span><span class="op">-</span><span class="dv">1</span><span class="op">/</span>(<span class="dv">8</span><span class="op">*</span>np.pi))<span class="op">*</span>np.cos(X[:,<span class="dv">0</span>])<span class="op">+</span><span class="dv">10</span>)</span>
<span id="cb20-4"><a href="#cb20-4"></a>    <span class="cf">return</span>(y)</span></code></pre></div>
<p>We’ll define a grid of twenty five observations over [−5, 10] × [0, 15] and a set of 25 observations.</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1"></a><span class="co"># Training set defined as a 5*5 grid:</span></span>
<span id="cb21-2"><a href="#cb21-2"></a>xg1 <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">5</span>,<span class="dv">10</span>,<span class="dv">5</span>)</span>
<span id="cb21-3"><a href="#cb21-3"></a>xg2 <span class="op">=</span> np.linspace(<span class="dv">0</span>,<span class="dv">15</span>,<span class="dv">5</span>)</span>
<span id="cb21-4"><a href="#cb21-4"></a>X <span class="op">=</span> np.zeros((xg1.size <span class="op">*</span> xg2.size,<span class="dv">2</span>))</span>
<span id="cb21-5"><a href="#cb21-5"></a><span class="cf">for</span> i,x1 <span class="kw">in</span> <span class="bu">enumerate</span>(xg1):</span>
<span id="cb21-6"><a href="#cb21-6"></a>    <span class="cf">for</span> j,x2 <span class="kw">in</span> <span class="bu">enumerate</span>(xg2):</span>
<span id="cb21-7"><a href="#cb21-7"></a>        X[i<span class="op">+</span>xg1.size<span class="op">*</span>j,:] <span class="op">=</span> [x1,x2]</span>
<span id="cb21-8"><a href="#cb21-8"></a></span>
<span id="cb21-9"><a href="#cb21-9"></a>Y <span class="op">=</span> branin(X)[:,np.newaxis]</span></code></pre></div>
<p>The task here will be to consider the distribution of <span class="math inline">$\mappingFunction(U)$</span>, where <span class="math inline"><em>U</em></span> is a random variable with uniform distribution over the input space of <span class="math inline">$\mappingFunction$</span>. We focus on the computaiton of two quantities, the expectation of <span class="math inline">$\mappingFunction(U)$</span>, <span class="math inline">$\expSamp{\mappingFunction(U)}$</span>, and the probability that the value is greater than 200.</p>
<h2 id="computation-of-expsampmappingfunctionu">Computation of <span class="math inline">$\expSamp{\mappingFunction(U)}$</span></h2>
<p>The expectation of <span class="math inline">$\mappingFunction (U )$</span> is given by <span class="math inline">$\int_\inputVector \mappingFunction ( \inputVector)\text{d}\inputVector$</span>. A basic approach to approximate this integral is to compute the mean of the 25 observations: <code>np.mean(Y)</code>. Since the points are distributed on a grid, this can be seen as the approximation of the integral by a rough Riemann sum.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1"></a><span class="bu">print</span>(<span class="st">&#39;Estimate of the expectation is given by: </span><span class="sc">{mean}</span><span class="st">&#39;</span>.<span class="bu">format</span>(mean<span class="op">=</span>Y.mean()))</span></code></pre></div>
<p>The result can be compared with the actual mean of the Branin function which is 54.31.</p>
<p>Alternatively, we can fit a GP model and compute the integral of the best predictor by Monte Carlo sampling.</p>
<p>First we create the covariance function. Here we’re going to use an exponentiated quadratic, but we’ll augment it with the ‘bias’ covariance function. This covariance function represents a single fixed bias that is added to the overall covariance. It allows us to deal with non-zero-mean emulations.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1"></a><span class="co"># Create an exponentiated quadratic plus bias covariance function</span></span>
<span id="cb23-2"><a href="#cb23-2"></a>kern_eq <span class="op">=</span> GPy.kern.RBF(input_dim<span class="op">=</span><span class="dv">2</span>, ARD <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb23-3"><a href="#cb23-3"></a>kern_bias <span class="op">=</span> GPy.kern.Bias(input_dim<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb23-4"><a href="#cb23-4"></a>kern <span class="op">=</span> kern_eq <span class="op">+</span> kern_bias</span></code></pre></div>
<p>Now we construct the Gaussian process regression model in GPy.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1"></a><span class="co"># Build a GP model</span></span>
<span id="cb24-2"><a href="#cb24-2"></a>model <span class="op">=</span> GPy.models.GPRegression(X,Y,kern)</span></code></pre></div>
<p>In the sinusoid example above, we learnt the variance of the process. But in this example, we are fitting an emulator to a function we know is noise-free. However, we don’t fix the noise value to precisely zero, as this can lead to some numerical errors. Instead we fix the variance of the Gaussian noise to a very small value.</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1"></a><span class="co"># fix the noise variance</span></span>
<span id="cb25-2"><a href="#cb25-2"></a>model.likelihood.variance.fix(<span class="fl">1e-5</span>)</span></code></pre></div>
<p>Now we fit the model. Note, that the initial values for the lengthscale are not appropriate. So first set the lengthscale of the model needs to be reset.</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1"></a>kern.rbf.lengthscale <span class="op">=</span> np.asarray([<span class="dv">3</span>, <span class="dv">3</span>])</span></code></pre></div>
<p>It’s a common error in Gaussian process fitting to initialise the lengthscale too small or too big. The challenge is that the error surface is normally multimodal, and the final solution can be very sensitive to this initialisation. If the lengthscale is initialized too small, the solution can converge on an place where the signal isn’t extracted by the covariance function. If the lengthscale is initialized too large, then the variations of the function are often missing. Here the lengthscale is set for each dimension of inputs as 3. Now that’s done, we can optimize the model.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1"></a><span class="co"># Randomize the model and optimize</span></span>
<span id="cb27-2"><a href="#cb27-2"></a>model.optimize(messages<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<div class="figure">
<div id="branin-sine-gp-optimized-fit-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/gp/branin-sine-gp-optimized-fit.svg" width="80%" style=" ">
</object>
</div>
<div id="branin-sine-gp-optimized-fit-magnify" class="magnify" onclick="magnifyFigure(&#39;branin-sine-gp-optimized-fit&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="branin-sine-gp-optimized-fit-caption" class="caption-frame">
<p>Figure: A Gaussian process fit to the Branin test function, used to assess the mean of the function by emulation.</p>
</div>
</div>
<p>Finally we can compute the mean of the model predictions using very many Monte Carlo samples.</p>
<p>Note, that in this example, because we’re using at est function, we could simply have done the Monte Carlo estimation directly on the Branin function. However, imagine inststead that we were trying to understand the results of a complex Computational Fluid Dynamics simulation, where each run of the simulation (which is equivalent to our function) took many hours. In that case the advantage of the emulator is clear.</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1"></a><span class="co"># Compute the mean of model prediction on 1e5 Monte Carlo samples</span></span>
<span id="cb28-2"><a href="#cb28-2"></a>Xp <span class="op">=</span> np.random.uniform(size<span class="op">=</span>(<span class="bu">int</span>(<span class="fl">1e5</span>),<span class="dv">2</span>))</span>
<span id="cb28-3"><a href="#cb28-3"></a>Xp[:,<span class="dv">0</span>] <span class="op">=</span> Xp[:,<span class="dv">0</span>]<span class="op">*</span><span class="dv">15</span><span class="op">-</span><span class="dv">5</span></span>
<span id="cb28-4"><a href="#cb28-4"></a>Xp[:,<span class="dv">1</span>] <span class="op">=</span> Xp[:,<span class="dv">1</span>]<span class="op">*</span><span class="dv">15</span></span>
<span id="cb28-5"><a href="#cb28-5"></a>mu, var <span class="op">=</span> m.predict(Xp)</span>
<span id="cb28-6"><a href="#cb28-6"></a><span class="bu">print</span>(<span class="st">&#39;The estimate of the mean of the Branin function is </span><span class="sc">{mean}</span><span class="st">&#39;</span>.<span class="bu">format</span>(mean<span class="op">=</span>np.mean(mu)))</span></code></pre></div>
<h3 id="exercise-2">Exercise 2</h3>
<p>Now think about how to make use of the variance estimation from the Gaussian process to obtain error bars around your estimate.</p>
<h3 id="exercise-3">Exercise 3</h3>
<p>You’ve seen how the Monte Carlo estimates work with the Gaussian process. Now make your estimate of the probability that the Branin function is greater than 200 with the uniform random inputs.</p>
<h2 id="uncertainty-quantification">Uncertainty Quantification</h2>
<p>We’re introducing you to the optimization and analysis of real world models through emulation, this domain is part of a broader field known as surrogate modelling.</p>
<p>Although we’re approaching this from the machine learning perspective, with a computer-scientist’s approach, you won’t be suprised to find out that this field is not new and there are a range of research groups interested in this domain.</p>
<p>Related publications and links will appear here.</p>
<p>Examle paper: <span class="citation" data-cites="McKay-selecting79">McKay, Beckman, and Conover (1979)</span> <span class="citation" data-cites="Kennedy-bayesian01">Kennedy and O’Hagan (2001)</span></p>
<p>The MUCM project <a href="http://www.mucm.ac.uk/" class="uri">http://www.mucm.ac.uk/</a></p>
<p>The MASCOT-NUM Research Group <a href="https://www.gdr-mascotnum.fr/" class="uri">https://www.gdr-mascotnum.fr/</a></p>
<p>http://www.mucm.ac.uk/Pages/Dissemination/TechnicalReports.html</p>
<blockquote>
<p>Random Sampling. Let the input values <span class="math inline">$x_1, \dots, x_\numData$</span> be a random sample from <span class="math inline"><em>f</em>(<em>x</em>)</span>. This method of sampling is perhaps the most obvious, and an entire body of statistical literature may be used in making infer- ences regarding the distribution of <span class="math inline"><em>Y</em>(<em>t</em>)</span>. Stratified Sampling. Using stratified sampling, all areas of the sample space of <span class="math inline"><em>X</em></span> are represented by input values. Let the sample space <span class="math inline"><em>S</em></span> of <span class="math inline"><em>X</em></span> be partitioned into <span class="math inline"><em>I</em></span> disjoint strata <span class="math inline"><em>S</em><sub><em>t</em></sub></span>. Let <span class="math inline"><em>π</em> = <em>P</em>(<em>X</em><em>C</em><em>S</em><sub><em>i</em></sub>)</span> represent the size of <span class="math inline"><em>S</em><sub><em>i</em></sub></span>. Obtain a random sample <span class="math inline"><em>X</em><em>i</em><em>J</em>, <em>j</em> = 1, …, <em>n</em></span> from <span class="math inline"><em>S</em><sub><em>i</em></sub></span>. Then of course the <span class="math inline"><em>n</em><sub><em>i</em></sub></span> sum to <span class="math inline"><em>N</em></span>. If <span class="math inline"><em>I</em> = 1</span>, we have random sampling over the entire sample space. Latin Hypercube Sampling. The same reasoning that led to stratified sampling, ensuring that all por- tions of <span class="math inline"><em>S</em></span> were sampled, could lead further. If we wish to ensure also that each of the input variables <span class="math inline"><em>X</em><sub><em>k</em></sub></span> has all portions of its distribution represented by input values, we can divide the range of each <span class="math inline"><em>X</em><sub><em>k</em></sub></span> into <span class="math inline"><em>N</em></span> strata of equal marginal probability <span class="math inline">1/<em>N</em></span>, and sample once from each stratum. Let this sample be <span class="math inline"><em>X</em><em>k</em><em>j</em>, <em>j</em> = 1, …, <em>N</em></span>. These form the <span class="math inline"><em>X</em><sub><em>k</em></sub></span> component, <span class="math inline"><em>k</em> = 1, *,<em>K</em>, <em>i</em><em>n</em><em>X</em><em>i</em>, <em>i</em> = 1, *,<em>N</em></span>. The components of the various <span class="math inline"><em>X</em>, <em>A</em>′<em>s</em></span> are matched at random. This method of selecting input values is an extension of quota sam- pling [13], and can be viewed as a K-dimensional extension of Latin square sampling [11]. One advantage of the Latin hypercube sample ap- pears when the output <span class="math inline"><em>Y</em>(<em>t</em>)</span> is dominated by only a few of the components of <span class="math inline"><em>X</em></span>. This method ensures that each of those components is represented in a fully stratified manner, no matter which components might turn out to be important. We mention here that the <span class="math inline"><em>N</em></span> intervals on the range of each component of X combine to form <span class="math inline"><em>N</em><em>K</em></span> cells which cover the sample space of <span class="math inline"><em>X</em></span>. These cells, which are labeled by coordinates corresponding to the inter- vals, are used when finding the properties of the sampling plan.</p>
</blockquote>
<p><br /><span class="math display"></span><br /></p>
<h2 id="emukit-playground">Emukit Playground</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_uq/includes/emukit-playground.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_uq/includes/emukit-playground.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>Emukit playground is a software toolkit for exploring the use of statistical emulation as a tool. It was built by <a href="https://twitter.com/_AdamHirst">Adam Hirst</a>, during his software engineering internship at Amazon and supervised by Cliff McCollum.</p>
<div class="figure">
<div id="emukit-playground-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/uq/emukit-playground.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="emukit-playground-magnify" class="magnify" onclick="magnifyFigure(&#39;emukit-playground&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="emukit-playground-caption" class="caption-frame">
<p>Figure: Emukit playground is a tutorial for understanding the simulation/emulation relationship. <a href="https://amzn.github.io/emukit-playground/" class="uri">https://amzn.github.io/emukit-playground/</a></p>
</div>
</div>
<div class="figure">
<div id="emukit-playground-bayes-opt-figure" class="figure-frame">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/uq/emukit-playground-bayes-opt.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="emukit-playground-bayes-opt-magnify" class="magnify" onclick="magnifyFigure(&#39;emukit-playground-bayes-opt&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="emukit-playground-bayes-opt-caption" class="caption-frame">
<p>Figure: Tutorial on Bayesian optimization of the number of taxis deployed from Emukit playground. <a href="https://amzn.github.io/emukit-playground/#!/learn/bayesian_optimization" class="uri">https://amzn.github.io/emukit-playground/#!/learn/bayesian_optimization</a></p>
</div>
</div>
<p>You can explore Bayesian optimization of a taxi simulation.</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1"></a><span class="op">%</span>pip install pyDOE</span></code></pre></div>
<div class="sourceCode" id="cb30"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1"></a><span class="op">%</span>pip install emukit</span></code></pre></div>
<p>This introduction is based on <a href="https://github.com/EmuKit/emukit/blob/master/notebooks/Emukit-tutorial-experimental-design-introduction.ipynb">An Introduction to Experimental Design with Emukit</a> written by Andrei Paleyes and Maren Mahsereci.</p>
<h2 id="alex-forrester">Alex Forrester</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_uq/includes/alex-forrester.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_uq/includes/alex-forrester.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<div class="centered" style="">
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip1">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Alex Forrester
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="../slides/diagrams/people/alex-forrester.jpg" clip-path="url(#clip1)"/>
</svg>
</div>
<p>We’re going to make use of the Forrester function in our example below, a function developed as a demonstrator by <a href="https://www.southampton.ac.uk/engineering/research/groups/performance-sports/staff-profiles/alexander-forrester.page">Alex Forrester</a>. Alex is a design engineer who makes extensive use of surrogate modelling in Engineering design.</p>
<p>You can see Alex talking about the use of Gaussian process surrogates <a href="http://videolectures.net/mla09_forrester_sbcmoo/">in this online video lecture</a>.</p>
<div class="figure">
<div id="kinematic-human-simulation-figure" class="figure-frame">
<iframe width="800" height="600" src="https://www.youtube.com/embed/2ngc2aw9xYs?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
<div id="kinematic-human-simulation-magnify" class="magnify" onclick="magnifyFigure(&#39;kinematic-human-simulation&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="kinematic-human-simulation-caption" class="caption-frame">
<p>Figure: A kinematic simulation of the human body doing breaststroke that Alex uses as part of his work in optimization of human motion during sports.</p>
</div>
</div>
<p>The Forrester function <span class="citation" data-cites="Forrester-engineering08">(Forrester, Sóbester, and Keane 2008)</span> is commonly used as a demonstrator function in surrogate modelling. It has the form <br /><span class="math display"><em>f</em>(<em>x</em>) = (6<em>x</em> − 2)<sup>2</sup>sin (12<em>x</em> − 4).</span><br /></p>
<div class="sourceCode" id="cb31"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<div class="sourceCode" id="cb32"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1"></a>x <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">100</span>)</span>
<span id="cb32-2"><a href="#cb32-2"></a>f <span class="op">=</span> (<span class="dv">6</span><span class="op">*</span>x<span class="op">-</span><span class="dv">2</span>)<span class="op">**</span><span class="dv">2</span> <span class="op">*</span> np.sin(<span class="dv">12</span><span class="op">*</span>x<span class="op">-</span><span class="dv">4</span>)</span></code></pre></div>
<div class="figure">
<div id="forrester-function-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/uq/forrester-function.svg" width="80%" style=" ">
</object>
</div>
<div id="forrester-function-magnify" class="magnify" onclick="magnifyFigure(&#39;forrester-function&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="forrester-function-caption" class="caption-frame">
<p>Figure: The Forrester function is commonly used as an exemplar function for surrogate modelling and emulation. It has the form <span class="math inline"><em>f</em>(<em>x</em>) = (6<em>x</em> − 2)<sup>2</sup>sin (12<em>x</em> − 4)</span></p>
</div>
</div>
<p>Experimental design.</p>
<p>Latin hypercube</p>
<p>Linear example</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb33-2"><a href="#cb33-2"></a></span>
<span id="cb33-3"><a href="#cb33-3"></a><span class="im">from</span> emukit.test_functions <span class="im">import</span> forrester_function</span>
<span id="cb33-4"><a href="#cb33-4"></a><span class="im">from</span> emukit.core.loop.user_function <span class="im">import</span> UserFunctionWrapper</span>
<span id="cb33-5"><a href="#cb33-5"></a><span class="im">from</span> emukit.core <span class="im">import</span> ContinuousParameter, ParameterSpace</span></code></pre></div>
<div class="sourceCode" id="cb34"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1"></a>target_function, space <span class="op">=</span> forrester_function()</span></code></pre></div>
<div class="sourceCode" id="cb35"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1"></a>x_plot <span class="op">=</span> np.linspace(space.parameters[<span class="dv">0</span>].<span class="bu">min</span>, space.parameters[<span class="dv">0</span>].<span class="bu">max</span>, <span class="dv">301</span>)[:, <span class="va">None</span>]</span>
<span id="cb35-2"><a href="#cb35-2"></a>y_plot <span class="op">=</span> target_function(x_plot)</span></code></pre></div>
<div class="figure">
<div id="forrester-function-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/uq/forrester-function.svg" width="80%" style=" ">
</object>
</div>
<div id="forrester-function-magnify" class="magnify" onclick="magnifyFigure(&#39;forrester-function&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="forrester-function-caption" class="caption-frame">
<p>Figure: The Forrester function <span class="citation" data-cites="Forrester-engineering08">(Forrester, Sóbester, and Keane 2008)</span>.</p>
</div>
</div>
<h2 id="initial-design">Initial Design</h2>
<p>Usually, before we start the actual ExpDesign loop we need to gather a few observations such that we can fit the model. This is called the initial design and common strategies are either a predefined grid or sampling points uniformly at random.</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1"></a>X_init <span class="op">=</span> np.array([[<span class="fl">0.2</span>],[<span class="fl">0.6</span>], [<span class="fl">0.9</span>]])</span>
<span id="cb36-2"><a href="#cb36-2"></a>Y_init <span class="op">=</span> target_function(X_init)</span></code></pre></div>
<div class="figure">
<div id="forrester-function-initial-design-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/uq/forrester-function-initial-design.svg" width="80%" style=" ">
</object>
</div>
<div id="forrester-function-initial-design-magnify" class="magnify" onclick="magnifyFigure(&#39;forrester-function-initial-design&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="forrester-function-initial-design-caption" class="caption-frame">
<p>Figure: The initial design for the Forrester function example.</p>
</div>
</div>
<h2 id="the-model">The Model</h2>
<p>Now we can start with the ExpDesign loop by first fitting a model on the collected data. A popular model for ExpDesign is a Gaussian process (GP) which defines a probability distribution across classes of functions, typically smooth, such that each linear finite-dimensional restriction is multivariate Gaussian <span class="citation" data-cites="Rasmussen:book06">(Rasmussen and Williams 2006)</span>. Gaussian processes are fully parametrized by a mean <span class="math inline">$\mu(\inputVector)$</span> and a covariance function <span class="math inline">$\kernelScalar(\inputVector,\inputVector^\prime)$</span>. Without loss of generality <span class="math inline">$\mu(\inputVector)$</span> is assumed to be zero. The covariance function <span class="math inline">$\kernelScalar(\inputVector,\inputVector^\prime)$</span> characterizes the smoothness and other properties of <span class="math inline">$\mappingFunction$</span>. It is known that the kernel of the process has to be continuous, symmetric and positive definite. A widely used kernel is the exponentiated quadratic or RBF kernel: <br /><span class="math display">$$ 
\kernelScalar(\inputVector,\inputVector^\prime) = \alpha \exp{ \left(-\frac{\|\inputVector-\inputVector^\prime\|^2}{2 \ell}\right)} 
$$</span><br /> where <span class="math inline"><em>α</em></span> and <span class="math inline">ℓ</span> are hyperparameters.</p>
<p>To denote that <span class="math inline">$\mappingFunction$</span> is a sample from a GP with mean <span class="math inline"><em>μ</em></span> and covariance <span class="math inline"><em>k</em></span> we write <br /><span class="math display">$$
\mappingFunction \sim \mathcal{GP}(\mu,k).
$$</span><br /></p>
<p>For regression tasks, the most important feature of GPs is that process priors are conjugate to the likelihood from finitely many observations <span class="math inline">$\dataMatrix = (y_1,\dots,y_\numData)^\top$</span> and <span class="math inline">$\inputMatrix =\{\inputVector_1,\dots,\inputVector_\numData\}$</span>, <span class="math inline">$\inputVector_i\in \mathcal{X}$</span> of the form <span class="math inline">$\dataScalar_i = \mappingFunction(\inputVector_i) + \noiseScalar_i$</span> where <span class="math inline">$\noiseScalar_i \sim \gaussianSamp{0}{\dataStd^2}$</span> and we typically estimate <span class="math inline">$\dataStd^2$</span> by maximum likelihood alongside <span class="math inline"><em>α</em></span> and <span class="math inline">ℓ</span>.</p>
<p>We obtain the Gaussian posterior <br /><span class="math display">$$
\mappingFunction(\inputVector^*)|\inputMatrix, \dataMatrix, \theta \sim \gaussianSamp{\mu(\inputVector^*)}{\sigma^2(\inputVector^*)},
$$</span><br /> where <span class="math inline">$\mu(\inputVector^*)$</span> and <span class="math inline">$\sigma^2(\inputVector^*)$</span> have a closed form solution as we’ve seen in the earlier lectures (see also <span class="citation" data-cites="Rasmussen:book06">Rasmussen and Williams (2006)</span>).</p>
<p>Note that Gaussian processes are also characterized by hyperparameters, for example in the exponatiated quadratic case we have <span class="math inline">$\paramVector = \left\{ \alpha, \ell, \dataStd^2 \right\}$</span> for the scale of the covariance, the lengthscale and the noise variance. Here, for simplicitly we will keep these hyperparameters fixed. However, we will usually either optimize or sample these hyperparameters using the marginal loglikelihood of the GP.</p>
<p>In this module we’ve focussed on Gaussian processes, but we could also use any other model that returns a mean <span class="math inline">$\mu(\inputVector)$</span> and variance <span class="math inline">$\sigma^2(\inputVector)$</span> on an arbitrary input points <span class="math inline">$\inputVector$</span> such as Bayesian neural networks or random forests. In Emukit these different models can be used by defining a new <code>ModelWrapper</code>.</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1"></a><span class="im">import</span> GPy</span>
<span id="cb37-2"><a href="#cb37-2"></a><span class="im">from</span> emukit.model_wrappers.gpy_model_wrappers <span class="im">import</span> GPyModelWrapper</span></code></pre></div>
<div class="sourceCode" id="cb38"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1"></a>kern <span class="op">=</span> GPy.kern.RBF(<span class="dv">1</span>, lengthscale<span class="op">=</span><span class="fl">0.08</span>, variance<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb38-2"><a href="#cb38-2"></a>gpy_model <span class="op">=</span> GPy.models.GPRegression(X_init, Y_init, kern, noise_var<span class="op">=</span><span class="fl">1e-10</span>)</span>
<span id="cb38-3"><a href="#cb38-3"></a>emukit_model <span class="op">=</span> GPyModelWrapper(gpy_model)</span>
<span id="cb38-4"><a href="#cb38-4"></a></span>
<span id="cb38-5"><a href="#cb38-5"></a>mu_plot, var_plot <span class="op">=</span> emukit_model.predict(x_plot)</span></code></pre></div>
<div class="figure">
<div id="forrester-function-entropy-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/uq/forrester-function-entropy.svg" width="80%" style=" ">
</object>
</div>
<div id="forrester-function-entropy-magnify" class="magnify" onclick="magnifyFigure(&#39;forrester-function-entropy&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="forrester-function-entropy-caption" class="caption-frame">
<p>Figure: The emulator fitted to the Forrester function with only three observations. The error bars show 1, 2 and 3 standard deviations.</p>
</div>
</div>
<h2 id="the-acquisition-function">The Acquisition Function</h2>
<p>In the second step of our ExpDesign loop we use our model to compute the acquisition function. We’ll review two different forms of acquisition funciton for doing this.</p>
<h3 id="uncertainty-sampling">Uncertainty Sampling</h3>
<p>In uncertainty sampling (US) we hoose the next value <span class="math inline">$\inputVector_{n+1}$</span> at the location where the model on <span class="math inline">$\mappingFunction(\inputVector)$</span> has the highest marginal predictive variance <br /><span class="math display">$$
a_{US}(\inputVector) = \sigma^2(\inputVector).
$$</span><br /> This makes sure, that we learn the function <span class="math inline">$\mappingFunction(\cdot)$</span> everywhere on <span class="math inline">𝕏</span> to a similar level of absolute error.</p>
<h3 id="integrated-variance-reduction">Integrated Variance Reduction</h3>
<p>In the integrated variance reduction (IVR) you choose the next value <span class="math inline">$\inputVector_{n+1}$</span> such that the total variance of the model is reduced maximally <span class="citation" data-cites="Sacks-design89">(Sacks et al. 1989)</span>, <br /><span class="math display">$$
a_{IVR} = \int_{\mathbb{X}}[\sigma^2(\inputVector') - \sigma^2(\inputVector'; \inputVector)]\text{d}\inputVector'\approx 
\frac{1}{\# \text{samples}}\sum_i^{\# \text{samples}}[\sigma^2(\inputVector_i) - \sigma^2(\inputVector_i; \inputVector)].
$$</span><br /> Here <span class="math inline">$\sigma^2(\inputVector'; \inputVector)$</span> is the predictive variance at <span class="math inline">$\inputVector'$</span> had <span class="math inline">$\inputVector$</span> been observed. Thus IVR computes the overall reduction in variance (for all points in <span class="math inline">𝕏</span>) had <span class="math inline"><em>f</em></span> been observed at <span class="math inline">$\inputVector$</span>.</p>
<p>The finite sum approximation on the right hand side of the equation is usually used because the integral over <span class="math inline">$\inputVector'$</span> is not analytic. In that case <span class="math inline">$\inputVector_i$</span> are sampled randomly. For a GP model the right hand side simplifies to <br /><span class="math display">$$
a_{LCB} \approx \frac{1}{\# \text{samples}}\sum_i^{\# \text{samples}}\frac{\kernelScalar^2(\inputVector_i, \inputVector)}{\sigma^2(\inputVector)}.
$$</span><br /></p>
<p>IVR is arguably the more principled approach, but often US is preferred over IVR simply because it lends itself to gradient based optimization more easily, is cheaper to compute, and is exact.</p>
<p>For both of them (stochastic) gradient base optimizers are used to retrieve <span class="math inline">$\inputVector_{n+1} \in \operatorname*{arg\:max}_{\inputVector \in \mathbb{X}} a(\inputVector)$</span>.</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1"></a><span class="im">from</span> emukit.experimental_design.acquisitions <span class="im">import</span> IntegratedVarianceReduction, ModelVariance</span></code></pre></div>
<div class="sourceCode" id="cb40"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1"></a>us_acquisition <span class="op">=</span> ModelVariance(emukit_model)</span>
<span id="cb40-2"><a href="#cb40-2"></a>ivr_acquisition <span class="op">=</span> IntegratedVarianceReduction(emukit_model, space)</span>
<span id="cb40-3"><a href="#cb40-3"></a></span>
<span id="cb40-4"><a href="#cb40-4"></a>us_plot <span class="op">=</span> us_acquisition.evaluate(x_plot)</span>
<span id="cb40-5"><a href="#cb40-5"></a>ivr_plot <span class="op">=</span> ivr_acquisition.evaluate(x_plot)</span></code></pre></div>
<div class="figure">
<div id="experimental-design-acquisition-functions-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/uq/experimental-design-acquisition-functions-forrester.svg" width="80%" style=" ">
</object>
</div>
<div id="experimental-design-acquisition-functions-magnify" class="magnify" onclick="magnifyFigure(&#39;experimental-design-acquisition-functions&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="experimental-design-acquisition-functions-caption" class="caption-frame">
<p>Figure: The <em>uncertainty sampling</em> and <em>integrated variance reduction</em> acquisition functions for the Forrester example.</p>
</div>
</div>
<h2 id="evaluating-the-objective-function">Evaluating the objective function</h2>
<p>To find the next point to evaluate we optimize the acquisition function using a standard gradient descent optimizer.</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1"></a><span class="im">from</span> emukit.core.optimization <span class="im">import</span> GradientAcquisitionOptimizer</span></code></pre></div>
<div class="sourceCode" id="cb42"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1"></a>optimizer <span class="op">=</span> GradientAcquisitionOptimizer(space)</span>
<span id="cb42-2"><a href="#cb42-2"></a>x_new, _ <span class="op">=</span> optimizer.optimize(us_acquisition)</span></code></pre></div>
<div class="figure">
<div id="experimental-design-acquisition-functions-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/uq/experimental-design-acquisition-next-point-forrester.svg" width="80%" style=" ">
</object>
</div>
<div id="experimental-design-acquisition-functions-magnify" class="magnify" onclick="magnifyFigure(&#39;experimental-design-acquisition-functions&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="experimental-design-acquisition-functions-caption" class="caption-frame">
<p>Figure: The maxima of the acquisition function is found and this point is selected for inclusion.</p>
</div>
</div>
<p>Afterwards we evaluate the true objective function and append it to our initial observations.</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1"></a>y_new <span class="op">=</span> target_function(x_new)</span></code></pre></div>
<div class="sourceCode" id="cb44"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1"></a>X <span class="op">=</span> np.append(X_init, x_new, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb44-2"><a href="#cb44-2"></a>Y <span class="op">=</span> np.append(Y_init, y_new, axis<span class="op">=</span><span class="dv">0</span>)</span></code></pre></div>
<p>After updating the model, you can see that the uncertainty about the true objective function in this region decreases and our model becomes more certain.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1"></a>emukit_model.set_data(X, Y)</span>
<span id="cb45-2"><a href="#cb45-2"></a>mu_plot, var_plot <span class="op">=</span> emukit_model.predict(x_plot)</span></code></pre></div>
<div class="figure">
<div id="forrester-function-multi-errorbars-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/uq/forrester-function-multi-errorbars.svg" width="80%" style=" ">
</object>
</div>
<div id="forrester-function-multi-errorbars-magnify" class="magnify" onclick="magnifyFigure(&#39;forrester-function-multi-errorbars&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="forrester-function-multi-errorbars-caption" class="caption-frame">
<p>Figure: The target Forrester function plotted alongside the emulation model and error bars from the emulation at 1, 2 and 3 standard deviations.</p>
</div>
</div>
<p>Entropy of posterior</p>
<h2 id="emukits-experimental-design-interface">Emukit’s experimental design interface</h2>
<p>Of course in practice we don’t want to implement all of these steps our self. Emukit provides a convenient and flexible interface to apply experimental design. Below we can see how to run experimental design on the exact same function for 10 iterations.</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1"></a><span class="im">from</span> emukit.experimental_design.experimental_design_loop <span class="im">import</span> ExperimentalDesignLoop</span></code></pre></div>
<div class="sourceCode" id="cb47"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1"></a>ed <span class="op">=</span> ExperimentalDesignLoop(space<span class="op">=</span>space, model<span class="op">=</span>emukit_model)</span>
<span id="cb47-2"><a href="#cb47-2"></a></span>
<span id="cb47-3"><a href="#cb47-3"></a>ed.run_loop(target_function, <span class="dv">10</span>)</span></code></pre></div>
<div class="sourceCode" id="cb48"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1"></a>mu_plot, var_plot <span class="op">=</span> ed.model.predict(x_plot)</span></code></pre></div>
<div class="figure">
<div id="forrester-function-full-fit-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/uq/forrester-function-full-fit.svg" width="80%" style=" ">
</object>
</div>
<div id="forrester-function-full-fit-magnify" class="magnify" onclick="magnifyFigure(&#39;forrester-function-full-fit&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="forrester-function-full-fit-caption" class="caption-frame">
<p>Figure: The fit of the model to the Forrester function.</p>
</div>
</div>
<h2 id="thanks">Thanks!</h2>
<p>For more information on these subjects and more you might want to check the following resources.</p>
<ul>
<li>twitter: <a href="https://twitter.com/lawrennd">@lawrennd</a></li>
<li>podcast: <a href="http://thetalkingmachines.com">The Talking Machines</a></li>
<li>newspaper: <a href="http://www.theguardian.com/profile/neil-lawrence">Guardian Profile Page</a></li>
<li>blog: <a href="http://inverseprobability.com/blog.html">http://inverseprobability.com</a></li>
</ul>
<h1 class="unnumbered" id="references">References</h1>
<div id="refs" class="references hanging-indent" role="doc-bibliography">
<div id="ref-Byrd:lbfgsb95">
<p>Byrd, Richard H., Peihuang Lu, and Jorge Nocedal. 1995. “A Limited Memory Algorithm for Bound Constrained Optimization.” <em>SIAM Journal on Scientific and Statistical Computing</em> 16 (5): 1190–1208.</p>
</div>
<div id="ref-Forrester-engineering08">
<p>Forrester, Alexander I. J., András Sóbester, and Andy J. Keane. 2008. <em>Engineering Design via Surrogate Modelling: A Practical Guide</em>. wiley. <a href="https://doi.org/10.1002/9780470770801">https://doi.org/10.1002/9780470770801</a>.</p>
</div>
<div id="ref-Kennedy-bayesian01">
<p>Kennedy, Marc C., and Anthony O’Hagan. 2001. “Bayesian Calibration of Computer Models.” <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 63 (3): 425–64. <a href="https://doi.org/10.1111/1467-9868.00294">https://doi.org/10.1111/1467-9868.00294</a>.</p>
</div>
<div id="ref-McKay-selecting79">
<p>McKay, Michael D., Richard J. Beckman, and W. Jay Conover. 1979. “A Comparison of Three Methods for Selecting Values of Input Variables in the Analysis of Output from a Computer Code.” <em>Technometrics</em> 21 (2): 239–45. <a href="http://www.jstor.org/stable/1268522">http://www.jstor.org/stable/1268522</a>.</p>
</div>
<div id="ref-Rasmussen:book06">
<p>Rasmussen, Carl Edward, and Christopher K. I. Williams. 2006. <em>Gaussian Processes for Machine Learning</em>. Cambridge, MA: mit.</p>
</div>
<div id="ref-Sacks-design89">
<p>Sacks, Jerome, William J. Welch, Toby J. Mitchell, and Henry P. Wynn. 1989. “Design and Analysis of Computer Experiments.” <em>Statistical Science</em> 4 (4): 409–23. <a href="https://doi.org/10.1214/ss/1177012413">https://doi.org/10.1214/ss/1177012413</a>.</p>
</div>
</div>

