---
title: "Simulation"
venue: "Virtual (Zoom)"
abstract: "<p>This lecture will introduce the notion of simulation and review the different types of simulation we might use to represent the physical world.</p>"
author:
- given: Neil D.
  family: Lawrence
  url: http://inverseprobability.com
  institute: 
  twitter: 
  gscholar: 
  orchid: 
date: 2020-10-22
published: 2020-10-22
week: 3
reveal: 03-simulation.slides.html
ipynb: 03-simulation.ipynb
youtube: "AmI5nq8s4qc"
layout: lecture
categories:
- notes
---



<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!--

-->
<p>Last lecture Carl Henrik introduced you to some of the challenges of approximate inference. Including the problem of mathematical tractability. Before that he introduced you to a particular form of model, the Gaussian process.</p>
<h2 id="bayesian-inference-by-rejection-sampling">Bayesian Inference by Rejection Sampling</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-intro-very-short.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gp-intro-very-short.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>One view of Bayesian inference is to assume we are given a mechanism for generating samples, where we assume that mechanism is representing on accurate view on the way we believe the world works.</p>
<p>This mechanism is known as our <em>prior</em> belief.</p>
<p>We combine our prior belief with our observations of the real world by discarding all those samples that are inconsistent with our prior. The <em>likelihood</em> defines mathematically what we mean by inconsistent with the prior. The higher the noise level in the likelihood, the looser the notion of consistent.</p>
<p>The samples that remain are considered to be samples from the <em>posterior</em>.</p>
<p>This approach to Bayesian inference is closely related to two sampling techniques known as <em>rejection sampling</em> and <em>importance sampling</em>. It is realized in practice in an approach known as <em>approximate Bayesian computation</em> (ABC) or likelihood-free inference.</p>
<p>In practice, the algorithm is often too slow to be practical, because most samples will be inconsistent with the data and as a result the mechanism has to be operated many times to obtain a few posterior samples.</p>
<p>However, in the Gaussian process case, when the likelihood also assumes Gaussian noise, we can operate this mechanism mathematically, and obtain the posterior density <em>analytically</em>. This is the benefit of Gaussian processes.</p>
<p>First we will load in two python functions for computing the covariance function.</p>
<p>Next we sample from a multivariate normal density (a multivariate Gaussian), using the covariance function as the covariance matrix.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a>plot.rejection_samples(kernel<span class="op">=</span>kernel, </span>
<span id="cb1-2"><a href="#cb1-2"></a>    diagrams<span class="op">=</span><span class="st">&#39;../slides/diagrams/gp&#39;</span>)</span></code></pre></div>
<div class="figure">
<div id="gp-rejection-samples-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/gp/gp_rejection_sample003.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<div class="centered" style="">
<img class="" src="../slides/diagrams/gp/gp_rejection_sample004.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
<div class="centered" style="">
<img class="" src="../slides/diagrams/gp/gp_rejection_sample005.png" width="100%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="gp-rejection-samples-magnify" class="magnify" onclick="magnifyFigure(&#39;gp-rejection-samples&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gp-rejection-samples-caption" class="caption-frame">
<p>Figure: One view of Bayesian inference is we have a machine for generating samples (the <em>prior</em>), and we discard all samples inconsistent with our data, leaving the samples of interest (the <em>posterior</em>). This is a rejection sampling view of Bayesian inference. The Gaussian process allows us to do this analytically by multiplying the <em>prior</em> by the <em>likelihood</em>.</p>
</div>
</div>
<p>So, Gaussian processes provide an example of a particular type of model. Or, scientifically, we can think of such a model as a mathematical representation of a hypothesis around data. The rejection sampling view of Bayesian inference can be seen as rejecting portions of that initial hypothesis that are inconsistent with the data. From a Popperian perspective, areas of the prior space are falsified by the data, leaving a posterior space that represents remaining plausible hypotheses.</p>
<p>The flaw with this point of view is that the initial hypothesis space was also restricted. It only contained functions where the instantiated points from the function are jointly Gaussian distributed.</p>
<h2 id="universe-isnt-as-gaussian-as-it-was">Universe isn’t as Gaussian as it Was</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/planck-cmp-master-gp.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/planck-cmp-master-gp.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>The <a href="https://en.wikipedia.org/wiki/Planck_(spacecraft)">Planck space craft</a> was a European Space Agency space telescope that mapped the cosmic microwave background (CMB) from 2009 to 2013. The <a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background">Cosmic Microwave Background</a> is the first observable echo we have of the big bang. It dates to approximately 400,000 years after the big bang, at the time the universe was approximately <span class="math inline">10<sup>8</sup></span> times smaller and the temperature of the Univers was high, around <span class="math inline">3 × 10<sup>8</sup></span> degrees Kelvin. The Universe was in the form of a hydrogen plasma. The echo we observe is the moment when the Universe was cool enough for Protons and electrons to combine to form hydrogen atoms. At this moment, the Universe became transparent for the first time, and photons could travel through space.</p>
<div class="figure">
<div id="planck-spacecraft-figure" class="figure-frame">
<div class="centered centered" style="">
<img class="" src="../slides/diagrams/physics/Front_view_of_the_European_Space_Agency_Planck_satellite.jpg" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="planck-spacecraft-magnify" class="magnify" onclick="magnifyFigure(&#39;planck-spacecraft&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="planck-spacecraft-caption" class="caption-frame">
<p>Figure: Artist’s impression of the Planck spacecraft which measured the Cosmic Microwave Background between 2009 and 2013.</p>
</div>
</div>
<p>The objective of the Planck space craft was to measure the anisotropy and statistics of the Cosmic Microwave Background. This was important, because if the standard model of the Universe is correct the variations around the very high temperature of the Universe of the CMB should be distributed according to a Gaussian process.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> Currently our best estimates show this to be the case <span class="citation" data-cites="Jaffe:cmb98 Pontzen-cmb10 Elsner-unbiased15 Elsner-unbiased16">(Jaffe et al. 1998; Pontzen and Peiris 2010; Elsner, Leistedt, and Peiris 2015, 2016)</span>.</p>
<p>To the high degree of precision that we could measure with the Planck space telescope, the CMB appears to be a Gaussian process. The parameters of its covariance function are given by the fundamental parameters of the universe, for example the amount of dark matter and matter in the universe</p>
<div class="figure">
<div id="cosmic-microwave-background-figure" class="figure-frame">
<div class="centered" style="">
<img class="vertical-align:middle" src="../slides/diagrams/Planck_CMB.png" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="cosmic-microwave-background-magnify" class="magnify" onclick="magnifyFigure(&#39;cosmic-microwave-background&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="cosmic-microwave-background-caption" class="caption-frame">
<p>Figure: The cosmic microwave background is, to a very high degree of precision, a Gaussian process. The parameters of its covariance function are given by fundamental parameters of the universe, such as the amount of dark matter and mass.</p>
</div>
</div>
<h2 id="simulating-a-cmb-map">Simulating a CMB Map</h2>
<p>You can find a Jupyter notebook that allows you to sample from the covariance function to make different Cosmic Microwave Backgrounds <a href="https://github.com/lawrennd/Prob-tools/blob/master/notebooks/The%20CMB%20as%20a%20Gaussian%20Process.ipynb">in this Jupyter notebook</a>.</p>
<p>Here we use that code to simulate our own universe and sample from what it looks like.</p>
<p>First we install some specialist software as well as <code>matplotlib</code>, <code>scipy</code>, <code>numpy</code> we require</p>
<ul>
<li><code>camb</code>: <a href="http://camb.readthedocs.io/en/latest/" class="uri">http://camb.readthedocs.io/en/latest/</a></li>
<li><code>healpy</code>: <a href="https://healpy.readthedocs.io/en/latest/" class="uri">https://healpy.readthedocs.io/en/latest/</a></li>
</ul>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="op">%</span>pip install camb</span></code></pre></div>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="op">%</span>pip install healpy</span></code></pre></div>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb4-2"><a href="#cb4-2"></a><span class="op">%</span>config IPython.matplotlib.backend <span class="op">=</span> <span class="st">&#39;retina&#39;</span></span>
<span id="cb4-3"><a href="#cb4-3"></a><span class="op">%</span>config InlineBackend.figure_format <span class="op">=</span> <span class="st">&#39;retina&#39;</span></span>
<span id="cb4-4"><a href="#cb4-4"></a></span>
<span id="cb4-5"><a href="#cb4-5"></a><span class="im">import</span> matplotlib</span>
<span id="cb4-6"><a href="#cb4-6"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-7"><a href="#cb4-7"></a><span class="im">from</span> matplotlib <span class="im">import</span> rc</span>
<span id="cb4-8"><a href="#cb4-8"></a><span class="im">from</span> cycler <span class="im">import</span> cycler</span>
<span id="cb4-9"><a href="#cb4-9"></a></span>
<span id="cb4-10"><a href="#cb4-10"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-11"><a href="#cb4-11"></a></span>
<span id="cb4-12"><a href="#cb4-12"></a>rc(<span class="st">&quot;font&quot;</span>, family<span class="op">=</span><span class="st">&quot;serif&quot;</span>, size<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb4-13"><a href="#cb4-13"></a>rc(<span class="st">&quot;text&quot;</span>, usetex<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb4-14"><a href="#cb4-14"></a>matplotlib.rcParams[<span class="st">&#39;lines.linewidth&#39;</span>] <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb4-15"><a href="#cb4-15"></a>matplotlib.rcParams[<span class="st">&#39;patch.linewidth&#39;</span>] <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb4-16"><a href="#cb4-16"></a>matplotlib.rcParams[<span class="st">&#39;axes.prop_cycle&#39;</span>] <span class="op">=\</span></span>
<span id="cb4-17"><a href="#cb4-17"></a>    cycler(<span class="st">&quot;color&quot;</span>, [<span class="st">&#39;k&#39;</span>, <span class="st">&#39;c&#39;</span>, <span class="st">&#39;m&#39;</span>, <span class="st">&#39;y&#39;</span>])</span>
<span id="cb4-18"><a href="#cb4-18"></a>matplotlib.rcParams[<span class="st">&#39;axes.labelsize&#39;</span>] <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb4-19"><a href="#cb4-19"></a></span>
<span id="cb4-20"><a href="#cb4-20"></a><span class="im">import</span> healpy <span class="im">as</span> hp</span>
<span id="cb4-21"><a href="#cb4-21"></a></span>
<span id="cb4-22"><a href="#cb4-22"></a><span class="im">import</span> camb</span>
<span id="cb4-23"><a href="#cb4-23"></a><span class="im">from</span> camb <span class="im">import</span> model, initialpower</span></code></pre></div>
<p>Now we use the theoretical power spectrum to design the covariance function.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a>nside <span class="op">=</span> <span class="dv">512</span>  <span class="co"># Healpix parameter, giving 12*nside**2 equal-area pixels on the sphere.</span></span>
<span id="cb5-2"><a href="#cb5-2"></a>lmax <span class="op">=</span> <span class="dv">3</span><span class="op">*</span>nside <span class="co"># band-limit. Should be 2*nside &lt; lmax &lt; 4*nside to get information content.</span></span></code></pre></div>
<p>Now we design our Universe. It is parameterised according to the <a href="https://en.wikipedia.org/wiki/Lambda-CDM_model"><span class="math inline"><em>Λ</em></span>CDM model</a>. The variables are as follows. <code>H0</code> is the Hubble parameter (in Km/s/Mpc). The <code>ombh2</code> is Physical Baryon density parameter. The <code>omch2</code> is the physical dark matter density parameter. <code>mnu</code> is the sum of the neutrino masses (in electron Volts). <code>omk</code> is the <span class="math inline"><em>Ω</em><sub><em>k</em></sub></span> is the curvature parameter, which is here set to 0, tiving the minimal six parameter Lambda-CDM model. <code>tau</code> is the reionization optical depth.</p>
<p>Then we set <code>ns</code>, the “scalar spectral index”. This was estimated by Planck to be 0.96. Then there’s <code>r</code>, the ratio of the tensor power spectrum to scalar power spectrum. This has been estimated by Planck to be under 0.11. Here we set it to zero. These parameters are associated <a href="https://en.wikipedia.org/wiki/Primordial_fluctuations">with inflation</a>.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a><span class="co"># Mostly following http://camb.readthedocs.io/en/latest/CAMBdemo.html with parameters from https://en.wikipedia.org/wiki/Lambda-CDM_model</span></span>
<span id="cb6-2"><a href="#cb6-2"></a></span>
<span id="cb6-3"><a href="#cb6-3"></a>pars <span class="op">=</span> camb.CAMBparams()</span>
<span id="cb6-4"><a href="#cb6-4"></a>pars.set_cosmology(H0<span class="op">=</span><span class="fl">67.74</span>, ombh2<span class="op">=</span><span class="fl">0.0223</span>, omch2<span class="op">=</span><span class="fl">0.1188</span>, mnu<span class="op">=</span><span class="fl">0.06</span>, omk<span class="op">=</span><span class="dv">0</span>, tau<span class="op">=</span><span class="fl">0.066</span>)</span>
<span id="cb6-5"><a href="#cb6-5"></a>pars.InitPower.set_params(ns<span class="op">=</span><span class="fl">0.96</span>, r<span class="op">=</span><span class="dv">0</span>)</span></code></pre></div>
<p>Having set the parameters, we now use the python software “Code for Anisotropies in the Microwave Background” to get the results.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a>pars.set_for_lmax(lmax, lens_potential_accuracy<span class="op">=</span><span class="dv">0</span>)<span class="op">;</span></span>
<span id="cb7-2"><a href="#cb7-2"></a>results <span class="op">=</span> camb.get_results(pars)</span>
<span id="cb7-3"><a href="#cb7-3"></a>powers <span class="op">=</span> results.get_cmb_power_spectra(pars)</span>
<span id="cb7-4"><a href="#cb7-4"></a>totCL <span class="op">=</span> powers[<span class="st">&#39;total&#39;</span>]</span>
<span id="cb7-5"><a href="#cb7-5"></a>unlensedCL <span class="op">=</span> powers[<span class="st">&#39;unlensed_scalar&#39;</span>]</span>
<span id="cb7-6"><a href="#cb7-6"></a></span>
<span id="cb7-7"><a href="#cb7-7"></a>ells <span class="op">=</span> np.arange(totCL.shape[<span class="dv">0</span>])</span>
<span id="cb7-8"><a href="#cb7-8"></a>Dells <span class="op">=</span> totCL[:, <span class="dv">0</span>]</span>
<span id="cb7-9"><a href="#cb7-9"></a>Cells <span class="op">=</span> Dells <span class="op">*</span> <span class="dv">2</span><span class="op">*</span>np.pi <span class="op">/</span> ells <span class="op">/</span> (ells <span class="op">+</span> <span class="dv">1</span>)  <span class="co"># change of convention to get C_ell</span></span>
<span id="cb7-10"><a href="#cb7-10"></a>Cells[<span class="dv">0</span>:<span class="dv">2</span>] <span class="op">=</span> <span class="dv">0</span></span></code></pre></div>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a>cmbmap <span class="op">=</span> hp.synfast(Cells, nside, </span>
<span id="cb8-2"><a href="#cb8-2"></a>                 lmax<span class="op">=</span>lmax, mmax<span class="op">=</span><span class="va">None</span>, alm<span class="op">=</span><span class="va">False</span>, pol<span class="op">=</span><span class="va">False</span>, </span>
<span id="cb8-3"><a href="#cb8-3"></a>                 pixwin<span class="op">=</span><span class="va">False</span>, fwhm<span class="op">=</span><span class="fl">0.0</span>, sigma<span class="op">=</span><span class="va">None</span>, new<span class="op">=</span><span class="va">False</span>, verbose<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<div class="figure">
<div id="mollweide-sample-cmb-figure" class="figure-frame">
<div class="centered" style="">
<img class="vertical-align:middle" src="../slides/diagrams/physics/mollweide-sample-cmb.png" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="mollweide-sample-cmb-magnify" class="magnify" onclick="magnifyFigure(&#39;mollweide-sample-cmb&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="mollweide-sample-cmb-caption" class="caption-frame">
<p>Figure: A simulation of the Cosmic Microwave Background obtained through sampling from the relevant Gaussian process covariance (in polar co-ordinates).</p>
</div>
</div>
<p>The simulation was created by <a href="https://ixkael.github.io/">Boris Leistedt</a>, see the <a href="https://github.com/ixkael/Prob-tools/blob/master/notebooks/The%20CMB%20as%20a%20Gaussian%20Process.ipynb">original Jupter notebook here</a>.</p>
<p>The world we see today, of course, is not a Gaussian process. There are many dicontinuities, for example, in the density of matter, and therefore in the temperature of the Universe.</p>
<div class="figure">
<div id="modern-universe-non-linear-function-figure" class="figure-frame">
<div style="fontsize:120px;vertical-align:middle">
<img src="../slides/diagrams/earth_PNG37.png" width="20%" style="display:inline-block;background:none;vertical-align:middle;border:none;box-shadow:none;"><span class="math inline"> = <em>f</em>(</span><img src="../slides/diagrams/Planck_CMB.png"  width="50%" style="display:inline-block;background:none;vertical-align:middle;border:none;box-shadow:none;"><span class="math inline">)</span>
</div>
</div>
<div id="modern-universe-non-linear-function-magnify" class="magnify" onclick="magnifyFigure(&#39;modern-universe-non-linear-function&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="modern-universe-non-linear-function-caption" class="caption-frame">
<p>Figure: What we observe today is some non-linear function of the cosmic microwave background.</p>
</div>
</div>
<p>We can think of todays observed Universe, though, as a being a consequence of those temperature fluctuations in the CMB. Those fluctuations are only order <span class="math inline">10<sup>−</sup>6</span> of the scale of the overal temperature of the Universe. But minor fluctations in that density is what triggered the pattern of formation of the Galaxies and how stars formed and created the elements that are the building blocks of our Earth <span class="citation" data-cites="Vogelsberger-cosmological20">(Vogelsberger et al. 2020)</span>.</p>
<p>Those Cosmological simulations are based on a relatively simple set of ‘rules’ that stem from our understanding of natural laws. These ‘rules’ are mathematical abstractions of the physical world. Representations of behaviour in mathematical form that capture the interaction forces between particles. The grand aim of physics has been to unify these rules into a single unifying theory. Popular understanding of this quest developed as a result of Stephen Hawking’s book, “<a href="https://en.wikipedia.org/wiki/A_Brief_History_of_Time">A Brief History of Time</a>”. The idea of these laws as ‘ultimate causes’ has given them a pseudo religious feel, see for example Paul Davies’s book “<a href="https://en.wikipedia.org/wiki/The_Mind_of_God">The Mind of God</a>” which comes from a quotation form Stephen Hawking.</p>
<blockquote>
<p>If we do discover a theory of everything … it would be the ultimate triumph of human reason-for then we would truly know the mind of God</p>
<p>Stephen Hawking in <em>A Brief History of Time</em> 1988</p>
</blockquote>
<p>This is an entrancing quote, that seems to work well for selling books (A Brief History of Time sold over 10 million copies), but as Laplace has already pointed out to us, the Universe doesn’t work quite so simply as that. Commonly, God is thought to be omniscient, but having a grand unifying theory alone doesn’t give us omniscience.</p>
<p>Laplace’s demon still applies. Even if we had a grand unifying theory, which encoded “all the forces that set nature in motion” we have an amount of work left to do in any quest for ‘omniscience’.</p>
<blockquote>
<p>We may regard the present state of the universe as the effect of its past and the cause of its future. An intellect which at a certain moment would know all forces that set nature in motion, and all positions of all items of which nature is composed, …</p>
</blockquote>
<blockquote>
<p>… if this intellect were also vast enough to submit these data to analysis, it would embrace in a single formula the movements of the greatest bodies of the universe and those of the tiniest atom; for such an intellect nothing would be uncertain and the future just like the past would be present before its eyes.</p>
<p>— Pierre Simon Laplace <span class="citation" data-cites="Laplace-essai14">(Laplace 1814)</span></p>
</blockquote>
<p>We summarized this notion as <br /><span class="math display">$$
\text{data} + \text{model} \stackrel{\text{compute}}{\rightarrow} \text{prediction}
$$</span><br /> As we pointed out, there is an irony in Laplace’s demon forming the cornerstone of a movement known as ‘determinism’, because Laplace wrote about this idea in an essay on probabilities. The more important quote in the essay was</p>
<blockquote>
<p>The curve described by a simple molecule of air or vapor is regulated in a manner just as certain as the planetary orbits; the only difference between them is that which comes from our ignorance.</p>
</blockquote>
<blockquote>
<p>Probability is relative, in part to this ignorance, in part to our knowledge. We know that of three or greater number of events a single one ought to occur; but nothing induces us to believe that one of them will occur rather than the others. In this state of indecision it is impossible for us to announce their occurrence with certainty. It is, however, probable that one of these events, chosen at will, will not occur because we see several cases equally possible which exclude its occurrence, while only a single one favors it.</p>
<p>— Pierre-Simon Laplace <span class="citation" data-cites="Laplace-essai14">(Laplace 1814)</span></p>
</blockquote>
<p>The representation of ignorance through probability is the true message of Laplace, I refer to this message as “Laplace’s gremlin”, because it is the gremlin of uncertainty that interferes with the demon of determinism to mean that our predictions are not deterministic.</p>
<p>Our separation of the uncertainty into the data, the model and the computation gives us three domains in which our doubts can creep into our ability to predict. Over the last three lectures we’ve introduced some of the basic tools we can use to unpick this uncertainty. In particular, you’ve been introduced to, (or have yow reviewed) <em>Bayes’ rule</em>. The rule, which is a simple consequence of the product rule of probability, is the foundation of how we update our beliefs in the presence of new information.</p>
<p>Carl Henrik described how a prior probability <span class="math inline">$p(\parameterVector)$</span> represents our hypothesis about the way the world might behave. This can be combined with a <em>likelihood</em> through the process of multiplication. Correctly normalized, this gives an updated hypothesis that represents our <em>posterior</em> belief about the model in the light of the data.</p>
<p>There is a nice symmetry between this approach and how Karl Popper describes the process of scientific discovery. In conjectures and refutations, Popper describes the process of scientific discovery as involving hypothesis and experiment. In our description hypothesis maps onto the <em>model</em>. The model is an abstraction of the hypothesis, represented for example as a set of mathematical equations, a computational description or an analogous system (physical system). The data is the product of previous experiments, our readings, our observation of the world around us. We can combine these to make a prediction about what we might expect the future to hold. Popper’s view on the philosophy of science was that the prediction should be falsifiable.</p>
<p>We can see this process as a spiral driving forward, importantly Popper relates the relationship between hypothesis (model) and experiment (predictions) as akin to the relationship between the chicken and the egg. Which comes first? The answer is that they co-evolve together.</p>
<div class="figure">
<div id="experiment-analyze-design-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/ml/experiment-analyze-design.svg" width="50%" style=" ">
</object>
</div>
<div id="experiment-analyze-design-magnify" class="magnify" onclick="magnifyFigure(&#39;experiment-analyze-design&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="experiment-analyze-design-caption" class="caption-frame">
<p>Figure: Experiment, analyze and design is a flywheel of knowledge that is the dual of the model, data and compute. By running through this spiral, we refine our hypothesis/model and develop new experiments which can be analyzed to further refine our hypothesis.</p>
</div>
</div>
<div class="figure">
<div id="-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/physics/different-models.svg" width="90%" style=" ">
</object>
</div>
<div id="-magnify" class="magnify" onclick="magnifyFigure(&#39;&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="-caption" class="caption-frame">
<p>Figure: The sets of different models. There are all the models in the Universe we might like to work with. Then there are those models that are computable e.g. by a Turing machine. Then there are those which are analytical tractable. I.e. where the solution might be found analytically. Finally, there are Gaussian processes, where the joint distribution of the states in the model is Gaussian.</p>
</div>
</div>
<p>The approach we’ve taken to the model so far has been severely limiting. By constraining ourselves to models for which the mathematics of probability is tractable, we severely limit what we can say about the universe.</p>
<p>Although Bayes’ rule only implies multiplication of probabilities, to acquire theposterior we also need to normalize. Very often it is this normalization step that gets in the way. The normalization step involves integration over the updated hypothesis space, to ensure the updated posterior prediction is correct.</p>
<p>We can map the process of Bayesian inference onto the the <span class="math inline">model + data</span> perspective in the following way. We can see the model as the prior, the data as the likelihood and the prediction as the posterior<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>.</p>
<p>So, if we think of our model as incorporating what we know about the physical problem of interest (from Newton, or Bernoulli or Laplace or Einstein or whoever) and the data as being the observations (e.g. from Piazzi’s telescope or a particle accelerator) then we can make predictions about what we might expect to happen in the future by combining the two. It is <em>those</em> predictions that Popper sees as important in verifying the scientific theory (which is incorporated in the model).</p>
<p>But while Gaussian processes are highly flexible non-parametric function models, they are <em>not</em> going to be sufficient to capture the type of physical processes we might expect to encounter in the real world. To give a sense, let’s consider a few examples of the phenomena we might want to capture, either in the scientific world, or in real world decision making.</p>
<h1 id="precise-physical-laws">Precise Physical Laws</h1>
<p>We’ve already reviewed the importance of Newton’s laws in forging our view of science: we mentioned the influence <a href="https://en.wikipedia.org/wiki/Christiaan_Huygens">Christiaan Huygens’</a> work on collisions had on Daniel Bernoulli in forming the kinetic theory of gases. These ideas inform many of the physical models we have today around a number of natural phenomena. The MET Office super computer in Exeter spends its mornings computing the weather across the world, and in its afternoons it’s used for climate modelling. It uses the same set of principles that Newton and Bernoulli explored for gases. They are encoded in the Navier-Stokes equations. The rules that govern the flow of compressible and incompressible fluids. As well as predicting our weather, these equations are used in fluid dynamics models to understand the flight of aircraft, the driving characteristics of racing cars and the efficiency of gas turbine engines.</p>
<p>This broad class of physical models, or ‘natural laws’ is probably the closest to what Laplace was referring to in the Demon. The search for unifying physical laws that dictate everything we observe around us has gone on. Alongside Newton we must mention James Clerk Maxwell, who unified electricity and magnetism in one set of equations that were inspired by the work and ideas of Michael Faraday. And still today we look for unifying equations that bring together in a single mathematical model the ‘natural laws’ we observe. One equation that for Laplace would be “all forces that set nature in motion”. We can think of this as our first time of physical model, a ‘precise model’ of the known laws of our Universe, a model where we expect that the mapping from the mathematical abstraction to the physical reality is ‘exact.’<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<h2 id="abstraction-and-emergent-properties">Abstraction and Emergent Properties</h2>
<div class="figure">
<div id="simulation-scales-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/physics/simulation-scales.svg" width="90%" style=" ">
</object>
</div>
<div id="simulation-scales-magnify" class="magnify" onclick="magnifyFigure(&#39;simulation-scales&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="simulation-scales-caption" class="caption-frame">
<p>Figure: A scale of different simulations we might be interested in when modelling the physical world. The scale is <span class="math inline">log<sub>10</sub></span> meters. The scale reflects something about the level of granularity where we might choose to know “all positions of all items of which nature is composed”.</p>
</div>
</div>
<p>Unfortunately, even if such an equation were to exist, we would be unlikely to know “all positions of all items of which nature is composed”. A good example here is computational systems biology. In that domain we are interested in understanding the undelying function of the cell. These systems sit somewhere between the two extremes that Laplace described: “the movements of the greatest bodies of the universe and those of the smallest atom”.</p>
<p>When the smallest atom is considered, we need to introduce uncertainty. We again turn to a different work of Maxwell, building on Bernoulli’s kinetic theory of gases we end up with probabilities for representing the location of the ‘molecules of air’. Instead of a deterministic location for these particles we represent our belief about their location in a distribution.</p>
<p>Computational systems biology is a world of micro-machines, built of three dimensional foldings of strings of proteins. There are spindles (stators) and rotors (e.g. <a href="https://en.wikipedia.org/wiki/ATP_synthase">ATP Synthase</a>), there are small copying machines (e.g. <a href="https://en.wikipedia.org/wiki/RNA_polymerase">RNA Polymerase</a>) there are sequence to sequence translators (<a href="https://en.wikipedia.org/wiki/Ribosome">Ribosomes</a>). The cells store information in DNA, but have an ecosystem of structures and messages being sent and built in proteins and RNA. Unpikcing these structures has been a major preoccupation of biology. That is knowing where the atoms of these molecules are in the structure, and how the parts of the structure move when these small micro-machines are carrying out their roles.</p>
<p>We understand most (if not all) of the physical laws that drive the movements of these molecules, but we don’t understand all the actions of the cell, nor can we intervene reliably to improve things. So even in the case where we have a good understanding of the physical laws, Laplace’s gremlin emerges in our knowledge of “the positions of all items of which nature is composed”.</p>
<h2 id="molecular-dynamics-simulations">Molecular Dynamics Simulations</h2>
<p>By understanding and simulating the physics, we can recreate operations that are happening at the level of proteins in the human cell. <a href="https://en.wikipedia.org/wiki/V-ATPase">V-ATPase</a> is an enzyme that pumps protons. But at the microscopic level it’s a small machine. produces ATP in response to a proton gradient. A recent paper in Science Advanes simulates the functioning of these proteins that operate across from The response to this is to use a mathematical model which (somewhat) abstracts the processes. You can also check this <a href="https://www6.slac.stanford.edu/news/2020-10-07-first-detailed-look-how-molecular-ferris-wheel-delivers-protons-cellular-factories">blog post</a> from the paper’s press release.</p>
<div class="figure">
<div id="v-atp-ase-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/sysbio/rotary_proton_sv_pump_anim_final.gif" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="v-atp-ase-magnify" class="magnify" onclick="magnifyFigure(&#39;v-atp-ase&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="v-atp-ase-caption" class="caption-frame">
<p>Figure: The V-ATPase enzyme pumps proteins across membranes. This molecular dynamics simulation was recently published in Science Advances <span class="citation" data-cites="Roh-cryo-em20">(Roh et al. 2020)</span>. The scale is roughly <span class="math inline">10<sup> − 8</sup><em>m</em></span>.</p>
</div>
</div>
<h2 id="quantum-mechanics">Quantum Mechanics</h2>
<p>Alternative we can drop down a few scales and consider simulation of the Schrödinger equation. A recent paper uses deep neural networks to speed up the solution of the many-electron Schrödinger equation enabling simulation of chemical bonds <span class="citation" data-cites="Pfau-abinitio20">(Pfau et al. 2020)</span>. The <a href="https://deepmind.com/blog/article/FermiNet">PR-blog post is also available</a> and is rather better written than the V-ATPase version. The paper uses a neural network to model the quantum state of a number of elctrons.</p>
<div class="figure">
<div id="many-electron-schroedinger-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/physics/many-electron-schroedinger.gif" width="40%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="many-electron-schroedinger-magnify" class="magnify" onclick="magnifyFigure(&#39;many-electron-schroedinger&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="many-electron-schroedinger-caption" class="caption-frame">
<p>Figure: The many-electron Schroedinger equation is important in understanding how Chemical bonds are formed.</p>
</div>
</div>
<p>Each of these simulations have the same property of being based on a set of (physical) rules about how particles interact. But one of the interesting characteristics of such systems is how the properties of the system are emergent as the dynamics are allowed to continue.</p>
<p>These properties cannot be predicted without running the physics, or the equivalently the equation. Computation is required. And often the amount of computation that is required is prohibitive.</p>
<h2 id="accelerate-programme">Accelerate Programme</h2>
<p>The Computer Lab is hosting a new initiative known as the Accelerate Programme for Scientific Discovery. The aim is to address scientific challenges, and accelerate the progress of research, through using tools in machine learning.</p>
<p>We now have four fellows appointed, each of whom works at the interface of machine learning and scientific discovery. They are using the ideas around machine learning modelling to drive their scientific research.</p>
<p>For example, <a href="https://sites.google.com/site/tonicbq/">Bingqing Cheng</a>, one of the Department’s new DECAF Fellows has used neural network accelerated molecular dynamics simulations to understand a new form of metallic hydrogen, likely to occur at the heart of stars <span class="citation" data-cites="Cheng-evidence20">(Cheng et al. 2020)</span>. The University’s <a href="https://www.cam.ac.uk/research/news/ai-shows-how-hydrogen-becomes-a-metal-inside-giant-planets">press release is here</a>.</p>
<p>On her website Bingqing quotes Paul Dirac.</p>
<blockquote>
<p>The fundamental laws necessary for the mathematical treatment of a large part of physics and the whole of chemistry are thus completely known, and the difficulty lies only in the fact that application of these laws leads to equations that are too complex to be solved.</p>
</blockquote>
<blockquote>
<p>..approximate practical methods of applying quantum mechanics should be developed, which can lead to an explanation of the main features of complex atomic systems without too much computation.</p>
<p>— Paul Dirac (6 April 1929)</p>
</blockquote>
<p>As well as Bingqing, we have appointed <a href="https://oatml.cs.ox.ac.uk/members/challenger_mishra/">Challenger Mishra</a>, a physicist interested in string theoryand quantising gravity. <a href="https://www.neuroscience.cam.ac.uk/directory/profile.php?SarahMorgan">Sarah Morgan</a> from the Brain Mapping Unit, who is focussed on predicting psychosis trajectories and <a href="https://b2du.github.io/">Bianca Dumitrascu</a> who focusses on the interface of machine learning and biology.</p>
<p>For those interested in Part III/MPhil projects, you can see their project suggestions on <a href="https://www.cst.cam.ac.uk/teaching/masters/projects/suggestions">this page</a>.</p>
<h2 id="surrogate-models-and-emulators">Surrogate Models and Emulators</h2>
<p>There are a number of ways we can use machine learning to accelerate scientific discovery. But one way is to have the machine learning model learn the effect of the rules. Rather than worrying about the detail of the rules through coputing each step, we can have the machine learning model look to abstract the rules and capture emergent phenomena, just as the Maxwell-Boltzmann distribution captures the essence of the behaviour of the ideal gas.</p>
<p>In the papers listed above, neural networks are being used to speed up computations. In this course we’ve introduced Gaussian processes that will be used to speed up these computations. In both cases the ideas are similar. Rather than rerunning the simulation, we use data from the simulation to <em>fit</em> the neural network or the Gaussian process to the data.</p>
<p>We’ll see an example of how this is done in a moment, taken from a simple ride hailing simulator, but before we look at that, we’ll first consider why this might be a useful approach.</p>
<h2 id="game-of-life">Game of Life</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_simulation/includes/game-of-life.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_simulation/includes/game-of-life.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p><a href="https://en.wikipedia.org/wiki/John_Horton_Conway">John Horton Conway</a> was a mathematician who developed a game known as the Game of Life. He unfortunately died in April 2020, but since he invented the game he was in effect ‘god’ for this game. But as we will see, just inventing the rules doesn’t give you omniscience in the game.</p>
<p>The Game of Life is played on a grid of squares, or pixels. Each pixel is either on or off. The game has no players, but a set of simple rules that are followed at each turn the rules are.</p>
<ul>
<li><strong>Survival</strong> Every pixel surrounded by two or three other pixels survives for the next turn.</li>
<li><strong>Death</strong> Each pixel surrounded by four or more pixels dies from overpopulation. Likewise, every pixel next to one or no pixels at all dies from isolation.</li>
<li><strong>Birth</strong> Each square adjacent to exactly three pixels gives birth to a new pixel.</li>
</ul>
<p>And that’s it. Those are the simple ‘physical laws’ for Conway’s game.</p>
<p>The game leads to patterns emerging, some of these patterns are static, but some oscilate, with varying periods. Others oscilalate, but when they complete their cycle they’ve translated to a new location, in other words they move. In Life the former are known as <a href="https://conwaylife.com/wiki/Oscillator">oscillators</a> and the latter as <a href="https://conwaylife.com/wiki/Spaceship">spaceships</a>.</p>
<div class="figure">
<div id="glider-gif-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/simulation/Glider.gif" width="50%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="glider-gif-magnify" class="magnify" onclick="magnifyFigure(&#39;glider-gif&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="glider-gif-caption" class="caption-frame">
<p>Figure: The glider is an oscillator that moves diagonally after creation. From the simple rules of Life it’s not obvious that such an object does exist, until you do the necessary computation.</p>
</div>
</div>
<div class="figure">
<div id="gosper-glider-gun-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/simulation/Gosperglidergun.gif" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="gosper-glider-gun-magnify" class="magnify" onclick="magnifyFigure(&#39;gosper-glider-gun&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gosper-glider-gun-caption" class="caption-frame">
<p>Figure: The Gosper glider gun is a configuration that creates gliders. A new glider is released after every 30 turns.</p>
</div>
</div>
<p>These patterns had to be discovered, in the same way that a scientist might discover a disease, or an explorer a new land. For example, the Gosper glider gun was <a href="https://conwaylife.com/wiki/Bill_Gosper">discovered by Bill Gosper in 1970</a>.</p>
<p>Despite widespread interest in Life, some of its patterns were only very recently discovered like the Loafer, discovered in 2013 by Josh Ball.</p>
<div class="figure">
<div id="the-loafer-spaceship-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/simulation/Loafer.gif" width="60%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="the-loafer-spaceship-magnify" class="magnify" onclick="magnifyFigure(&#39;the-loafer-spaceship&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="the-loafer-spaceship-caption" class="caption-frame">
<p>Figure: The Loafer, discovered by Josh Ball in 2013 is named for its slow movement.</p>
</div>
</div>
<p>Once these patterns are discovered, they are combined (or engineered) to create new Life patterns that do some remarkable things. For example there’s a life pattern that runs a Turing machine, or more remarkably there’s a Life pattern that runs Life itself.</p>
<div class="figure">
<div id="life-in-life-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/simulation/life-in-life.gif" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="life-in-life-magnify" class="magnify" onclick="magnifyFigure(&#39;life-in-life&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="life-in-life-caption" class="caption-frame">
<p>Figure: The Game of Life running in Life. The video is drawing out recursively showing pixels that are being formed by filling cells with moving spaceships. Each individual pixel in this game of life is made up of <span class="math inline">2048 × 2048</span> pixels called an <a href="https://www.conwaylife.com/wiki/OTCA_metapixel">OTCA metapixel</a>.</p>
</div>
</div>
<p>To find out more about the Game of Life you can watch this video by Alan Zucconi or read his <a href="https://www.alanzucconi.com/2020/10/13/conways-game-of-life/">associated blog post</a>.</p>
<div class="figure">
<div id="intro-to-life-figure" class="figure-frame">
<iframe width="800" height="600" src="https://www.youtube.com/embed/Kk2MH9O4pXY?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
<div id="intro-to-life-magnify" class="magnify" onclick="magnifyFigure(&#39;intro-to-life&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="intro-to-life-caption" class="caption-frame">
<p>Figure: An introduction to the Game of Life by Alan Zucconi.</p>
</div>
</div>
<!--
Inspired by <https://gist.github.com/jiffyclub/3778422#file-game_of_life-ipynb>
```{.python}
import numpy as np
from scipy.signal import convolve
```

```{.python}
# used for counting the number of living neighbors each cell has
FILTER = np.array([[1, 1, 1],
                   [1, 100, 1],
                   [1, 1, 1]], dtype=np.uint8)
```


```{.python}
def evolve(length, generations):
    """
    Run the Game of Life. Starting state is random.

    Parameters
    ----------
    length : int
        Universe will be `length` units per side.
    generations : int
        Number of generations to run simulation.

    """
    current = np.random.randint(2, size=(length, length))
    next = np.empty_like(current)
    current[length//2, 1:(length-1)] = 1
    show_board(current)
    for _ in range(generations):
        advance(current, next)
        current, next = next, current
        show_board(current)
```

```{.python}
def advance(current, next):
    """
    Calculate the next iteration of the Game of Life.

    Parameters
    ----------
    current : 2D array
        Current state of universe.
    next : 2D array
        This array will be modified in place so that it contains the
        next step. Must be the same size as `current`.

    """
    assert current.shape[0] == current.shape[1], \
           'Expected square universe'
    next[:] = 0
    count = convolve(current, FILTER, mode='same')
    next[(count == 3) | (count == 102) | (count == 103)] = 1
```
-->
<h1 id="modelling-in-practice">Modelling in Practice</h1>
<p>As we’ve seen from the very simple rules in the Game of Life, emergent phenomena we might be interested in take computation power to discover, just as Laplace’s and Dirac’s quotes suggest. The objective in surrogate modelling is to harness machine learning models to learn those physical characteristics.</p>
<h2 id="types-of-simulations">Types of Simulations</h2>
<p>We’ve introduced simulations from the perspective of laws of physics. In practice, many simulations may not directly encode for the laws of physics, but they might encode expert intuitions about a problem.</p>
<p>For example, in Formula 1 races, the cars have tyres that wear at different rates. Softer tyres allow the cars to drive faster, but wear quicker. Harder tyres man the car drives slower but they last longer. Changing between tyres is part of the race, and it has a time penalty. Before each race the teams decide what their strategy will be with tyre changes. It’s not only how many tyre changes that are important, but when they happen. If you change your tyre early, you might get a speed advantage and be able to pass your rival when they change their tyre later. This is a trick known as ‘undercutting’, but if your early change puts you back onto the track behind other slower cars, you will loose this advantage.</p>
<p>Formula 1 teams determine their strategy through simulating the race. Each team knows how fast other teams are around the track, and what their top speeds are. So the teams simulate many thousands or millions of races with different strategies for their rivals, and they choose the strategy for which they maximize their number of expected points.</p>
<p>When many simulations are done, the results take time to come. During the actual race, the simulations are too slow to provide the real time information teams would need. In this case F1 teams can use emulators, models that have learnt the effect of the simulations, to give real time updates</p>
<p>Formula 1 race simulations contain assumptions that derive from physics but don’t directly encode the physical laws. For example, if one car is stuck behind another, in any given lap, it might overtake. A typical race simulation will look at the lap speed of each car and the top speed of each car (as measured in ‘speed traps’ that are placed on the straight). It will assume a probability of overtake for each lap that is a function of these values. Of course, underlying that function is the physics of how cars overtake each other, but that can be abstracted away into a simpler function that the Race Strategy Engineer defines from their knowledge and previous experience.</p>
<p>Many simulations have this characteristic: major parts of the simulation are the result of encoding expert knowledge in the code. But this can lead to challenges. I once asked a strategy engineer, who had completed a new simulation, how it was going. He replied that things had started out well, but over time its performance was degrading. We discussed this for a while and over time a challenge of misspecified granularity emerged.</p>
<h2 id="fidelity-of-the-simulation">Fidelity of the Simulation</h2>
<p>The engineer explained how there’d been a race where the simulation had suggested that their main driver <em>shouldn’t</em> pit because he would have emerged behind a car with a slower lap speed, but a high top speed. This would have made that car difficult to overtake. However, the driver of that slower car was also in the team’s ‘development program’, so every one in the team knew that the slower car would have moved aside to let their driver through. Unfortunately, the simulation didn’t know this. So the team felt the wrong stategy decision was made. After the race, the simulation was updated to include a special case for this situation. The new code checked whether the slower car was a development driver, making it ‘more realistic’.</p>
<p>Over time there were a number of similar changes, each of which should have improved the simulation, but the reality was the code was now ‘mixing granularities’. The formula for computing the probability of over take as a function of speeds is one that is relatively easy to verify. It ignores the relationships between drivers, whether or not a given driver is a development driver, whether one bears a grudge or not, whether one is fighting for their place in the team. That’s all assimilated into the equation. The original equation is easy to calibrate, but as soon as you move to a finer granularity and consider more details about indvidual drivers, the model seems more realistic but it becomes difficult to specify, and therefore performance degrades.</p>
<p>Simulations work at different fidelities, but as the Formula 1 example shows you have to be very careful about mixing fidelities within the same simulation. The appropriate fidelity of a simulation is strongly dependent on the question being asked of it. On the context. For example, in Formula 1 races you can also simulate the performance of the car in the wind tunnel and using computational fluid dynamics represenations of the Navier Stokes equations. That level of fidelity <em>is</em> appropriate when designing the aerodynamic components of the car, but inappropriate when building a strategy simulation.</p>
<h1 id="epidemiology">Epidemiology</h1>
<p>The same concept of modelling at a particular fidelity comes up in epidemiology. In reality, disease is transmitted by direct person to person interactions between individuals. But in theoretical epidemiology, this is approximated by differential equations. The resulting models look very similar to reaction rate models used in Chemistry for well mixed beakers. Let’s have a look at a simple example used for modelling the policy of ‘herd immunity’ for Covid19.</p>
<h2 id="modelling-herd-immunity">Modelling Herd Immunity</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_simulation/includes/herd-immunity.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_simulation/includes/herd-immunity.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>This example is taken from <a href="https://personalpages.manchester.ac.uk/staff/thomas.house/blog/modelling-herd-immunity.html">Thomas House’s blog post</a> on Herd Immunity. This model was shared at the beginning of the Covid19 pandemic when the first UK lockdown hadn’t yet occurred.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a><span class="co"># Pull in libraries needed</span></span>
<span id="cb9-2"><a href="#cb9-2"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb9-3"><a href="#cb9-3"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb9-4"><a href="#cb9-4"></a><span class="im">from</span> scipy <span class="im">import</span> integrate</span>
<span id="cb9-5"><a href="#cb9-5"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span></code></pre></div>
<p>The next piece of code sets up the dynamics of the compartmental model model. He doesn’t give the specific details in the blog post, but my understanding is that the four states are as follows. <code>x[0]</code> is the susceptible population, those that haven’thad the disease yet. The susceptible population decreases by encounters with infections people. In Thomas’s model, both <code>x[3]</code> and <code>x[4]</code> are infections. So the dynamics of the reduction of the susceptible is given by <br /><span class="math display">$$
\frac{\text{d}{S}}{\text{d}t} = - \beta S (I_1 + I_2).
$$</span><br /> Here, I’ve used <span class="math inline"><em>I</em><sub>1</sub></span> and <span class="math inline"><em>I</em><sub>2</sub></span> to represent what appears to be two separate infectious compartments in Thomas’s model. We’ll speculate about why there are two in a moment.</p>
<p>The model appears to be an SEIR model, so rather than becoming infectious directly you next move to an ‘exposed’, where you have the disease, but you are not yet infectious. There are again <em>two</em> exposed states, we’ll return to that in a moment. We denote the first, <code>x[1]</code> by <span class="math inline"><em>E</em><sub>1</sub></span>. We have <br /><span class="math display">$$
\frac{\text{d}{E_1}}{\text{d}t} = \beta S (I_1 + I_2) - \sigma E_1.
$$</span><br /> Note that the first term matches the term from the Susceptible equation. This is because it is the incoming exposed population.</p>
<p>The exposed population move to a second compartment of exposure, <span class="math inline"><em>E</em><sub>2</sub></span>. I believe the reason for this is that if you use only one exposure compartment, then the statistics of the duration of exposure are incorrect (implicitly they are exponetially distributed in the underlying stochastic version of the model). By using two exposure departments, Thomas is making a slight correction to this which would impose a first order gamma distribution on those statistics. A similar trick is being deployed for the ‘infectious group’. So we gain an additional equation to help with these statistics, <br /><span class="math display">$$
\frac{\text{d}{E_2}}{\text{d}t} = \sigma E_1 - \sigma E_2.
$$</span><br /> giving us the exposed group as the sum of the two compartments <span class="math inline"><em>E</em><sub>1</sub></span> and <span class="math inline"><em>E</em><sub>2</sub></span>. The exposed group from the second compartment then become ‘infected’, which we represent with <span class="math inline"><em>I</em><sub>1</sub></span>, in the code this is <code>x[3]</code>, <br /><span class="math display">$$
\frac{\text{d}{I_1}}{\text{d}t} = \sigma E_2 - \gamma I_1,
$$</span><br /> and similarly, Thomas is using a two compartment infectious group to fix up the duration model. So we have, <br /><span class="math display">$$
\frac{\text{d}{I_2}}{\text{d}t} = \gamma I_1 - \gamma I_2.
$$</span><br /> And finally we have those that have recovered emerging from the second infections compartment. In this model there is no separate model for ‘deaths’, so the recovered compartment, <span class="math inline"><em>R</em></span>, would also include those that die, <br /><span class="math display">$$
\frac{\text{d}R}{\text{d}t} = \gamma I_2.
$$</span><br /> All of these equations are then represented in code as follows.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a><span class="kw">def</span> odefun(t,x,beta0,betat,t0,t1,sigma,gamma):</span>
<span id="cb10-2"><a href="#cb10-2"></a>    dx <span class="op">=</span> np.zeros(<span class="dv">6</span>)</span>
<span id="cb10-3"><a href="#cb10-3"></a>    <span class="cf">if</span> ((t<span class="op">&gt;=</span>t0) <span class="kw">and</span> (t<span class="op">&lt;=</span>t1)):</span>
<span id="cb10-4"><a href="#cb10-4"></a>        beta <span class="op">=</span> betat</span>
<span id="cb10-5"><a href="#cb10-5"></a>    <span class="cf">else</span>:</span>
<span id="cb10-6"><a href="#cb10-6"></a>        beta <span class="op">=</span> beta0</span>
<span id="cb10-7"><a href="#cb10-7"></a>    dx[<span class="dv">0</span>] <span class="op">=</span> <span class="op">-</span>beta<span class="op">*</span>x[<span class="dv">0</span>]<span class="op">*</span>(x[<span class="dv">3</span>] <span class="op">+</span> x[<span class="dv">4</span>])</span>
<span id="cb10-8"><a href="#cb10-8"></a>    dx[<span class="dv">1</span>] <span class="op">=</span> beta<span class="op">*</span>x[<span class="dv">0</span>]<span class="op">*</span>(x[<span class="dv">3</span>] <span class="op">+</span> x[<span class="dv">4</span>]) <span class="op">-</span> sigma<span class="op">*</span>x[<span class="dv">1</span>]</span>
<span id="cb10-9"><a href="#cb10-9"></a>    dx[<span class="dv">2</span>] <span class="op">=</span> sigma<span class="op">*</span>x[<span class="dv">1</span>] <span class="op">-</span> sigma<span class="op">*</span>x[<span class="dv">2</span>]</span>
<span id="cb10-10"><a href="#cb10-10"></a>    dx[<span class="dv">3</span>] <span class="op">=</span> sigma<span class="op">*</span>x[<span class="dv">2</span>] <span class="op">-</span> gamma<span class="op">*</span>x[<span class="dv">3</span>]</span>
<span id="cb10-11"><a href="#cb10-11"></a>    dx[<span class="dv">4</span>] <span class="op">=</span> gamma<span class="op">*</span>x[<span class="dv">3</span>] <span class="op">-</span> gamma<span class="op">*</span>x[<span class="dv">4</span>]</span>
<span id="cb10-12"><a href="#cb10-12"></a>    dx[<span class="dv">5</span>] <span class="op">=</span> gamma<span class="op">*</span>x[<span class="dv">4</span>]</span>
<span id="cb10-13"><a href="#cb10-13"></a>    <span class="cf">return</span> dx</span></code></pre></div>
<p>Where the code takes in the states of the compartments (the values of <code>x</code>) and returns the gradients of those states for the provided parameters (<code>sigma</code>, <code>gamma</code> and <code>beta</code>). Those parameters are set according to the known characteristics of the disease.</p>
<p>The next block of code sets up the parameters of the SEIR model. A particularly important parameter is the reproduction number (<span class="math inline"><em>R</em><sub>0</sub></span>), here Thomas has assumed a reproduction number of 2.5, implying that each infected member of the population transmits the infection up to 2.5 other people. The effective <span class="math inline"><em>R</em></span> decreases over time though, because some of those people they meet will no longer be in the susceptible group.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a><span class="co"># Parameters of the model</span></span>
<span id="cb11-2"><a href="#cb11-2"></a>N <span class="op">=</span> <span class="fl">6.7e7</span> <span class="co"># Total population</span></span>
<span id="cb11-3"><a href="#cb11-3"></a>i0 <span class="op">=</span> <span class="fl">1e-4</span> <span class="co"># 0.5*Proportion of the population infected on day 0</span></span>
<span id="cb11-4"><a href="#cb11-4"></a>tlast <span class="op">=</span> <span class="fl">365.0</span> <span class="co"># Consider a year</span></span>
<span id="cb11-5"><a href="#cb11-5"></a>latent_period <span class="op">=</span> <span class="fl">5.0</span> <span class="co"># Days between being infected and becoming infectious</span></span>
<span id="cb11-6"><a href="#cb11-6"></a>infectious_period <span class="op">=</span> <span class="fl">7.0</span> <span class="co"># Days infectious</span></span>
<span id="cb11-7"><a href="#cb11-7"></a>R0 <span class="op">=</span> <span class="fl">2.5</span> <span class="co"># Basic reproduction number in the absence of interventions</span></span>
<span id="cb11-8"><a href="#cb11-8"></a>Rt <span class="op">=</span> <span class="fl">0.75</span> <span class="co"># Reproduction number in the presence of interventions</span></span>
<span id="cb11-9"><a href="#cb11-9"></a>tend <span class="op">=</span> <span class="fl">21.0</span> <span class="co"># Number of days of interventions</span></span></code></pre></div>
<p>The parameters are correct for the ‘discrete system’, where the inectious period is a discrete time, and the numbers are discrete values. To translate into our continuous differential equation system’s parameters, we need to do a couple of manipulations. Note the factor of 2 associated with <code>gamma</code> and <code>sigma</code>. This is a doubling of the rate to account for the fact that there are two compartments for each of these states (to fix-up the statistics of the duration models).</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a>beta0 <span class="op">=</span> R0 <span class="op">/</span> infectious_period</span>
<span id="cb12-2"><a href="#cb12-2"></a>betat <span class="op">=</span> Rt <span class="op">/</span> infectious_period</span>
<span id="cb12-3"><a href="#cb12-3"></a>sigma <span class="op">=</span> <span class="fl">2.0</span> <span class="op">/</span> latent_period</span>
<span id="cb12-4"><a href="#cb12-4"></a>gamma <span class="op">=</span> <span class="fl">2.0</span> <span class="op">/</span> infectious_period</span></code></pre></div>
<p>Next we solve the system using <code>scipy</code>’s initial value problem solver. The solution method is "Runge-Kutta-Fehlberg method, as indicated by the <code>'RK45'</code> solver. This is a numerical method for solving differential equations. The 45 is the order of the method and the error estimator.</p>
<p>We can view the solver itself as somehow a piece of simulation code, but here it’s being called as sub routine in the system. It returns a solution for each time step, stored in a list <code>sol</code>.</p>
<p>This is typical of this type of non-linear differential equation problem. Whether it’s partial differential equations, ordinary differential equations, there’s a step where a numerical solver needs to be called. These are often expensive to run. For climate and weather models, this would be where we solved the Navier-Stokes equations. For this simple model, the solution is relatively quick.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a></span>
<span id="cb13-2"><a href="#cb13-2"></a>t0ran <span class="op">=</span> np.array([<span class="op">-</span><span class="dv">100</span>, <span class="dv">40</span>, <span class="fl">52.5</span>, <span class="dv">65</span>])</span>
<span id="cb13-3"><a href="#cb13-3"></a>sol<span class="op">=</span>[]</span>
<span id="cb13-4"><a href="#cb13-4"></a><span class="cf">for</span> tt <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>,<span class="bu">len</span>(t0ran)):</span>
<span id="cb13-5"><a href="#cb13-5"></a>    sol.append(integrate.solve_ivp(<span class="kw">lambda</span> t,x: odefun(t,x,beta0,betat,t0ran[tt],t0ran[tt]<span class="op">+</span>tend,sigma,gamma),</span>
<span id="cb13-6"><a href="#cb13-6"></a>                              (<span class="fl">0.0</span>,tlast),</span>
<span id="cb13-7"><a href="#cb13-7"></a>                              np.array([<span class="fl">1.0</span><span class="op">-</span><span class="fl">2.0</span><span class="op">*</span>i0, <span class="fl">0.0</span>, <span class="fl">0.0</span>, i0, i0, <span class="fl">0.0</span>]),</span>
<span id="cb13-8"><a href="#cb13-8"></a>                              <span class="st">&#39;RK45&#39;</span>,</span>
<span id="cb13-9"><a href="#cb13-9"></a>                              atol<span class="op">=</span><span class="fl">1e-8</span>,</span>
<span id="cb13-10"><a href="#cb13-10"></a>                              rtol<span class="op">=</span><span class="fl">1e-9</span>))</span></code></pre></div>
<div class="figure">
<div id="house-model-zoom-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/simulation/house-model-zoom.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="house-model-zoom-magnify" class="magnify" onclick="magnifyFigure(&#39;house-model-zoom&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="house-model-zoom-caption" class="caption-frame">
<p>Figure: A zoomed in version of Thomas House’s variation on the SEIR model for evaluating the effect of early interventions.</p>
</div>
</div>
<div class="figure">
<div id="house-model-full-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/simulation/house-model-full.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="house-model-full-magnify" class="magnify" onclick="magnifyFigure(&#39;house-model-full&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="house-model-full-caption" class="caption-frame">
<p>Figure: The full progress of the disease in Thomas House’s variation on the SEIR model for evaluating the effect of early interventions.</p>
</div>
</div>
<p>In practice, immunity for Covid19 may only last around 6 months. As an exercise, try to extend Thomas’s model for the case where immunity is temporary. You’ll need to account for deaths as well in your new model.</p>
<p>Thinking about our Formula 1 example, and the differing levels of fidelity that might be included in a model, you can now imagine the challenges of doing large scale theoretical epidemiology. The compartment model is operating at a particular level of fidelity. Imagine trying to modify this model for a specific circumstance, like the way that the University of Cambridge chooses to do lectures. It’s not appropriate for this level of fidelity. You need to use different types of models for that decision making. Later, we’ll look at a simulation that was used to advise the government on the Test Trace Isolate program that took a different approach.</p>
<h1 id="strategies-for-simulation">Strategies for Simulation</h1>
<p>Within any simulation, we can roughly split the variables of interest into the state variables and the parameters. In the Herd immunity example, the state variables were the different susceptible, exposed, infectious and recovered groups. The parameters were the reproduction number and the expected lengths of infection and the timing of lockdown. Often parameters are viewed as the inputs to the simulation, the things we can control. We might want to know how to time lock down to minimize the number of deaths. This behaviour of the simulator is what we may want to emulate with our Gaussian process model.</p>
<p>So far we’ve introduced simulation motivated by the physical laws of the universe. Those laws are sometimes encoded in differential equations, in which case we can try to solve those systems (like with Herd Immunity or Navier Stokes). An alternative approach is taken in the Game of Life. There a turn based simulation is used, at each turn, we iterate through the simulation updating the sate of the simulation. This is known as a <em>discrete event simulation</em>. In race simulation for Formula 1 a discrete event simulation is also used. There is another form of discrete event simulation, often used in Chemical models, where the events don’t take place at regular intervals. Instead, the timing to the next event is computed, and the simulator advances that amount of time. For an example of this see <a href="https://en.wikipedia.org/wiki/Gillespie_algorithm">the Gillespie algorithm</a>.</p>
<p>There is a third type of simulation that we’d also like to introduce. That is simulation within computer software. In particular, the need to backtest software with ‘what if’ ideas, or to trace errors that may have occured in production. This can involve loading up entire code bases and rerunning them with simulated inputs. This is a third form of simulation where emulation can also come in useful.</p>
<h2 id="backtesting-production-code">Backtesting Production Code</h2>
<p>In Amazon the team I led looked at examples of simulations and emulation as varied as Prime Air drones across to the Amazon Supply Chain. In a purchasing system, the idea is to store stock so as to balance supply and demand. The aim is to keep product in stock for quick despatch while keeping prices (and therefore costs) low. This idea is at the heart of Amazon’s focus on customer experience.</p>
<h2 id="buying-system">Buying System</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_ai/includes/buying-system.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_ai/includes/buying-system.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>An example of a complex decision making system might be an automated buying system. In such a system, the idea is to match demand for products to supply of products.</p>
<p>The matching of demand and supply is a repetetive theme for decision making systems. Not only does it occur in automated buying, but also in the allocation of drivers to riders in a ride sharing system. Or in the allocation of compute resource to users in a cloud system.</p>
<p>The components of any of these system include: predictions of the demand for the product, or the drivers or the compute. Then predictions of the supply. Decisions are then made for how much material to keep in stock, or how many drivers to have on the road, or how much computer capacity to have in your data centres. These decisions have cost implications. The optimal amount of product will depend on the cost of making it available. For a buying system this is the storage costs.</p>
<p>Decisions are made on the basis of the supply and demand to make new orders, to encourage more drivers to come into the system or to build new data centers or rent more computational power.</p>
<div class="figure">
<div id="buying-system-components-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/software/buying-schematic.svg" width="40%" style=" ">
</object>
</div>
<div id="buying-system-components-magnify" class="magnify" onclick="magnifyFigure(&#39;buying-system-components&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="buying-system-components-caption" class="caption-frame">
<p>Figure: The components of a putative automated buying system</p>
</div>
</div>
<h2 id="monolithic-system">Monolithic System</h2>
<p>The classical approach to building these systems was a ‘monolithic system’. Built in a similar way to the successful applicaitons software such as Excel or Word, or large operating systems, a single code base was constructed. The complexity of such code bases run to many lines.</p>
<p>In practice, shared dynamically linked libraries may be used for aspects such as user interface, or networking, but the software often has many millions of lines of code. For example, the Microsoft Office suite is said to contain over 30 millions of lines of code.</p>
<div class="figure">
<div id="ml-system-monolith-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/ai/ml-system-monolith-purchasing.svg" width="60%" style=" ">
</object>
</div>
<div id="ml-system-monolith-magnify" class="magnify" onclick="magnifyFigure(&#39;ml-system-monolith&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="ml-system-monolith-caption" class="caption-frame">
<p>Figure: A potential path of models in a machine learning system.</p>
</div>
</div>
<h2 id="service-oriented-architecture">Service Oriented Architecture</h2>
<p>Such software is not only difficult to develop, it is difficult to scale when computation demands increase. Amazon’s original website software (called Obidos) was a <a href="https://en.wikipedia.org/wiki/Obidos_(software)">monolithic design</a> but by the early noughties it was becoming difficult to sustain and maintain. The software was phased out in 2006 to be replaced by a modularized software known as a ‘service oriented architecture’.</p>
<p>In Service Oriented Architecture, or “Software as a Service” the idea is that code bases are modularized and communicate with one another using network requests. A standard approach is to use a <a href="https://en.wikipedia.org/wiki/Representational_state_transfer">REST API</a>. So, rather than a single monolithic code base, the code is developed with individual services that handle the different requests.</p>
<div class="figure">
<div id="ml-system-downstream-purchasing-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/ai/ml-system-downstream-purchasing000.svg" width="60%" style=" ">
</object>
</div>
<div id="ml-system-downstream-purchasing-magnify" class="magnify" onclick="magnifyFigure(&#39;ml-system-downstream-purchasing&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="ml-system-downstream-purchasing-caption" class="caption-frame">
<p>Figure: A potential path of models in a machine learning system.</p>
</div>
</div>
<p>This is the landscape we now find ourselves in with regard to software development. In practice, each of these services is often ‘owned’ and maintained by an individual team. The team is judged by the quality of their service provision. They work to detailed specifications on what their service should output, what its availability should be and other objectives like speed of response. This allows for conditional independence between teams and for faster development.</p>
<!--[Simpy](https://simpy.readthedocs.io/en/latest/examples/gas_station_refuel.html)

* News Vendor Problem
* Trolley & Pendulum
* Mountain Car
* Hodgkin Huxley 
* Formula One Race
* Plane/F1 Car/Drone
* [Fluid Dynamics](https://github.com/barbagroup/CFDPython) Discretisation of PDEs
* [Stress in a connecting rod](https://solidspy.readthedocs.io/en/latest/readme.html) Discretisation of PDEs
* [Network simulation](https://github.com/mkalewski/sim2net) Discrete Event


* Reaction Rates: 
<>

-->
<h1 id="related-approaches">Related Approaches</h1>
<p>While this module is mainly focussing on emulation as a route to bringing machine learning closer to the physical world, I don’t want to give the impression that’s the only approach. In particular, it’s worth bearing in mind three important domains of machine learning (and statistics) that we also could have explored.</p>
<ul>
<li>Probabilistic Programming</li>
<li>Approximate Bayesian Computation</li>
<li>Causal inference</li>
</ul>
<p>Each of these domains also brings a lot to the table in terms of understanding the physical world.</p>
<h2 id="probabilistic-programming">Probabilistic Programming</h2>
<p>Probabilistic programming is an idea that, from our perspective, can be summarized as follows. What if, when constructing your simulator, or your model, you used a programming language that was aware of the state variables and the probability distributions. What if this language could ‘compile’ the program into code that would automatically compute the Bayesian posterior for you?</p>
<p>This is the objective of probabilistic programming. The idea is that you write your model in a language, and that language is automatically converted into the different modelling codes you need to perform Bayesian inference.</p>
<p>The ideas for probabilistic programming originate in <a href="https://www.mrc-bsu.cam.ac.uk/software/bugs/">BUGS</a>. The software was developed at the MRC Biostatistics Unit here in Cambridge in the early 1990s, by amoung others, David Spiegelhalter. Carl Henrik covered in last week’s lecture some of the approaches for approximate inference. BUGS uses Gibbs sampling. Gibbs sampling, however, can be slow to converge when there are strong correlations in the posterior between variables.</p>
<p>The descendent of BUGS that is probably most similar in the spirit of its design is <a href="https://mc-stan.org/">Stan</a>. Stan came from researchers at Columbia University and makes use of a variant of Hamiltonian Monte Carlo called the No-U-Turn sampler. It builds on automatic differentiation for the gradients it needs. It’s all written in C++ for speed, but has interfaces to Python, R, Julia, Matlab etc. Stan has been higly succesful during the Coronavirus pandemic, with a number of epidemiological simulations written in the language, for example see this <a href="https://mc-stan.org/users/documentation/case-studies/boarding_school_case_study.html">blog post</a>.</p>
<p>Other probabilistic programming languages of interest include those that make use of variational approaches (such as <a href="https://pyro.ai/">pyro</a>) and allow use of neural network components.</p>
<h2 id="approximate-bayesian-computation">Approximate Bayesian Computation</h2>
<p>We reintroduced Gaussian processes at the start of this lecture by sampling from the Gaussian process and matching the samples to data, discarding those that were distant from our observations. This approach to Bayesian inference is the starting point for <em>approximate Bayesian computation</em> or ABC.</p>
<p>The idea is straightforward, if we can measure ‘closeness’ in some relevant fashion, then we can sample from our simulation, compare our samples to real world data through ‘closeness measure’ and eliminate samples that are distant from our data. Through appropriate choice of closeness measure, our samples can be viewed as coming from an approximate posterior.</p>
<p>My Sheffield colleague, Rich Wilkinson, was one of the pioneers of this approach during his PhD in the Statslab here in Cambridge. You can hear Rich talking about ABC at NeurIPS in 2013 here.</p>
<div class="figure">
<div id="rich-wilkinson-abc-figure" class="figure-frame">
<iframe width="800" height="600" src="https://www.youtube.com/embed/sssbLkn2JjI?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
<div id="rich-wilkinson-abc-magnify" class="magnify" onclick="magnifyFigure(&#39;rich-wilkinson-abc&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="rich-wilkinson-abc-caption" class="caption-frame">
<p>Figure: Rich Wilkinson giving a Tutorial on ABC at NeurIPS in 2013. Unfortunately they’ve not synchronised the slides with the tutorial. You can find the slides <a href="http://media.nips.cc/Conferences/2013/Video/Tutorial2B.pdf">separately here</a>.</p>
</div>
</div>
<h2 id="causality">Causality</h2>
<div class="figure">
<div id="judea-pearl-causality-figure" class="figure-frame">
<iframe width="800" height="600" src="https://www.youtube.com/embed/yksduYxEusQ?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
<div id="judea-pearl-causality-magnify" class="magnify" onclick="magnifyFigure(&#39;judea-pearl-causality&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="judea-pearl-causality-caption" class="caption-frame">
<p>Figure: Judea Pearl and Elias Bareinboim giving a Tutorial on Causality at NeurIPS in 2013. Again, the slides aren’t synchronised, but you can find them separately <a href="http://media.nips.cc/Conferences/2013/nips-dec2013-pearl-bareinboim-tutorial-full.pdf">here</a>.</p>
</div>
</div>
<p>All of these approaches offer a lot of promise for developing machine learning at the interface with science, but covering each in detail would require four separate modules. We’ve chosen to focus on the emulation approach, for two princpal reasons. Firstly, it’s conceptual simplicity. Our aim is to replace all or part of our simulation with a machine learning model. Typically, we’re going to want uncertainties as part of that representation. That explains our focus on Gaussian process models. Secondly, the emulator method is flexible. Probabilistic programming requires that the simulator has been built in a particular way, otherwise we can’t compile the program. Finally, the emulation approach can be combined with any of the existing simulation approaches. For example, we might want to write our emulators as probabilistic programs. Or we might do causal analysis on our emulators, or we could speed up the simulation in ABC through emulation.</p>
<h1 id="conclusion">Conclusion</h1>
<p>We’ve introduced the notion of a simulator. A body of computer code that expresses our understanding of a particular physical system. We introduced such simulators through <em>physical laws</em>, such as laws of gravitation or electro-magnetism. But we soon saw that in may simulations those laws become abstracted and the simulation becomes more phenomological.</p>
<p>Even full knowledge of all laws does not give us access to ‘the mind of God’, because we are lacking information about the data, and we are missing the compute. These challenges further motivate the need for abstraction, and we’ve seen examples of where such abstractions are used in practice.</p>
<p>We also summarized the different types of simmulation into roughly three groups. Firstly, those based on physical laws in the form of differential equations. Examples include certain compartmental epidemiological models, climate models and weather models. Secondly, discrete event simulations. These simulations often run to a ‘clock’, where updates to the state are taken in turns. The Game of Life is an example of this type of simulation, and Formula 1 models of race strategy also use this approach. There is another type of discrete event simulation that doesn’t use a turn based approach, but waits for the next event. The <a href="https://en.wikipedia.org/wiki/Gillespie_algorithm">Gillespie algorithm</a> is an example of such an approach but we didn’t cover it here. Finally, we realised that general computer code bases are also simulations. If a company has a large body of code, and particularly if it’s hosted within a streaming environment (such as Apache Kafka), it’s possible to back test the code with different inputs. Such backtests can be viewed as simulations, and in the case of large bodies of code (such as the code that manages Amazon’s automated buying systems) the back tests can be slow and could also benefit from automation.</p>
<h1 class="unnumbered" id="references">References</h1>
<div id="refs" class="references hanging-indent" role="doc-bibliography">
<div id="ref-Cheng-evidence20">
<p>Cheng, Bingqing, Guglielmo Mazzola, Chris J. Pickard, and Michele Ceriotti. 2020. “Evidence for Supercritical Behaviour of High-Pressure Liquid Hydrogen.” <em>Nature</em> 585: 217–20. <a href="https://doi.org/10.1038/s41586-020-2677-y">https://doi.org/10.1038/s41586-020-2677-y</a>.</p>
</div>
<div id="ref-Elsner-unbiased15">
<p>Elsner, Franz, Boris Leistedt, and Hiranya V. Peiris. 2015. “Unbiased Methods for Removing Systematics from Galaxy Clustering Measurements.” <em>Monthly Notices of the Royal Astronomical Society</em> 456 (2): 2095–2104. <a href="https://doi.org/10.1093/mnras/stv2777">https://doi.org/10.1093/mnras/stv2777</a>.</p>
</div>
<div id="ref-Elsner-unbiased16">
<p>———. 2016. “Unbiased Pseudo-<span class="math inline"><em>C</em><sub>ℓ</sub></span> Power Spectrum Estimation with Mode Projection.” <em>Monthly Notices of the Royal Astronomical Society</em> 465 (2): 1847–55. <a href="https://doi.org/10.1093/mnras/stw2752">https://doi.org/10.1093/mnras/stw2752</a>.</p>
</div>
<div id="ref-Jaffe:cmb98">
<p>Jaffe, Andrew H., J. R. Bond, P. G. Ferreira, and L. E. Knox. 1998. “CMB Likelihood Functions for Beginners and Experts.” In. <a href="https://arxiv.org/abs/astro-ph/0306506">https://arxiv.org/abs/astro-ph/0306506</a>.</p>
</div>
<div id="ref-Laplace-essai14">
<p>Laplace, Pierre Simon. 1814. <em>Essai Philosophique Sur Les Probabilités</em>. 2nd ed. Paris: Courcier.</p>
</div>
<div id="ref-Mishra-Sharma-semi-parametric20">
<p>Mishra-Sharma, Siddharth, and Kyle Cranmer. 2020. “Semi-Parametric <span class="math inline"><em>γ</em></span>-Ray Modeling with Gaussian Processes and Variational Inference.” <a href="https://arxiv.org/abs/2010.10450">https://arxiv.org/abs/2010.10450</a>.</p>
</div>
<div id="ref-Pfau-abinitio20">
<p>Pfau, David, James S. Spencer, Alexander G. D. G. Matthews, and W. M. C. Foulkes. 2020. “Ab Initio Solution of the Many-Electron Schrödinger Equation with Deep Neural Networks.” <em>Phys. Rev. Research</em> 2 (3): 033429. <a href="https://doi.org/10.1103/PhysRevResearch.2.033429">https://doi.org/10.1103/PhysRevResearch.2.033429</a>.</p>
</div>
<div id="ref-Pontzen-cmb10">
<p>Pontzen, Andrew, and Hiranya V. Peiris. 2010. “The Cut-Sky Cosmic Microwave Background Is Not Anomalous.” <em>Phys. Rev. D</em> 81 (10): 103008. <a href="https://doi.org/10.1103/PhysRevD.81.103008">https://doi.org/10.1103/PhysRevD.81.103008</a>.</p>
</div>
<div id="ref-Roh-cryo-em20">
<p>Roh, Soung-Hun, Mrinal Shekhar, Grigore Pintilie, Christophe Chipot, Stephan Wilkens, Abhishek Singharoy, and Wah Chiu. 2020. “Cryo-EM and MD Infer Water-Mediated Proton Transport and Autoinhibition Mechanisms of Vo Complex.” <em>Science Advances</em> 6 (41). <a href="https://doi.org/10.1126/sciadv.abb9605">https://doi.org/10.1126/sciadv.abb9605</a>.</p>
</div>
<div id="ref-Vogelsberger-cosmological20">
<p>Vogelsberger, Mark, Federico Marinacci, Paul Torrey, and Ewald Puchwei. 2020. “Cosmological Simulations of Galaxy Formation.” <em>Nature Reviews Physics</em>, 42–66. <a href="https://doi.org/10.1038/s42254-019-0127-2">https://doi.org/10.1038/s42254-019-0127-2</a>.</p>
</div>
</div>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>Most of my understanding of this is taken from conversations with Kyle Cranmer, a physicist who makes extensive use of machine learning methods in his work. See e.g. <span class="citation" data-cites="Mishra-Sharma-semi-parametric20">Mishra-Sharma and Cranmer (2020)</span> from Kyle and Siddharth Mishra-Sharma. Of course, any errors in the above text are mine and do not stem from Kyle.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>We should be careful about such mappings, this is the one I prefer to think about because I try to think of my modelling assumptions as being stored in a probabilistic model, which I see as the prior distribution over what I expect the data to look like. In many domains of parametric modelling, however, the prior will be specified over the parameters of a model. In the Gaussian process formalism we’re using, this mapping is clearer though. The ‘prior’ is the Gaussian process prior over functions, the data is the relationship between those functions and observations we make. This mental model will also suit what follows in terms of our consideration of simulation. But it would likely confuse someone who had only come to Bayesian inference through parametric models such a neural networks. Note that even in such models, there will be a way of writing down the decomposition of the model that is akin to the above, but it might involve writing down intractable densities so it’s often avoided.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>Unfortunately, I have to use the term ‘exact’ loosely here! For example, most of these laws treat space/time as a continuum. But in reality, it is quantised. The smallest length we can define is Planck length (<span class="math inline">1.61 × 10<sup> − 35</sup></span>), and the the smallest time is Planck time. So even in this exact world of Maxwell and Newton there is an abstraction or a discretisation.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

