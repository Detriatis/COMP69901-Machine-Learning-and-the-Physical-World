---
title: "Emukit and Experimental Design"
venue: "Virtual (Zoom)"
abstract: "<p>A surrogate model can be explored to understand the sensitivity of the system. This lecture will review how to perform sensitivity analysis.</p>"
author:
- given: Neil D.
  family: Lawrence
  url: http://inverseprobability.com
  institute: 
  twitter: lawrennd
  gscholar: r3SJcvoAAAAJ
  orchid: 
date: 2020-10-30
published: 2020-10-30
week: 4
session: 2
reveal: 04-emukit-and-experimental-design.slides.html
ipynb: 04-emukit-and-experimental-design.ipynb
layout: lecture
categories:
- notes
---



<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!--

-->
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="op">%</span>pip install pyDOE</span></code></pre></div>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="op">%</span>pip install emukit</span></code></pre></div>
<p>This introduction is based on <a href="https://github.com/EmuKit/emukit/blob/master/notebooks/Emukit-tutorial-experimental-design-introduction.ipynb">An Introduction to Experimental Design with Emukit</a> written by Andrei Paleyes and Maren Mahsereci.</p>
<h2 id="alex-forrester">Alex Forrester</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_uq/includes/alex-forrester.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_uq/includes/alex-forrester.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<div class="centered" style="">
<svg viewBox="0 0 200 200" style="width:15%">
<defs> <clipPath id="clip0">
<style>
circle {
  fill: black;
}
</style>
<circle cx="100" cy="100" r="100"/> </clipPath> </defs>
<title>
Alex Forrester
</title>
<image preserveAspectRatio="xMinYMin slice" width="100%" xlink:href="../slides/diagrams/people/alex-forrester.jpg" clip-path="url(#clip0)"/>
</svg>
</div>
<p>We’re going to make use of the Forrester function in our example below, a function developed as a demonstrator by <a href="https://www.southampton.ac.uk/engineering/research/groups/performance-sports/staff-profiles/alexander-forrester.page">Alex Forrester</a>. Alex is a design engineer who makes extensive use of surrogate modelling in Engineering design.</p>
<p>You can see Alex talking about the use of Gaussian process surrogates <a href="http://videolectures.net/mla09_forrester_sbcmoo/">in this online video lecture</a>.</p>
<div class="figure">
<div id="kinematic-human-simulation-figure" class="figure-frame">
<iframe width="800" height="600" src="https://www.youtube.com/embed/2ngc2aw9xYs?start=" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
<div id="kinematic-human-simulation-magnify" class="magnify" onclick="magnifyFigure(&#39;kinematic-human-simulation&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="kinematic-human-simulation-caption" class="caption-frame">
<p>Figure: A kinematic simulation of the human body doing breaststroke that Alex uses as part of his work in optimization of human motion during sports.</p>
</div>
</div>
<p>The Forrester function <span class="citation" data-cites="Forrester-engineering08">(Forrester, Sóbester, and Keane 2008)</span> is commonly used as a demonstrator function in surrogate modelling. It has the form <br /><span class="math display"><em>f</em>(<em>x</em>) = (6<em>x</em> − 2)<sup>2</sup>sin (12<em>x</em> − 4).</span><br /></p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a>x <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">100</span>)</span>
<span id="cb4-2"><a href="#cb4-2"></a>f <span class="op">=</span> (<span class="dv">6</span><span class="op">*</span>x<span class="op">-</span><span class="dv">2</span>)<span class="op">**</span><span class="dv">2</span> <span class="op">*</span> np.sin(<span class="dv">12</span><span class="op">*</span>x<span class="op">-</span><span class="dv">4</span>)</span></code></pre></div>
<div class="figure">
<div id="forrester-function-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/uq/forrester-function.svg" width="80%" style=" ">
</object>
</div>
<div id="forrester-function-magnify" class="magnify" onclick="magnifyFigure(&#39;forrester-function&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="forrester-function-caption" class="caption-frame">
<p>Figure: The Forrester function is commonly used as an exemplar function for surrogate modelling and emulation. It has the form <span class="math inline"><em>f</em>(<em>x</em>) = (6<em>x</em> − 2)<sup>2</sup>sin (12<em>x</em> − 4)</span></p>
</div>
</div>
<p>Experimental design.</p>
<p>Latin hypercube</p>
<p>Linear example</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-2"><a href="#cb5-2"></a></span>
<span id="cb5-3"><a href="#cb5-3"></a><span class="im">from</span> emukit.test_functions <span class="im">import</span> forrester_function</span>
<span id="cb5-4"><a href="#cb5-4"></a><span class="im">from</span> emukit.core.loop.user_function <span class="im">import</span> UserFunctionWrapper</span>
<span id="cb5-5"><a href="#cb5-5"></a><span class="im">from</span> emukit.core <span class="im">import</span> ContinuousParameter, ParameterSpace</span></code></pre></div>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a>target_function, space <span class="op">=</span> forrester_function()</span></code></pre></div>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a>x_plot <span class="op">=</span> np.linspace(space.parameters[<span class="dv">0</span>].<span class="bu">min</span>, space.parameters[<span class="dv">0</span>].<span class="bu">max</span>, <span class="dv">301</span>)[:, <span class="va">None</span>]</span>
<span id="cb7-2"><a href="#cb7-2"></a>y_plot <span class="op">=</span> target_function(x_plot)</span></code></pre></div>
<div class="figure">
<div id="forrester-function-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/uq/forrester-function.svg" width="80%" style=" ">
</object>
</div>
<div id="forrester-function-magnify" class="magnify" onclick="magnifyFigure(&#39;forrester-function&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="forrester-function-caption" class="caption-frame">
<p>Figure: The Forrester function <span class="citation" data-cites="Forrester-engineering08">(Forrester, Sóbester, and Keane 2008)</span>.</p>
</div>
</div>
<h2 id="initial-design">Initial Design</h2>
<p>Usually, before we start the actual ExpDesign loop we need to gather a few observations such that we can fit the model. This is called the initial design and common strategies are either a predefined grid or sampling points uniformly at random.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a>X_init <span class="op">=</span> np.array([[<span class="fl">0.2</span>],[<span class="fl">0.6</span>], [<span class="fl">0.9</span>]])</span>
<span id="cb8-2"><a href="#cb8-2"></a>Y_init <span class="op">=</span> target_function(X_init)</span></code></pre></div>
<div class="figure">
<div id="forrester-function-initial-design-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/uq/forrester-function-initial-design.svg" width="80%" style=" ">
</object>
</div>
<div id="forrester-function-initial-design-magnify" class="magnify" onclick="magnifyFigure(&#39;forrester-function-initial-design&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="forrester-function-initial-design-caption" class="caption-frame">
<p>Figure: The initial design for the Forrester function example.</p>
</div>
</div>
<h2 id="the-model">The Model</h2>
<p>Now we can start with the ExpDesign loop by first fitting a model on the collected data. A popular model for ExpDesign is a Gaussian process (GP) which defines a probability distribution across classes of functions, typically smooth, such that each linear finite-dimensional restriction is multivariate Gaussian <span class="citation" data-cites="Rasmussen:book06">(Rasmussen and Williams 2006)</span>. Gaussian processes are fully parametrized by a mean <span class="math inline">$\mu(\inputVector)$</span> and a covariance function <span class="math inline">$\kernelScalar(\inputVector,\inputVector^\prime)$</span>. Without loss of generality <span class="math inline">$\mu(\inputVector)$</span> is assumed to be zero. The covariance function <span class="math inline">$\kernelScalar(\inputVector,\inputVector^\prime)$</span> characterizes the smoothness and other properties of <span class="math inline">$\mappingFunction$</span>. It is known that the kernel of the process has to be continuous, symmetric and positive definite. A widely used kernel is the exponentiated quadratic or RBF kernel: <br /><span class="math display">$$ 
\kernelScalar(\inputVector,\inputVector^\prime) = \alpha \exp{ \left(-\frac{\|\inputVector-\inputVector^\prime\|^2}{2 \ell}\right)} 
$$</span><br /> where <span class="math inline"><em>α</em></span> and <span class="math inline">ℓ</span> are hyperparameters.</p>
<p>To denote that <span class="math inline">$\mappingFunction$</span> is a sample from a GP with mean <span class="math inline"><em>μ</em></span> and covariance <span class="math inline"><em>k</em></span> we write <br /><span class="math display">$$
\mappingFunction \sim \mathcal{GP}(\mu,k).
$$</span><br /></p>
<p>For regression tasks, the most important feature of GPs is that process priors are conjugate to the likelihood from finitely many observations <span class="math inline">$\dataMatrix = (y_1,\dots,y_\numData)^\top$</span> and <span class="math inline">$\inputMatrix =\{\inputVector_1,\dots,\inputVector_\numData\}$</span>, <span class="math inline">$\inputVector_i\in \mathcal{X}$</span> of the form <span class="math inline">$\dataScalar_i = \mappingFunction(\inputVector_i) + \noiseScalar_i$</span> where <span class="math inline">$\noiseScalar_i \sim \gaussianSamp{0}{\dataStd^2}$</span> and we typically estimate <span class="math inline">$\dataStd^2$</span> by maximum likelihood alongside <span class="math inline"><em>α</em></span> and <span class="math inline">ℓ</span>.</p>
<p>We obtain the Gaussian posterior <br /><span class="math display">$$
\mappingFunction(\inputVector^*)|\inputMatrix, \dataMatrix, \theta \sim \gaussianSamp{\mu(\inputVector^*)}{\sigma^2(\inputVector^*)},
$$</span><br /> where <span class="math inline">$\mu(\inputVector^*)$</span> and <span class="math inline">$\sigma^2(\inputVector^*)$</span> have a closed form solution as we’ve seen in the earlier lectures (see also <span class="citation" data-cites="Rasmussen:book06">Rasmussen and Williams (2006)</span>).</p>
<p>Note that Gaussian processes are also characterized by hyperparameters, for example in the exponatiated quadratic case we have <span class="math inline">$\paramVector = \left\{ \alpha, \ell, \dataStd^2 \right\}$</span> for the scale of the covariance, the lengthscale and the noise variance. Here, for simplicitly we will keep these hyperparameters fixed. However, we will usually either optimize or sample these hyperparameters using the marginal loglikelihood of the GP.</p>
<p>In this module we’ve focussed on Gaussian processes, but we could also use any other model that returns a mean <span class="math inline">$\mu(\inputVector)$</span> and variance <span class="math inline">$\sigma^2(\inputVector)$</span> on an arbitrary input points <span class="math inline">$\inputVector$</span> such as Bayesian neural networks or random forests. In Emukit these different models can be used by defining a new <code>ModelWrapper</code>.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a><span class="im">import</span> GPy</span>
<span id="cb9-2"><a href="#cb9-2"></a><span class="im">from</span> emukit.model_wrappers.gpy_model_wrappers <span class="im">import</span> GPyModelWrapper</span></code></pre></div>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a>kern <span class="op">=</span> GPy.kern.RBF(<span class="dv">1</span>, lengthscale<span class="op">=</span><span class="fl">0.08</span>, variance<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb10-2"><a href="#cb10-2"></a>gpy_model <span class="op">=</span> GPy.models.GPRegression(X_init, Y_init, kern, noise_var<span class="op">=</span><span class="fl">1e-10</span>)</span>
<span id="cb10-3"><a href="#cb10-3"></a>emukit_model <span class="op">=</span> GPyModelWrapper(gpy_model)</span>
<span id="cb10-4"><a href="#cb10-4"></a></span>
<span id="cb10-5"><a href="#cb10-5"></a>mu_plot, var_plot <span class="op">=</span> emukit_model.predict(x_plot)</span></code></pre></div>
<div class="figure">
<div id="forrester-function-entropy-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/uq/forrester-function-entropy.svg" width="80%" style=" ">
</object>
</div>
<div id="forrester-function-entropy-magnify" class="magnify" onclick="magnifyFigure(&#39;forrester-function-entropy&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="forrester-function-entropy-caption" class="caption-frame">
<p>Figure: The emulator fitted to the Forrester function with only three observations. The error bars show 1, 2 and 3 standard deviations.</p>
</div>
</div>
<h2 id="the-acquisition-function">The Acquisition Function</h2>
<p>In the second step of our ExpDesign loop we use our model to compute the acquisition function. We’ll review two different forms of acquisition funciton for doing this.</p>
<h3 id="uncertainty-sampling">Uncertainty Sampling</h3>
<p>In uncertainty sampling (US) we hoose the next value <span class="math inline">$\inputVector_{n+1}$</span> at the location where the model on <span class="math inline">$\mappingFunction(\inputVector)$</span> has the highest marginal predictive variance <br /><span class="math display">$$
a_{US}(\inputVector) = \sigma^2(\inputVector).
$$</span><br /> This makes sure, that we learn the function <span class="math inline">$\mappingFunction(\cdot)$</span> everywhere on <span class="math inline">𝕏</span> to a similar level of absolute error.</p>
<h3 id="integrated-variance-reduction">Integrated Variance Reduction</h3>
<p>In the integrated variance reduction (IVR) you choose the next value <span class="math inline">$\inputVector_{n+1}$</span> such that the total variance of the model is reduced maximally <span class="citation" data-cites="Sacks-design89">(Sacks et al. 1989)</span>, <br /><span class="math display">$$
a_{IVR} = \int_{\mathbb{X}}[\sigma^2(\inputVector') - \sigma^2(\inputVector'; \inputVector)]\text{d}\inputVector'\approx 
\frac{1}{\# \text{samples}}\sum_i^{\# \text{samples}}[\sigma^2(\inputVector_i) - \sigma^2(\inputVector_i; \inputVector)].
$$</span><br /> Here <span class="math inline">$\sigma^2(\inputVector'; \inputVector)$</span> is the predictive variance at <span class="math inline">$\inputVector'$</span> had <span class="math inline">$\inputVector$</span> been observed. Thus IVR computes the overall reduction in variance (for all points in <span class="math inline">𝕏</span>) had <span class="math inline"><em>f</em></span> been observed at <span class="math inline">$\inputVector$</span>.</p>
<p>The finite sum approximation on the right hand side of the equation is usually used because the integral over <span class="math inline">$\inputVector'$</span> is not analytic. In that case <span class="math inline">$\inputVector_i$</span> are sampled randomly. For a GP model the right hand side simplifies to <br /><span class="math display">$$
a_{LCB} \approx \frac{1}{\# \text{samples}}\sum_i^{\# \text{samples}}\frac{\kernelScalar^2(\inputVector_i, \inputVector)}{\sigma^2(\inputVector)}.
$$</span><br /></p>
<p>IVR is arguably the more principled approach, but often US is preferred over IVR simply because it lends itself to gradient based optimization more easily, is cheaper to compute, and is exact.</p>
<p>For both of them (stochastic) gradient base optimizers are used to retrieve <span class="math inline">$\inputVector_{n+1} \in \operatorname*{arg\:max}_{\inputVector \in \mathbb{X}} a(\inputVector)$</span>.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a><span class="im">from</span> emukit.experimental_design.acquisitions <span class="im">import</span> IntegratedVarianceReduction, ModelVariance</span></code></pre></div>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a>us_acquisition <span class="op">=</span> ModelVariance(emukit_model)</span>
<span id="cb12-2"><a href="#cb12-2"></a>ivr_acquisition <span class="op">=</span> IntegratedVarianceReduction(emukit_model, space)</span>
<span id="cb12-3"><a href="#cb12-3"></a></span>
<span id="cb12-4"><a href="#cb12-4"></a>us_plot <span class="op">=</span> us_acquisition.evaluate(x_plot)</span>
<span id="cb12-5"><a href="#cb12-5"></a>ivr_plot <span class="op">=</span> ivr_acquisition.evaluate(x_plot)</span></code></pre></div>
<div class="figure">
<div id="experimental-design-acquisition-functions-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/uq/experimental-design-acquisition-functions-forrester.svg" width="80%" style=" ">
</object>
</div>
<div id="experimental-design-acquisition-functions-magnify" class="magnify" onclick="magnifyFigure(&#39;experimental-design-acquisition-functions&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="experimental-design-acquisition-functions-caption" class="caption-frame">
<p>Figure: The <em>uncertainty sampling</em> and <em>integrated variance reduction</em> acquisition functions for the Forrester example.</p>
</div>
</div>
<h2 id="evaluating-the-objective-function">Evaluating the objective function</h2>
<p>To find the next point to evaluate we optimize the acquisition function using a standard gradient descent optimizer.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a><span class="im">from</span> emukit.core.optimization <span class="im">import</span> GradientAcquisitionOptimizer</span></code></pre></div>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a>optimizer <span class="op">=</span> GradientAcquisitionOptimizer(space)</span>
<span id="cb14-2"><a href="#cb14-2"></a>x_new, _ <span class="op">=</span> optimizer.optimize(us_acquisition)</span></code></pre></div>
<div class="figure">
<div id="experimental-design-acquisition-functions-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/uq/experimental-design-acquisition-next-point-forrester.svg" width="80%" style=" ">
</object>
</div>
<div id="experimental-design-acquisition-functions-magnify" class="magnify" onclick="magnifyFigure(&#39;experimental-design-acquisition-functions&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="experimental-design-acquisition-functions-caption" class="caption-frame">
<p>Figure: The maxima of the acquisition function is found and this point is selected for inclusion.</p>
</div>
</div>
<p>Afterwards we evaluate the true objective function and append it to our initial observations.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a>y_new <span class="op">=</span> target_function(x_new)</span></code></pre></div>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1"></a>X <span class="op">=</span> np.append(X_init, x_new, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb16-2"><a href="#cb16-2"></a>Y <span class="op">=</span> np.append(Y_init, y_new, axis<span class="op">=</span><span class="dv">0</span>)</span></code></pre></div>
<p>After updating the model, you can see that the uncertainty about the true objective function in this region decreases and our model becomes more certain.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1"></a>emukit_model.set_data(X, Y)</span>
<span id="cb17-2"><a href="#cb17-2"></a>mu_plot, var_plot <span class="op">=</span> emukit_model.predict(x_plot)</span></code></pre></div>
<div class="figure">
<div id="forrester-function-multi-errorbars-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/uq/forrester-function-multi-errorbars.svg" width="80%" style=" ">
</object>
</div>
<div id="forrester-function-multi-errorbars-magnify" class="magnify" onclick="magnifyFigure(&#39;forrester-function-multi-errorbars&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="forrester-function-multi-errorbars-caption" class="caption-frame">
<p>Figure: The target Forrester function plotted alongside the emulation model and error bars from the emulation at 1, 2 and 3 standard deviations.</p>
</div>
</div>
<p>Entropy of posterior</p>
<h2 id="emukits-experimental-design-interface">Emukit’s experimental design interface</h2>
<p>Of course in practice we don’t want to implement all of these steps our self. Emukit provides a convenient and flexible interface to apply experimental design. Below we can see how to run experimental design on the exact same function for 10 iterations.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1"></a><span class="im">from</span> emukit.experimental_design.experimental_design_loop <span class="im">import</span> ExperimentalDesignLoop</span></code></pre></div>
<div class="sourceCode" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1"></a>ed <span class="op">=</span> ExperimentalDesignLoop(space<span class="op">=</span>space, model<span class="op">=</span>emukit_model)</span>
<span id="cb19-2"><a href="#cb19-2"></a></span>
<span id="cb19-3"><a href="#cb19-3"></a>ed.run_loop(target_function, <span class="dv">10</span>)</span></code></pre></div>
<div class="sourceCode" id="cb20"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1"></a>mu_plot, var_plot <span class="op">=</span> ed.model.predict(x_plot)</span></code></pre></div>
<div class="figure">
<div id="forrester-function-full-fit-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/uq/forrester-function-full-fit.svg" width="80%" style=" ">
</object>
</div>
<div id="forrester-function-full-fit-magnify" class="magnify" onclick="magnifyFigure(&#39;forrester-function-full-fit&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="forrester-function-full-fit-caption" class="caption-frame">
<p>Figure: The fit of the model to the Forrester function.</p>
</div>
</div>
<p><span class="citation" data-cites="Kennedy-predicting00 Sobol-sensitivity90 Sobol-global01 Saltelli-sensitivity04 Saltelli-global08 Saltelli-variance10">(Kennedy and O’Hagan 2000; Sobol 1990, 2001; Saltelli et al. 2004, 2008, 2010)</span></p>
<p>This introduction is based on <a href="https://github.com/EmuKit/emukit/blob/master/notebooks/Emukit-tutorial-sensitivity-montecarlo.ipynb">Introduction to Global Sensitivity Analysis with Emukit</a> written by Mark Pullin, Javier Gonzalez, Juan Emmanuel Johnson and Andrei Paleyes.</p>
<blockquote>
<p>A possible definition of sensitivity analysis is the following: The study of how uncertainty in the output of a model (numerical or otherwise) can be apportioned to different sources of uncertainty in the model input <span class="citation" data-cites="Saltelli-sensitivity04">(Saltelli et al. 2004)</span>. A related practice is ‘uncertainty analysis’, which focuses rather on quantifying uncertainty in model output. Ideally, uncertainty and sensitivity analyses should be run in tandem, with uncertainty analysis preceding in current practice.</p>
<p>In Chapter 1 of <span class="citation" data-cites="Saltelli-global08">Saltelli et al. (2008)</span></p>
</blockquote>
<div class="sourceCode" id="cb21"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb21-2"><a href="#cb21-2"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb21-3"><a href="#cb21-3"></a></span>
<span id="cb21-4"><a href="#cb21-4"></a><span class="im">from</span> matplotlib <span class="im">import</span> colors <span class="im">as</span> mcolors</span>
<span id="cb21-5"><a href="#cb21-5"></a><span class="im">from</span> matplotlib <span class="im">import</span> cm</span></code></pre></div>
<div class="sourceCode" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1"></a><span class="im">import</span> urllib.request</span></code></pre></div>
<div class="sourceCode" id="cb23"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1"></a>urllib.request.urlretrieve(<span class="st">&#39;https://raw.githubusercontent.com/lawrennd/talks/gh-pages/mlai.py&#39;</span>,<span class="st">&#39;mlai.py&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb24"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1"></a>urllib.request.urlretrieve(<span class="st">&#39;https://raw.githubusercontent.com/lawrennd/talks/gh-pages/teaching_plots.py&#39;</span>,<span class="st">&#39;teaching_plots.py&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb25"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1"></a>urllib.request.urlretrieve(<span class="st">&#39;https://raw.githubusercontent.com/lawrennd/talks/gh-pages/gp_tutorial.py&#39;</span>,<span class="st">&#39;gp_tutorial.py&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb26"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1"></a><span class="op">%</span>pip install gpy</span></code></pre></div>
<div class="sourceCode" id="cb27"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1"></a><span class="op">%</span>pip install pyDOE</span></code></pre></div>
<div class="sourceCode" id="cb28"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1"></a></span></code></pre></div>
<div class="sourceCode" id="cb29"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1"></a><span class="im">import</span> mlai</span>
<span id="cb29-2"><a href="#cb29-2"></a><span class="im">import</span> teaching_plots <span class="im">as</span> plot</span></code></pre></div>
<p>Sensitivity analysis is a statistical technique widely used to test the reliability of real systems. Imagine a simulator of taxis picking up customers in a city like the one showed in the <a href="https://github.com/amzn/emukit-playground">Emukit playground</a>. The profit of the taxi company depends on factors like the number of taxis on the road and the price per trip. In this example, a global sensitivity analysis of the simulator could be useful to decompose the variance of the profit in a way that can be assigned to the input variables of the simulator.</p>
<p>There are different ways of doing a sensitivity analysis of the variables of a simulator. In this notebook we will start with an approach based on Monte Carlo sampling that is useful when evaluating the simulator is cheap. If evaluating the simulator is expensive, emulators can then be used to speed up computations. We will show this in the last part of the notebook. Next, we start with a few formal definitions and literature review so we can understand the basics of Sensitivity Analysis and how it can performed with Emukit.</p>
<p>Any simulator can be viewed as a function <br /><span class="math display">$$
\dataScalar=\mappingFunction(\inputVector),
$$</span><br /> where <span class="math inline">$\inputVector$</span> is a vector of <span class="math inline">$\dataDim$</span> uncertain model inputs <span class="math inline">$\inputScalar_1,\dots,\inputScalar_\dataDim$</span>, and <span class="math inline">$\dataScalar$</span> is some univariate model output. We assume that <span class="math inline"><em>f</em></span> is a square integrable function and that the inputs are statistically independent and uniformly distributed within the hypercube <span class="math inline">$\inputScalar_i \in [0,1]$</span> for <span class="math inline">$i=1,2,\dots,\dataDim$</span>, although the bounds can be generalized. The Sobol decomposition of <span class="math inline">$\mappingFunction(\cdot)$</span> allows us to write it as <br /><span class="math display">$$
\dataScalar = \mappingFunction_0 + \sum_{i=1}^\dataDim \mappingFunction_i(\inputScalar_i) + \sum_{i&lt;j}^{\dataDim} \mappingFunction_{ij}(\inputScalar_i,\inputScalar_j) + \cdots + \mappingFunction_{1,2,\dots,\dataDim}(\inputScalar_1,\inputScalar_2,\dots,\inputScalar_\dataDim),
$$</span><br /> where <span class="math inline">$\mappingFunction_0$</span> is a constant term, <span class="math inline">$\mappingFunction_i$</span> is a function of <span class="math inline">$\inputScalar_i$</span>, <span class="math inline">$\mappingFunction_{ij}$</span> a function of <span class="math inline">$\inputScalar_i$</span> and <span class="math inline">$\inputScalar_j$</span>, etc. A condition of this decomposition is that, <br /><span class="math display">$$ 
\int_0^1 \mappingFunction_{i_1 i_2 \dots i_\dataDim}(\inputScalar_{i_1},\inputScalar_{i_2},\dots,\inputScalar_{i_\dataDim}) \text{d}\inputScalar_{k}=0, \text{ for } k = i_1,...,i_\dataDim. 
$$</span><br /> This means that all the terms in the decomposition are orthogonal, which can be written in terms of conditional expected values as <br /><span class="math display">$$\begin{align*}
\mappingFunction_0 &amp;= E(\dataScalar) \\
\mappingFunction_i(\inputScalar_i) &amp; = E(\dataScalar|\inputScalar_i) - \mappingFunction_0 \\
\mappingFunction_{ij}(\inputScalar_i,\inputScalar_j) &amp; = E(\dataScalar|\inputScalar_i,\inputScalar_j) - \mappingFunction_0 - \mappingFunction_i - \mappingFunction_j 
\end{align*}$$</span><br /> with all the expectations computed over <span class="math inline">$\dataScalar$</span>.</p>
<p>Each component <span class="math inline">$\mappingFunction_i$</span> (main effects) can be seen as the effect on <span class="math inline">$\dataScalar$</span> of varying <span class="math inline">$\inputScalar_i$</span> alone. The same interpretation follows for <span class="math inline">$\mappingFunction_{ij}$</span> which accounts for the (extra) variation of changing <span class="math inline">$\inputScalar_i$</span> and <span class="math inline">$\inputScalar_j$</span> simultaneously (second-order interaction). Higher-order terms have analogous definitions.</p>
<p>The key step to decompose the variation of <span class="math inline">$\dataScalar$</span> is to notice that <br /><span class="math display">$$
\text{var}(\dataScalar) = E(\dataScalar^2) - E(\dataScalar)^2 = \int_0^1 \mappingFunction^2(\inputVector) \text{d}\inputVector - \mappingFunction_0^2
$$</span><br /> and that this variance can be decomposed as <br /><span class="math display">$$ 
\text{var}(\dataScalar) = \int_0^1 \sum_{i=1}^\dataDim \mappingFunction_i(\inputScalar_i) \text{d}\inputScalar_i + \int_0^1 \sum_{i&lt;j}^{\dataDim} \mappingFunction_{ij}(\inputScalar_i,\inputScalar_j)\text{d} \inputScalar_i \text{d} \inputScalar_j + \cdots +\int_0^1 \mappingFunction_{1,2,\dots,d}(\inputScalar_1,\inputScalar_2,\dots,\inputScalar_\dataDim)\text{d}\inputVector.
$$</span><br /> This expression leads to the decomposition of the variance of <span class="math inline">$\dataScalar$</span> as, <br /><span class="math display">$$ 
\text{var}(\dataScalar) = \sum_{i=1}^\dataDim V_i + \sum_{i&lt;j}^{\dataDim} V_{ij} + \cdots + V_{12 \dots \dataDim},
$$</span><br /> where <br /><span class="math display">$$ 
V_{i} = \text{var}_{\inputScalar_i} \left( E_{\inputVector_{\sim i}} (\dataScalar \mid \inputScalar_{i}) \right),
$$</span><br /> <br /><span class="math display">$$ 
V_{ij} = \text{var}_{\inputScalar_{ij}} \left( E_{\inputVector_{\sim ij}} \left( \dataScalar \mid \inputScalar_i, \inputScalar_j\right)\right) - \operatorname{V}_{i} - \operatorname{V}_{j}
$$</span><br /> and so on. The <span class="math inline">$\inputVector_{\sim i}$</span> notation is used to indicate all the set of variables but the <span class="math inline"><em>i</em><sup><em>t</em><em>h</em></sup></span>.</p>
<p><strong>Note</strong>: The previous decomposition is important because it shows how the variance in the output <span class="math inline">$\dataScalar$</span> can be associated to each input or interaction separately</p>
<h2 id="example-the-ishigami-function">Example: the Ishigami function</h2>
<p>We illustrate the exact calculation of the Sobol indexes with the three dimensional Ishigami function of <span class="citation" data-cites="Ishigami-importance90">(Ishigami and Homma 1989)</span>. This is a well-known example for uncertainty and sensitivity analysis methods because of its strong nonlinearity and peculiar dependence on <span class="math inline">$\inputScalar_3$</span>. More details of this function can be found in <span class="citation" data-cites="Sobol-variance99">(Sobol and Levitan 1999)</span>.</p>
<p>Mathematically, the from of the Ishigami function is <br /><span class="math display">$$
\mappingFunction(\textbf{x}) = \sin(\inputScalar_1) + a \sin^2(\inputScalar_2) + b \inputScalar_3^4 \sin(\inputScalar_1). 
$$</span><br /> In this notebook we will set the parameters to be <span class="math inline"><em>a</em> = 5</span> and <span class="math inline"><em>b</em> = 0.1</span> . The input variables are sampled randomly <span class="math inline">$\inputScalar_i \sim \text{Uniform}(-\pi,\pi)$</span>.</p>
<p>Next we create the function object and visualize its shape marginally for each one of its three inputs.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1"></a><span class="im">from</span> emukit.test_functions.sensitivity <span class="im">import</span> Ishigami</span></code></pre></div>
<div class="sourceCode" id="cb31"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1"></a><span class="co">### --- Load the Ishigami function</span></span>
<span id="cb31-2"><a href="#cb31-2"></a>ishigami <span class="op">=</span> Ishigami(a<span class="op">=</span><span class="dv">5</span>, b<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb31-3"><a href="#cb31-3"></a>target_simulator <span class="op">=</span> ishigami.fidelity1</span>
<span id="cb31-4"><a href="#cb31-4"></a></span>
<span id="cb31-5"><a href="#cb31-5"></a><span class="co">### --- Define the input space in which the simulator is defined</span></span>
<span id="cb31-6"><a href="#cb31-6"></a>variable_domain <span class="op">=</span> (<span class="op">-</span>np.pi,np.pi)</span>
<span id="cb31-7"><a href="#cb31-7"></a>x_grid <span class="op">=</span> np.linspace(<span class="op">*</span>variable_domain,<span class="dv">100</span>)</span>
<span id="cb31-8"><a href="#cb31-8"></a>X, Y <span class="op">=</span> np.meshgrid(x_grid, x_grid)</span></code></pre></div>
<p>Before moving to any further analysis, we first plot the non-zero components <span class="math inline">$\mappingFunction(\inputVector)$</span>. These components are <br /><span class="math display">$$\begin{align*}
\mappingFunction_1(\inputScalar_1) &amp; = \sin(\inputScalar_1) \\
\mappingFunction_2(\inputScalar_1) &amp; = a \sin^2 (\inputScalar_2) \\
\mappingFunction_{13}(\inputScalar_1,\inputScalar_3) &amp; = b \inputScalar_3^4 \sin(\inputScalar_1) 
\end{align*}$$</span><br /></p>
<div class="sourceCode" id="cb32"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1"></a>f1 <span class="op">=</span> ishigami.f1(x_grid)</span>
<span id="cb32-2"><a href="#cb32-2"></a>f2 <span class="op">=</span> ishigami.f2(x_grid)</span>
<span id="cb32-3"><a href="#cb32-3"></a>F13 <span class="op">=</span> ishigami.f13(np.array([x_grid,x_grid]).T)[:,<span class="va">None</span>]</span></code></pre></div>
<div class="figure">
<div id="non-zero-sobol-ishigami-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/uq/non-zero-sobol-ishigami.svg" width="80%" style=" ">
</object>
</div>
<div id="non-zero-sobol-ishigami-magnify" class="magnify" onclick="magnifyFigure(&#39;non-zero-sobol-ishigami&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="non-zero-sobol-ishigami-caption" class="caption-frame">
<p>Figure: The non-zero components of the Ishigami function.</p>
</div>
</div>
<p>The total variance <span class="math inline">$\text{var}(\dataScalar)$</span> in this example is</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1"></a><span class="bu">print</span>(ishigami.variance_total)</span></code></pre></div>
<p>which is the sum of the variance of <span class="math inline"><em>V</em><sub>1</sub></span>, <span class="math inline"><em>V</em><sub>2</sub></span> and <span class="math inline"><em>V</em><sub>13</sub></span></p>
<div class="sourceCode" id="cb34"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1"></a><span class="bu">print</span>(ishigami.variance_x1, ishigami.variance_x2, ishigami.variance_x13)</span>
<span id="cb34-2"><a href="#cb34-2"></a><span class="bu">print</span>(ishigami.variance_x1 <span class="op">+</span> ishigami.variance_x2 <span class="op">+</span> ishigami.variance_x13)</span></code></pre></div>
<h2 id="first-order-sobol-indices-using-monte-carlo">First Order Sobol Indices using Monte Carlo</h2>
<p>The first order Sobol indexes are a measure of “first order sensitivity” of each input variable. They account for the proportion of variance of <span class="math inline">$\dataScalar$</span> explained by changing each variable alone while marginalizing over the rest. The Sobol index of the <span class="math inline"><em>i</em></span>th variable is computed as <br /><span class="math display">$$
S_i = \frac{V_i}{\text{var}(\dataScalar)}.
$$</span><br /> This value is standardized using the total variance so it is possible to account for a fractional contribution of each variable to the total variance of the output.</p>
<p>The Sobol indexes for higher order interactions <span class="math inline"><em>S</em><sub><em>i</em><em>j</em></sub></span> are computed similarly. Note that the sum of all Sobol indexes equals to one.</p>
<p>In most cases we are interested in the first order indexes. In the Ishigami function their values are:</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1"></a>ishigami.main_effects</span></code></pre></div>
<p>The most standard way of computing the Sobol indexes is using Monte Carlo. Details are given in <span class="citation" data-cites="Sobol-global01">(Sobol 2001)</span>.</p>
<p>With Emukit, the first-order Sobol indexes can be easily computed. We first need to define the space where of target simulator is analyzed.</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1"></a><span class="im">from</span> emukit.core <span class="im">import</span> ContinuousParameter, ParameterSpace</span></code></pre></div>
<div class="sourceCode" id="cb37"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1"></a>target_simulator <span class="op">=</span> ishigami.fidelity1</span>
<span id="cb37-2"><a href="#cb37-2"></a>variable_domain <span class="op">=</span> (<span class="op">-</span>np.pi,np.pi)</span>
<span id="cb37-3"><a href="#cb37-3"></a></span>
<span id="cb37-4"><a href="#cb37-4"></a>space <span class="op">=</span> ParameterSpace(</span>
<span id="cb37-5"><a href="#cb37-5"></a>          [ContinuousParameter(<span class="st">&#39;x1&#39;</span>, variable_domain[<span class="dv">0</span>], variable_domain[<span class="dv">1</span>]), </span>
<span id="cb37-6"><a href="#cb37-6"></a>           ContinuousParameter(<span class="st">&#39;x2&#39;</span>, variable_domain[<span class="dv">0</span>], variable_domain[<span class="dv">1</span>]),</span>
<span id="cb37-7"><a href="#cb37-7"></a>           ContinuousParameter(<span class="st">&#39;x3&#39;</span>, variable_domain[<span class="dv">0</span>], variable_domain[<span class="dv">1</span>])])</span></code></pre></div>
<p>Compute the indexes is as easy as doing</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1"></a><span class="im">from</span> emukit.sensitivity.monte_carlo <span class="im">import</span> ModelFreeMonteCarloSensitivity</span></code></pre></div>
<div class="sourceCode" id="cb39"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1"></a>np.random.seed(<span class="dv">10</span>)  <span class="co"># for reproducibility</span></span>
<span id="cb39-2"><a href="#cb39-2"></a></span>
<span id="cb39-3"><a href="#cb39-3"></a>num_mc <span class="op">=</span> <span class="dv">10000</span>  <span class="co"># Number of MC samples</span></span>
<span id="cb39-4"><a href="#cb39-4"></a>senstivity_ishigami <span class="op">=</span> ModelFreeMonteCarloSensitivity(target_simulator, space)</span>
<span id="cb39-5"><a href="#cb39-5"></a>main_effects, total_effects, _ <span class="op">=</span> senstivity_ishigami.compute_effects(num_monte_carlo_points <span class="op">=</span> num_mc)</span>
<span id="cb39-6"><a href="#cb39-6"></a><span class="bu">print</span>(main_effects)</span></code></pre></div>
<p>We compare the true effects with the Monte Carlo effects in a bar-plot. The total effects are discussed later.</p>
<div class="figure">
<div id="first-order-sobol-indices-ishigami-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/uq/first-order-sobol-indices-ishigami.svg" width="80%" style=" ">
</object>
</div>
<div id="first-order-sobol-indices-ishigami-magnify" class="magnify" onclick="magnifyFigure(&#39;first-order-sobol-indices-ishigami&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="first-order-sobol-indices-ishigami-caption" class="caption-frame">
<p>Figure: The non-zero components of the Ishigami function.</p>
</div>
</div>
<h2 id="total-effects-using-monte-carlo">Total Effects Using Monte Carlo</h2>
<p>Computing high order sensitivity indexes can be computationally very demanding in high dimensional scenarios and measuring the total influence of each variable on the variance of the output is infeasible. To solve this issue the <em>total</em> indexes are used which account for the contribution to the output variance of <span class="math inline">$\inputScalar_i$</span> including all variance caused by the variable alone and all its interactions of any order. The total effect for <span class="math inline">$\inputScalar_i$</span> is given by: <br /><span class="math display">$$ 
S_{Ti} = \frac{E_{\inputVector_{\sim i}} \left(\text{var}_{\inputScalar_i} (\dataScalar \mid \inputVector_{\sim i}) \right)}{\text{var}(\dataScalar)} = 1 - \frac{\text{var}_{\inputVector_{\sim i}} \left(E_{\inputScalar_i} (\dataScalar \mid \inputVector_{\sim i}) \right)}{\text{var}(\dataScalar)}
$$</span><br /></p>
<p>Note that the sum of <span class="math inline"><em>S</em><sub><em>T</em><em>i</em></sub></span> is not necessarily one in this case unless the model is additive. In the Ishigami example the value of the total effects is</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1"></a>ishigami.total_effects</span></code></pre></div>
<p>As in the previous example, the total effects can be computed with Monte Carlo. In the next plot we show the comparison with the true total effects.</p>
<div class="figure">
<div id="total-effects-ishigami-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/uq/total-effects-ishigami.svg" width="80%" style=" ">
</object>
</div>
<div id="total-effects-ishigami-magnify" class="magnify" onclick="magnifyFigure(&#39;total-effects-ishigami&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="total-effects-ishigami-caption" class="caption-frame">
<p>Figure: The total effects from the Ishigami function as computed via Monte Carlo estimate alongside the true total effects for the Ishigami function.</p>
</div>
</div>
<h2 id="computing-the-sensitivity-coefficients-using-the-output-of-a-model">Computing the sensitivity coefficients using the output of a model</h2>
<p>In the example used above the Ishigami function is very cheap to evaluate. However, in most real scenarios the functions of interest are expensive and we need to limit ourselves to a few number of evaluations. Using Monte Carlo methods is infeasible in these scenarios as a large number of samples are typically required to provide good estimates of the Sobol coefficients.</p>
<p>An alternative in these cases is to use Gaussaian process emulator of the function of interest trained on a few inputs and outputs. If the model is properly trained, its mean prediction which is cheap to evaluate, can be used to compute the Monte Carlo estimates of the Sobol coefficients. Let’s see how we can do this in Emukit.</p>
<p>We start by generating 100 samples in the input domain. Note that this a just 1% of the number of samples that we used to compute the Sobol coefficients using Monte Carlo.</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1"></a><span class="im">from</span> emukit.core.initial_designs <span class="im">import</span> RandomDesign</span></code></pre></div>
<div class="sourceCode" id="cb42"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1"></a>desing <span class="op">=</span> RandomDesign(space)</span>
<span id="cb42-2"><a href="#cb42-2"></a>x <span class="op">=</span> desing.get_samples(<span class="dv">500</span>)</span>
<span id="cb42-3"><a href="#cb42-3"></a>y <span class="op">=</span> ishigami.fidelity1(x)[:,<span class="va">None</span>]</span></code></pre></div>
<p>Now, we fit a standard Gaussian process to the samples and we wrap it as an Emukit model.</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1"></a><span class="im">from</span> GPy.models <span class="im">import</span> GPRegression</span>
<span id="cb43-2"><a href="#cb43-2"></a><span class="im">from</span> emukit.model_wrappers <span class="im">import</span> GPyModelWrapper</span>
<span id="cb43-3"><a href="#cb43-3"></a><span class="im">from</span> emukit.sensitivity.monte_carlo <span class="im">import</span> MonteCarloSensitivity</span></code></pre></div>
<div class="sourceCode" id="cb44"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1"></a>model_gpy <span class="op">=</span> GPRegression(x,y)</span>
<span id="cb44-2"><a href="#cb44-2"></a>model_emukit <span class="op">=</span> GPyModelWrapper(model_gpy)</span>
<span id="cb44-3"><a href="#cb44-3"></a>model_emukit.optimize()</span></code></pre></div>
<p>The final step is to compute the coefficients using the class <code>ModelBasedMonteCarloSensitivity</code> which directly calls the model and uses its predictive mean to compute the Monte Carlo estimates of the Sobol indices. We plot the true estimates, those computed using 10000 direct evaluations of the objecte using Monte Carlo and those computed using a Gaussian process model trained on 100 evaluations.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1"></a>senstivity_ishigami_gpbased <span class="op">=</span> MonteCarloSensitivity(model <span class="op">=</span> model_emukit, input_domain <span class="op">=</span> space)</span>
<span id="cb45-2"><a href="#cb45-2"></a>main_effects_gp, total_effects_gp, _ <span class="op">=</span> senstivity_ishigami_gpbased.compute_effects(num_monte_carlo_points <span class="op">=</span> num_mc)</span></code></pre></div>
<div class="figure">
<div id="first-order-sobol-indices-gp-ishigami-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/uq/first-order-sobol-indices-gp-ishigami.svg" width="80%" style=" ">
</object>
</div>
<div id="first-order-sobol-indices-gp-ishigami-magnify" class="magnify" onclick="magnifyFigure(&#39;first-order-sobol-indices-gp-ishigami&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="first-order-sobol-indices-gp-ishigami-caption" class="caption-frame">
<p>Figure: First Order sobol indices as estimated by Monte Carlo and GP-emulator based Monte Carlo.</p>
</div>
</div>
<div class="figure">
<div id="total-effects-sobol-indices-gp-ishigami-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/uq/total-effects-sobol-indices-gp-ishigami.svg" width="80%" style=" ">
</object>
</div>
<div id="total-effects-sobol-indices-gp-ishigami-magnify" class="magnify" onclick="magnifyFigure(&#39;total-effects-sobol-indices-gp-ishigami&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="total-effects-sobol-indices-gp-ishigami-caption" class="caption-frame">
<p>Figure: Total effects as estimated by Monte Carlo and GP based Monte Carlo.</p>
</div>
</div>
<p>We observe some discrepacies with respect to the real value of the coefficient when using the Gaussian process but we get a fairly good a approximation a very reduced number of evaluations of the original target function.</p>
<h2 id="conclusions">Conclusions</h2>
<p>The Sobol indexes are a tool for explaining the variance of the output of a function as components of the input variables. Monte Carlo is an approach for computing these indexes if the function is cheap to evaluate. Other approaches will be needed if <span class="math inline">$\mappingFunction(\cdot)$</span> is expensive to compute.</p>
<h2 id="thanks">Thanks!</h2>
<p>For more information on these subjects and more you might want to check the following resources.</p>
<ul>
<li>twitter: <a href="https://twitter.com/lawrennd">@lawrennd</a></li>
<li>podcast: <a href="http://thetalkingmachines.com">The Talking Machines</a></li>
<li>newspaper: <a href="http://www.theguardian.com/profile/neil-lawrence">Guardian Profile Page</a></li>
<li>blog: <a href="http://inverseprobability.com/blog.html">http://inverseprobability.com</a></li>
</ul>
<h1 class="unnumbered" id="references">References</h1>
<div id="refs" class="references hanging-indent" role="doc-bibliography">
<div id="ref-Forrester-engineering08">
<p>Forrester, Alexander I. J., András Sóbester, and Andy J. Keane. 2008. <em>Engineering Design via Surrogate Modelling: A Practical Guide</em>. wiley. <a href="https://doi.org/10.1002/9780470770801">https://doi.org/10.1002/9780470770801</a>.</p>
</div>
<div id="ref-Ishigami-importance90">
<p>Ishigami, Tsutomu, and Toshimitsu Homma. 1989. “An Importance Quantification Technique in Uncertainty Analysis for Computer Models.” <em>[1990] Proceedings. First International Symposium on Uncertainty Modeling and Analysis</em>, 398–403.</p>
</div>
<div id="ref-Kennedy-predicting00">
<p>Kennedy, Marc C., and Anthony O’Hagan. 2000. “Predicting the Output from a Complex Computer Code When Fast Approximations Are Available.” <em>Biometrika</em> 87 (1): 1–13. <a href="http://www.jstor.org/stable/2673557">http://www.jstor.org/stable/2673557</a>.</p>
</div>
<div id="ref-Rasmussen:book06">
<p>Rasmussen, Carl Edward, and Christopher K. I. Williams. 2006. <em>Gaussian Processes for Machine Learning</em>. Cambridge, MA: mit.</p>
</div>
<div id="ref-Sacks-design89">
<p>Sacks, Jerome, William J. Welch, Toby J. Mitchell, and Henry P. Wynn. 1989. “Design and Analysis of Computer Experiments.” <em>Statistical Science</em> 4 (4): 409–23. <a href="https://doi.org/10.1214/ss/1177012413">https://doi.org/10.1214/ss/1177012413</a>.</p>
</div>
<div id="ref-Saltelli-variance10">
<p>Saltelli, Andrea, Paola Annoni, Ivano Azzini, Francesca Campolongo, Marco Ratto, and Stefano Tarantola. 2010. “Variance Based Sensitivity Analysis of Model Output. Design and Estimator for the Total Sensitivity Index.” <em>Computer Physics Communications</em> 181 (2): 259–70. <a href="https://doi.org/10.1016/j.cpc.2009.09.018">https://doi.org/10.1016/j.cpc.2009.09.018</a>.</p>
</div>
<div id="ref-Saltelli-global08">
<p>Saltelli, Andrea, Marco Ratto, Terry Andres, Francesca Campolongo, Jessica Cariboni, Debora Gatelli, Michaela Saisana, and Stefan Tarantola. 2008. <em>Global Sensitivity Analysis: The Primer</em>. wiley.</p>
</div>
<div id="ref-Saltelli-sensitivity04">
<p>Saltelli, Andrea, Stefano Tarantola, Francesca Campolongo, and Marco Ratto. 2004. <em>Sensitivity Analysis in Practice: A Guide to Assessing Scientific Methods</em>. wiley.</p>
</div>
<div id="ref-Sobol-sensitivity90">
<p>Sobol, Ilya M. 1990. “On Sensitivity Estimation for Nonlinear Mathematical Models.” <em>Matematicheskoe Modelirovanie</em> 2 (1): 112–18.</p>
</div>
<div id="ref-Sobol-global01">
<p>———. 2001. “Global Sensitivity Indices for Nonlinear Mathematical Models and Their Monte Carlo Estimates.” <em>Mathematics and Computers in Simulation</em> 55 (1): 271–80. <a href="https://doi.org/10.1016/S0378-4754(00)00270-6">https://doi.org/10.1016/S0378-4754(00)00270-6</a>.</p>
</div>
<div id="ref-Sobol-variance99">
<p>Sobol, Ilya M., and Yu L. Levitan. 1999. “On the Use of Variance Reducing Multipliers in Monte Carlo Computations of a Global Sensitivity Index.” <em>Computer Physics Communications</em> 117 (1): 52–61. <a href="https://doi.org/10.1016/S0010-4655(98)00156-8">https://doi.org/10.1016/S0010-4655(98)00156-8</a>.</p>
</div>
</div>

